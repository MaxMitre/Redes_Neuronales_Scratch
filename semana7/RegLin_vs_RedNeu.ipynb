{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semana7.1_RNS_RegLin_vs_RedNeu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MaxMitre/Redes_Neuronales_Scratch/blob/main/semana7/RegLin_vs_RedNeu.ipynb)"
      ],
      "metadata": {
        "id": "14LLt_r5BCHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción"
      ],
      "metadata": {
        "id": "nilXCRj7A8Xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este notebook se utilizará un dataset para regresión, con la finalidad de comparar el desempeño entre una regresión lineal y un par de redes neuronales. \n",
        "\n",
        "Se verá la importación de las depencias, la carga de los datos, el uso de [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) junto a [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) para conectar facilmente al procesamiento de los datos con el algoritmo (en este caso, regresión lineal). \n",
        "\n",
        "También, se verá la creación de las redes neuronales y el uso de callbacks (en particular, guardar checkpoints) durante el entrenamiento.\n",
        "\n",
        "Para todos los modelos se evaluará el desempeño que se obtuvo y se harán comparaciones entre los tres."
      ],
      "metadata": {
        "id": "1nfpaMZA1LlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencias"
      ],
      "metadata": {
        "id": "C5RrB_RNCNDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U plotly"
      ],
      "metadata": {
        "id": "WCX83vYyk1iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysF3G6ZafSmP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Manejo de los datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Utilidades, modelo y métricas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Para la red neuronal\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Visualización\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "vcCzm4LSm132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41011fda-7ca2-40de-a96c-b34fda1ea72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pio.templates.default = \"plotly_white\""
      ],
      "metadata": {
        "id": "DlJUqRPUh8ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directorio para guardar los checkpoints de las redes neuronales, para código local\n",
        "# checkpoint_filepath = 'checkpoints'"
      ],
      "metadata": {
        "id": "n8BwHVdbsvvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2K5cAH665biQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8c22bf-f4d3-44f7-edef-7f3499c95bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones"
      ],
      "metadata": {
        "id": "SKHnUQyofNqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_evaluation(real_train, predecido_train, real_test, predecido_test, plot_title):\n",
        "    \n",
        "    fig = make_subplots(\n",
        "        rows = 1, cols = 2, \n",
        "        subplot_titles = ['Conjunto de entrenamiento', 'Conjunto de prueba'],        \n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x = real_train, \n",
        "            y = predecido_train, \n",
        "            mode='markers', \n",
        "            ), \n",
        "        row = 1, col = 1\n",
        "    )\n",
        "\n",
        "    fig.add_shape(\n",
        "        type = 'line', \n",
        "        x0 = np.min(predecido_train), y0 = np.min(predecido_train), \n",
        "        x1 = np.max(predecido_train), y1 = np.max(predecido_train), \n",
        "        line=dict(dash = 'dot')\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x = real_test, y = predecido_test, mode = 'markers'), \n",
        "        row = 1, col = 2\n",
        "    )\n",
        "\n",
        "    fig.add_shape(\n",
        "        type = 'line', \n",
        "        x0 = np.min(predecido_test), y0 = np.min(predecido_test), \n",
        "        x1 = np.max(predecido_test), y1 = np.max(predecido_test), \n",
        "        line=dict(dash = 'dot'), \n",
        "        row = 1, col = 2\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        showlegend=False, \n",
        "        title_text=plot_title\n",
        "    )\n",
        "\n",
        "    fig.update_xaxes(title_text = 'Cargos reales', row = 1, col = 1)\n",
        "    fig.update_xaxes(title_text = 'Cargos reales', row = 1, col = 2)\n",
        "    fig.update_yaxes(title_text = 'Cargos predichos', row = 1, col = 1)\n",
        "    fig.update_yaxes(title_text = 'Cargos predichos', row = 1, col = 2)\n",
        "\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "lXVjHz8CfPCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datos"
      ],
      "metadata": {
        "id": "vRb33QprCO2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se trabajará con los datos de [Medical Cost Personal Datasets](https://www.kaggle.com/mirichoi0218/insurance). Contiene 7 columnas descritas a continuación:\n",
        "\n",
        "\n",
        "\n",
        "* age: Edad del beneficiario del seguro\n",
        "\n",
        "* sex: Género del contratante del seguro (female, male)\n",
        "\n",
        "* bmi: Índice de masa corporal, se obtiene como el pesó entre la estatura del paciente (kg / m ^ 2) usando la escala convencional, idealmente de 18.5 a 24.9\n",
        "\n",
        "* children: Número de niños que cubre el seguro (Dependientes del asegurado)\n",
        "\n",
        "* smoker: Fumador\n",
        "\n",
        "* region: Donde se encuentra el área residencial del beneficiario en EUA (northeast, southeast, southwest, northwest)\n",
        "\n",
        "* charges: Cargos de gastos médicos cubiertos por el seguro\n",
        "\n",
        "\n",
        "Se quiere saber si se pueden predecir los cargos (charges) en función de las demás variables."
      ],
      "metadata": {
        "id": "sZiAlTuAWOz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de los datos"
      ],
      "metadata": {
        "id": "En-vePooXzlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "xwH_MzmFhS8u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "82c0e6c0-bc69-4622-f670-a530534ebba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      age     sex     bmi  children smoker     region      charges\n",
              "0      19  female  27.900         0    yes  southwest  16884.92400\n",
              "1      18    male  33.770         1     no  southeast   1725.55230\n",
              "2      28    male  33.000         3     no  southeast   4449.46200\n",
              "3      33    male  22.705         0     no  northwest  21984.47061\n",
              "4      32    male  28.880         0     no  northwest   3866.85520\n",
              "...   ...     ...     ...       ...    ...        ...          ...\n",
              "1333   50    male  30.970         3     no  northwest  10600.54830\n",
              "1334   18  female  31.920         0     no  northeast   2205.98080\n",
              "1335   18  female  36.850         0     no  southeast   1629.83350\n",
              "1336   21  female  25.800         0     no  southwest   2007.94500\n",
              "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
              "\n",
              "[1338 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41c0390c-023e-446f-b033-8f99fe3f0662\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>female</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>southwest</td>\n",
              "      <td>16884.92400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>male</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1725.55230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>male</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>4449.46200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>male</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>21984.47061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>male</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>3866.85520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1333</th>\n",
              "      <td>50</td>\n",
              "      <td>male</td>\n",
              "      <td>30.970</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>10600.54830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1334</th>\n",
              "      <td>18</td>\n",
              "      <td>female</td>\n",
              "      <td>31.920</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northeast</td>\n",
              "      <td>2205.98080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1335</th>\n",
              "      <td>18</td>\n",
              "      <td>female</td>\n",
              "      <td>36.850</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1629.83350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336</th>\n",
              "      <td>21</td>\n",
              "      <td>female</td>\n",
              "      <td>25.800</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>southwest</td>\n",
              "      <td>2007.94500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1337</th>\n",
              "      <td>61</td>\n",
              "      <td>female</td>\n",
              "      <td>29.070</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>northwest</td>\n",
              "      <td>29141.36030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1338 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41c0390c-023e-446f-b033-8f99fe3f0662')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41c0390c-023e-446f-b033-8f99fe3f0662 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41c0390c-023e-446f-b033-8f99fe3f0662');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "7rJxT0IzQQEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b25f1fc-304e-4b47-9657-f7403845c0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descripción de los datos"
      ],
      "metadata": {
        "id": "zQ9T_gR0X1Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "ep_Sn_9Vj37c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e8ed2c-d4dd-4e7c-f3f2-229c77a5ad74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1338 entries, 0 to 1337\n",
            "Data columns (total 7 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       1338 non-null   int64  \n",
            " 1   sex       1338 non-null   object \n",
            " 2   bmi       1338 non-null   float64\n",
            " 3   children  1338 non-null   int64  \n",
            " 4   smoker    1338 non-null   object \n",
            " 5   region    1338 non-null   object \n",
            " 6   charges   1338 non-null   float64\n",
            "dtypes: float64(2), int64(2), object(3)\n",
            "memory usage: 73.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## División en conjuntos de entrenamiento y prueba"
      ],
      "metadata": {
        "id": "nbw8X7yPX8zX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos el DataFrame en dos: \n",
        "1. X: Contiene las variables regresoras.\n",
        "2. y: La variable de respuesta."
      ],
      "metadata": {
        "id": "Eb5HfsgiYS_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(columns = 'charges')\n",
        "y = data['charges']"
      ],
      "metadata": {
        "id": "_h9fy3zBhVrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora divimos el dataset en el conjunto de entrenamiento y de prueba. Esto, como hemos visto, nos ayuda a evaluar si el modelo \"aprendió\" a generalizar las predicciones o si se sobreajustó.\n",
        "\n",
        "El conjunto de entrenamiento tendrá el 70% de los datos totales."
      ],
      "metadata": {
        "id": "IOK-uULdYm0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 10)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "bfX60prNhWJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682f6a01-4b60-4cd2-8559-86959f263ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(936, 6) (936,) (402, 6) (402,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressión Lineal"
      ],
      "metadata": {
        "id": "TQPMRLmPCQ_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero veremos los resultados que obtenemos con una regresión lineal (múltiple). Sin entrar en detalle a los supuestos del algoritmo, la hipótesis es la siguiente.\n",
        "\n",
        "\\begin{align}\n",
        "    y_i = \\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_px_{ip} + \\epsilon_i,\n",
        "\\end{align}\n",
        "\n",
        "donde, para el *i-ésimo* caso, $y_i$ es la variable de respuesta, $x_{ij}$ es la medición de la *j-ésima* variable, $b_j$ es el coeficiente de ésta variable y $\\epsilon_i$ es el *error* (que queremos que sea lo más pequeño posible). \n",
        "\n",
        "Es decir, **la variable de respuesta es una *combinación lineal* de los regresores más un error**. Equivalentemente, se encuentra el hiperplano que \n",
        "\"mejor se ajuste\" a los datos cuando se minimiza el error."
      ],
      "metadata": {
        "id": "IIHThcdcZAd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo/Arquitectura"
      ],
      "metadata": {
        "id": "eq777ocRclwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crearemos un [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) para facilitar el uso del modelo para las predicciones. \n",
        "\n",
        "El primer paso es un tranformador de columnas que se va a encargar de crear las variables dummies de las variables categóricas.\n",
        "\n",
        "El segundo paso, la regresión lineal."
      ],
      "metadata": {
        "id": "KFwNMLsWbcWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = ColumnTransformer(\n",
        "    [\n",
        "     (\"dummies\", OneHotEncoder(drop = 'first'), ['sex', 'smoker', 'region'])\n",
        "    ], \n",
        "    remainder = 'passthrough'\n",
        ")\n",
        "\n",
        "lr_pipe = Pipeline(\n",
        "    [\n",
        "     (\"transformador\", transformer), \n",
        "     ('linear_regression', LinearRegression())\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "EBPMQLhXiqRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "Td3pEmDjcqNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos la función ```fit``` para ajustar el modelo a nuestros datos."
      ],
      "metadata": {
        "id": "aTzQkSmscuNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "XGN27lfvcrft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef5fec6-4613-488c-f718-c9f6c8c5ff86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('transformador',\n",
              "                 ColumnTransformer(remainder='passthrough',\n",
              "                                   transformers=[('dummies',\n",
              "                                                  OneHotEncoder(drop='first'),\n",
              "                                                  ['sex', 'smoker',\n",
              "                                                   'region'])])),\n",
              "                ('linear_regression', LinearRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "4Qp8nI5TCTVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el modelo entrenado, usamos la función ```predict``` para obtener las predicciones del modelo con los datos que le suministramos.\n",
        "\n",
        "Obtenemos las predicciones de ambos conjuntos de datos (entrenamiento y prueba)."
      ],
      "metadata": {
        "id": "aIPN7alXc0kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_lr = lr_pipe.predict(X_train)\n",
        "y_pred_test_lr = lr_pipe.predict(X_test)"
      ],
      "metadata": {
        "id": "UnCzu25Zh5Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verdaderos = y_test.reset_index(drop=True)\n",
        "predichos = pd.Series(y_pred_test_lr, name='pred_charges').reset_index(drop=True)\n",
        "comparativo = pd.concat([predichos, verdaderos], axis=1) "
      ],
      "metadata": {
        "id": "1L5KDyBO64Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparativo"
      ],
      "metadata": {
        "id": "kNCKNkL07PRn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a1a4790a-d9bf-4eb4-e793-3a7a952c0326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     pred_charges      charges\n",
              "0     8662.892824   7281.50560\n",
              "1     6261.494309   5267.81815\n",
              "2    15331.327092  12347.17200\n",
              "3    11299.024976  24513.09126\n",
              "4     4104.169420   3736.46470\n",
              "..            ...          ...\n",
              "397  34591.935065  24106.91255\n",
              "398   8013.547530  17878.90068\n",
              "399  32499.744617  22462.04375\n",
              "400   2917.938751   1391.52870\n",
              "401  10775.164566   8240.58960\n",
              "\n",
              "[402 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e353968-cb5e-41bc-9be6-5137fe6e7f9c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred_charges</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8662.892824</td>\n",
              "      <td>7281.50560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6261.494309</td>\n",
              "      <td>5267.81815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15331.327092</td>\n",
              "      <td>12347.17200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11299.024976</td>\n",
              "      <td>24513.09126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4104.169420</td>\n",
              "      <td>3736.46470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>34591.935065</td>\n",
              "      <td>24106.91255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>8013.547530</td>\n",
              "      <td>17878.90068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>32499.744617</td>\n",
              "      <td>22462.04375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>2917.938751</td>\n",
              "      <td>1391.52870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>10775.164566</td>\n",
              "      <td>8240.58960</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>402 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e353968-cb5e-41bc-9be6-5137fe6e7f9c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5e353968-cb5e-41bc-9be6-5137fe6e7f9c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5e353968-cb5e-41bc-9be6-5137fe6e7f9c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métricas"
      ],
      "metadata": {
        "id": "yaQFUywECXIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las métricas que usaremos son 3:\n",
        "\n",
        "1. **Coeficiente de determinación ($R^2$)**: El mejor posible valor es 1 (toda la variabilidad de la variable dependiente se puede explicar con las variables independientes). Un modelo que siempre predice la media de la variable de respuesta tiene un resultado de 0. Modelos con peor desempeño que el que sólo predice la media, obtienen valores negativos.\n",
        "\n",
        "2. **Error cuadrático promedio (MSE)**: El promedio de los errores al cuadrado.\n",
        "\n",
        "3. **Error absoluto promedio (MAE)**: El promedio de los valores absolutos de los errores."
      ],
      "metadata": {
        "id": "IXn79Fz1dS58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"R2 train: {r2_score(y_train, y_pred_train_lr)}\", \n",
        "    f\"R2 test: {r2_score(y_test, y_pred_test_lr)}\", \n",
        "    \"\",\n",
        "    f\"MSE train: {mean_squared_error(y_train, y_pred_train_lr)}\", \n",
        "    f\"MSE test: {mean_squared_error(y_test, y_pred_test_lr)}\", \n",
        "    \"\",\n",
        "    f\"MAE train: {mean_absolute_error(y_train, y_pred_train_lr)}\", \n",
        "    f\"MAE test: {mean_absolute_error(y_test, y_pred_test_lr)}\", \n",
        "    sep = '\\n'\n",
        ")"
      ],
      "metadata": {
        "id": "0UpBhxqbiUN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5304bd96-b62c-46b6-868b-cdc8ed0e3817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 train: 0.7618780003618578\n",
            "R2 test: 0.7166124432331722\n",
            "\n",
            "MSE train: 36576361.89065711\n",
            "MSE test: 36800107.88801562\n",
            "\n",
            "MAE train: 4175.618401731289\n",
            "MAE test: 4226.647664219914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El coeficiente de determinación de .76 en entramiento y .71 en prueba nos indica que el modelo se comporta de manera similar con datos vistos durante el entramiento y fuera de estos (aparentemente no hay sobreajuste)."
      ],
      "metadata": {
        "id": "c_B5iynagv98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización"
      ],
      "metadata": {
        "id": "_WL7bVz6CZNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_evaluation(y_train, y_pred_train_lr, y_test, y_pred_test_lr, \"Regresión lineal\")"
      ],
      "metadata": {
        "id": "9X7uPbRggOt7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "4b42b75e-d50c-43a2-8bb7-282bce3d7aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"5b62a84b-2db1-4672-8494-573d92586afb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5b62a84b-2db1-4672-8494-573d92586afb\")) {                    Plotly.newPlot(                        \"5b62a84b-2db1-4672-8494-573d92586afb\",                        [{\"mode\":\"markers\",\"x\":[3167.45585,2689.4954,11576.13,16586.49771,6746.7425,5976.8311,5649.715,15161.5344,2007.945,19214.70553,16455.70785,10594.50155,27117.99378,4296.2712,7151.092,8410.04685,22331.5668,37165.1638,17468.9839,2494.022,6356.2707,11305.93455,12235.8392,14256.1928,13822.803,7173.35995,9140.951,1711.0268,5926.846,32108.66282,17942.106,43896.3763,8671.19125,6112.35295,15019.76005,11365.952,2709.24395,43254.41795,1263.249,3757.8448,20420.60465,2302.3,11658.11505,11394.06555,1967.0227,3353.4703,10269.46,18963.17192,31620.00106,30259.99556,6571.544,1632.56445,2755.02095,5934.3798,14319.031,19964.7463,6849.026,2498.4144,1631.6683,3766.8838,9625.92,3309.7926,37079.372,15359.1045,36124.5737,2842.76075,1622.1885,46599.1084,6948.7008,4500.33925,2128.43105,9095.06825,19673.33573,48824.45,8688.85885,3645.0894,2480.9791,2154.361,33307.5508,12643.3778,27375.90478,8556.907,37270.1512,6593.5083,9101.798,13019.16105,28868.6639,1621.3402,6571.02435,34828.654,4260.744,8765.249,4040.55825,9875.6804,4337.7352,11356.6609,9290.1395,34439.8559,8891.1395,7162.0122,8026.6666,35160.13457,13555.0049,4719.73655,7222.78625,2534.39375,1719.4363,1635.73365,1725.5523,1824.2854,8342.90875,12829.4551,12979.358,4391.652,19933.458,7623.518,7640.3092,28101.33305,8442.667,2721.3208,2803.69785,12044.342,1131.5066,11743.9341,5974.3847,2731.9122,26140.3603,4243.59005,4846.92015,28287.89766,20630.28351,2741.948,29186.48236,1261.859,6877.9801,14283.4594,4402.233,37742.5757,13217.0945,11163.568,13451.122,26467.09737,58571.07448,13204.28565,1639.5631,2473.3341,48970.2476,9910.35985,3044.2133,60021.39897,41034.2214,9877.6077,8125.7845,2020.5523,25382.297,34303.1672,7742.1098,10702.6424,8516.829,27322.73386,37701.8768,26125.67477,30184.9367,10107.2206,3070.8087,5152.134,4746.344,5910.944,4415.1588,14043.4767,16085.1275,8428.0693,39727.614,8283.6807,14210.53595,10355.641,12265.5069,6455.86265,11488.31695,4134.08245,9563.029,18804.7524,11856.4115,11264.541,1631.8212,38792.6856,42560.4304,2150.469,7050.642,42112.2356,41949.2441,4462.7218,5484.4673,20234.85475,1877.9294,11085.5868,8219.2039,13126.67745,5227.98875,3268.84665,12479.70895,6250.435,28923.13692,2136.88225,13937.6665,36580.28216,6652.5288,3947.4131,19444.2658,12741.16745,1832.094,13770.0979,6113.23105,6198.7518,38511.6283,14382.70905,16450.8947,7133.9025,37829.7242,6406.4107,6933.24225,2137.6536,12404.8791,10096.97,26109.32905,3906.127,44423.803,12124.9924,1253.936,34166.273,45702.02235,13470.86,2632.992,38709.176,9182.17,1826.843,7243.8136,12430.95335,10118.424,4827.90495,5312.16985,3213.62205,4779.6023,4931.647,23082.95533,4433.9159,7985.815,33750.2918,48675.5177,33475.81715,9872.701,44260.7499,10601.412,2117.33885,11070.535,33900.653,2639.0429,5428.7277,11837.16,9301.89355,35491.64,47291.055,9566.9909,14410.9321,20878.78443,42983.4585,10796.35025,11884.04858,11244.3769,3279.86855,6796.86325,27037.9141,10848.1343,10560.4917,44202.6536,9288.0267,9620.3307,5458.04645,36149.4835,11381.3254,21595.38229,21880.82,1977.815,36837.467,6640.54485,4347.02335,36219.40545,26926.5144,4435.0942,6128.79745,3597.596,1704.5681,16776.30405,2727.3951,7348.142,1242.816,45863.205,24671.66334,6186.127,19539.243,2566.4707,49577.6624,39836.519,6402.29135,8827.2099,17904.52705,6272.4772,6500.2359,2257.47525,2155.6815,2362.22905,1815.8759,1875.344,9724.53,11538.421,2219.4451,2156.7518,42969.8527,1708.92575,9722.7695,15820.699,11842.442,8582.3023,33907.548,38998.546,7345.7266,4463.2051,15828.82173,2867.1196,5438.7491,4564.19145,46718.16325,12592.5345,23563.01618,6185.3208,10976.24575,27941.28758,4189.1131,41097.16175,36197.699,15817.9857,26018.95052,3857.75925,3490.5491,6238.298,2396.0959,7419.4779,8413.46305,1880.07,7050.0213,4518.82625,11552.904,10982.5013,4673.3922,23807.2406,20149.3229,12949.1554,4753.6368,27000.98473,24180.9335,7731.4271,7749.1564,9644.2525,42211.1382,6940.90985,24869.8368,13457.9608,12224.35085,8551.347,22478.6,12925.886,5012.471,9414.92,14988.432,4618.0799,2730.10785,19749.38338,3556.9223,39241.442,17043.3414,8277.523,4830.63,7518.02535,10214.636,18767.7377,1242.26,4536.259,8965.79575,34617.84065,16796.41194,11848.141,8932.084,20009.63365,1759.338,45008.9555,37465.34375,46661.4424,11881.9696,5972.378,1252.407,3875.7341,1743.214,3981.9768,13635.6379,10381.4787,9048.0273,3704.3545,5209.57885,7265.7025,47462.894,11674.13,1737.376,2775.19215,3693.428,5028.1466,1639.5631,9058.7303,10226.2842,4529.477,1629.8335,12815.44495,24915.04626,14692.66935,7986.47525,8017.06115,12105.32,3554.203,10461.9794,5615.369,4795.6568,19515.5416,2103.08,4349.462,12523.6048,10072.05505,1136.3994,21677.28345,1704.70015,21774.32215,13063.883,5116.5004,10825.2537,3353.284,4149.736,13405.3903,10977.2063,14349.8544,11741.726,44641.1974,4762.329,7371.772,3176.8159,2850.68375,40273.6455,17560.37975,4433.3877,1526.312,1702.4553,6373.55735,10942.13205,8547.6913,8604.48365,3176.2877,11150.78,4527.18295,12233.828,9850.432,7196.867,11729.6795,21223.6758,10435.06525,5272.1758,2250.8352,33732.6867,14358.36437,8233.0975,17496.306,2789.0574,14901.5167,5124.1887,8027.968,13143.33665,43578.9394,10807.4863,8522.003,5400.9805,9447.3824,10601.63225,4237.12655,14001.2867,1615.7667,11840.77505,40182.246,15230.32405,4877.98105,28340.18885,2699.56835,9748.9106,8539.671,47403.88,20167.33603,17352.6803,4058.1161,16297.846,8596.8278,2497.0383,38282.7495,36898.73308,14474.675,13224.693,42124.5153,14571.8908,11253.421,1534.3045,18246.4955,32548.3405,3877.30425,11842.62375,38126.2465,16577.7795,24915.22085,8569.8618,7804.1605,10736.87075,8968.33,9991.03765,21195.818,11658.37915,8603.8234,13919.8229,4915.05985,2217.6012,1634.5734,4934.705,32787.45859,12269.68865,2217.46915,5373.36425,8601.3293,12142.5786,7077.1894,20709.02034,1633.0444,2130.6759,7537.1639,14426.07385,1261.442,5757.41345,2211.13075,8782.469,6748.5912,11833.7823,10928.849,5709.1644,2203.73595,5354.07465,23065.4207,6875.961,22412.6485,2055.3249,12950.0712,40003.33225,12363.547,4032.2407,5855.9025,17626.23951,11033.6617,3594.17085,11396.9002,41999.52,9391.346,3577.999,4504.6624,40720.55105,3392.3652,9264.797,22192.43711,10450.552,2416.955,5989.52365,20177.67113,21984.47061,8835.26495,36307.7983,18648.4217,39722.7462,12485.8009,5327.40025,6496.886,10579.711,6686.4313,10577.087,1980.07,11455.28,2221.56445,5397.6167,1986.9334,17179.522,3761.292,8733.22925,6082.405,9855.1314,6117.4945,9866.30485,13844.7972,7935.29115,2913.569,18328.2381,1682.597,39611.7577,2201.0971,8823.279,30063.58055,39047.285,24535.69855,3659.346,1163.4627,4718.20355,2020.177,24059.68019,3206.49135,7201.70085,11881.358,46130.5265,23288.9284,8520.026,8023.13545,42760.5022,21348.706,35147.52848,14451.83515,12096.6512,34254.05335,4005.4225,14001.1338,9715.841,12890.05765,48673.5588,7228.21565,19719.6947,39774.2763,2974.126,6548.19505,1964.78,3861.20965,2200.83085,20781.48892,19521.9682,13981.85035,8515.7587,14235.072,41919.097,13143.86485,25992.82104,12638.195,10959.6947,42111.6647,7729.64575,24227.33724,10106.13425,1842.519,16138.76205,9283.562,43943.8761,3484.331,48885.13561,1731.677,14394.39815,44501.3982,2404.7338,3591.48,10806.839,11944.59435,2464.6188,46151.1245,4894.7533,9193.8385,14119.62,4133.64165,11945.1327,20745.9891,25081.76784,2710.82855,11657.7189,13470.8044,9800.8882,23244.7902,19350.3689,4449.462,13880.949,16115.3045,33471.97189,2304.0022,47305.305,2138.0707,3056.3881,28468.91901,2395.17155,13887.204,21771.3423,8269.044,12222.8983,48549.17835,8605.3615,5325.651,20984.0936,10407.08585,12032.326,10422.91665,12648.7034,9282.4806,47896.79135,13393.756,1981.5819,9411.005,9541.69555,2902.9065,21978.6769,3161.454,9704.66805,8444.474,11187.6567,20277.80751,3366.6697,11436.73815,4747.0529,1917.3184,13041.921,18806.14547,2457.502,7337.748,4922.9159,1727.54,3756.6216,7954.517,25333.33284,11362.755,43813.8661,5969.723,1769.53165,2438.0552,13844.506,5261.46945,9634.538,10923.9332,6610.1097,34838.873,24393.6224,3410.324,12957.118,3260.199,28476.73499,8116.26885,8978.1851,4661.28635,19107.7796,12574.049,2866.091,8334.5896,16884.924,13747.87235,47496.49445,4357.04365,5836.5204,5472.449,21797.0004,8823.98575,19496.71917,13112.6048,2690.1138,4667.60765,10602.385,4428.88785,11512.405,11830.6072,3393.35635,2719.27975,46200.9851,12982.8747,18955.22017,10564.8845,8534.6718,2254.7967,4571.41305,11326.71487,6551.7501,11520.09985,2045.68525,48517.56315,21659.9301,4438.2634,36950.2567,2527.81865,4687.797,5125.2157,7448.40395,30942.1918,8334.45755,3972.9247,6196.448,16657.71745,27724.28875,6289.7549,39725.51805,11743.299,7624.63,2205.9808,6653.7886,14455.64405,47928.03,1712.227,4889.9995,2585.269,12629.1656,40419.0191,25678.77845,35069.37452,7682.67,1909.52745,2643.2685,10704.47,8988.15875,4320.41085,24603.04837,5693.4305,12730.9996,11286.5387,13831.1152,8062.764,39125.33225,15006.57945,8871.1517,3943.5954,32734.1863,4670.64,8798.593,8083.9198,11345.519,9432.9253,7441.053,2026.9741,7045.499,1632.03625,37607.5277,13462.52,2102.2647,2104.1134,6414.178,11363.2832,4562.8421,4719.52405,6067.12675,5257.50795,6666.243,1984.4533,3935.1799,23401.30575,6600.361,1627.28245,8124.4084,3062.50825,1727.785,3180.5101,43753.33705,30166.61817,15555.18875,4234.927,18259.216,12129.61415,5148.5526,11737.84884,5594.8455,3292.52985,2457.21115,35595.5898,24667.419,40974.1649,24520.264,10043.249,3172.018,36910.60803,6799.458,3866.8552,10141.1362,6079.6715,1744.465,19040.876,14313.8463,4266.1658,12146.971,3989.841,4561.1885,10085.846,1837.237,47269.854,23045.56616,12981.3457,3925.7582,3558.62025,5245.2269,4185.0979,11272.33139,5729.0053,11879.10405,16069.08475,63770.42801,8280.6227,11082.5772,4837.5823,8527.532,4074.4537,5031.26955,11090.7178,1532.4697,34672.1472,2322.6218,6858.4796,13224.05705,52590.82939,8059.6791,7526.70645,7152.6714,8944.1151,12029.2867,22218.1149,3046.062,1748.774,43921.1837,1674.6323,4137.5227,9447.25035,7650.77375,2261.5688,9861.025,5979.731,7147.4728],\"y\":[-818.0912725610324,6666.581124375403,17055.057906856615,2394.5612964577376,7798.843356927071,7703.498098656397,8196.638724484405,16329.802384766983,1007.5897163084737,8425.064913728573,16668.471772219717,11305.190631483081,8825.955508475548,2457.7569156477657,8131.697484426026,10795.20798910546,32427.310839130725,29849.8044318271,26537.45534002943,3934.8387107352464,13345.568173843923,11931.256905767641,15403.503119238063,13227.67590691222,15160.294264005155,6593.8014926373435,11706.895773164691,-423.19267644996035,7361.849867796409,10063.11894272617,27469.987637547776,37011.098173164806,12771.706594456198,9610.70419944053,15651.327489103587,14758.402230110318,1566.8876331943975,37044.69132925247,3478.7609709620174,3832.7287161810455,6359.78148646413,351.91281622400857,11920.33196948055,14263.518688355194,1823.2834779368204,2012.2178554201055,13140.841450051936,8076.918484203823,16958.97354519885,12707.762416337639,9842.451828831887,999.2052617776608,6694.565547106315,9095.48348252962,17216.491414729506,30721.894603649962,8675.13362217076,4895.762452972991,4318.193834880283,7408.636077638461,11113.148486904574,3055.050058929486,31155.422624871026,25221.426188712438,29796.520023320278,2154.8883968169084,1986.3879364647692,39024.51765615588,12434.631071129152,3312.071709460699,2966.5065864627068,9048.7313159433,6222.916731547433,40584.26519839457,7698.262253923516,3643.050822551995,6368.077720734473,1718.4966157038325,27122.11634974945,15189.60172614384,4897.791375824221,12036.376765950183,31923.28679977577,6616.578247796242,9210.305715393413,14366.647609690055,38063.354429988045,-1761.6939507890947,9672.427856210848,27625.82305866806,5709.097546724894,11645.986853762908,5192.639653544935,14713.301358733079,5172.01134707774,14014.284372727525,11333.224217688701,27172.587665865125,7981.395478117836,11324.11853980882,9218.10250583839,11022.254305842667,14442.421141382052,4319.101631875827,6497.1559038544765,6040.775600877558,1645.344814079932,1778.7532747376845,3149.7357162834887,3453.9536506266923,13294.84012265889,23198.979638802586,15618.969092968888,3403.0804988970285,29289.113393710424,7476.116202844729,8969.71509089092,36909.010209922075,8165.2584303598705,2570.7613142110695,538.6113941211152,12106.819604892004,1294.399808420465,14410.63681075305,7101.741737774973,1010.3612859394689,9526.259797617968,5849.9492563223575,7122.495296090776,15291.79919725204,13719.7840062541,3478.929993646214,13616.241010034206,3136.8539477339364,4221.372284199053,23742.96713315745,6674.595656753016,31874.896382968378,12628.75020700262,14214.966092365583,11657.602634879007,14852.526202816793,33439.98347304115,9478.076987955854,2720.7071237310483,4487.589092980026,40911.14461394752,11710.056176345584,2257.8025028838656,39005.20720272194,35286.95755566904,13574.084541767184,11847.856779445356,3989.183965824177,35442.74639362801,27367.379050601965,8165.0780377706105,11264.033387365773,9468.12021804677,13608.476731034207,32127.990912204317,24467.843403211962,39024.24566078081,9070.209526506947,3038.6545181954425,6534.350097498362,5517.99039889569,7368.9640193534,5008.109749328494,14590.777102493706,18717.389639103523,7027.082678870582,34251.02804172134,10656.196918421978,17211.006612592875,10693.322883750356,15445.21725619636,8069.307716606079,13653.264857235445,3175.3724338355223,12671.708792813144,6908.087813482132,15276.182783275137,14790.233792232,4355.803607435373,30027.771018738655,36209.44008876847,761.1569506652049,8310.888757359346,33971.20477693515,35604.343973984556,7902.841221887935,6597.802472506115,30702.85614559109,2556.8532474175663,12256.37849209574,9615.399946953894,2292.6407058079585,5480.838468931266,6520.717840854399,12120.286382244449,5701.761748351913,12298.467930488987,5045.301287689443,13751.917262213476,16576.611572187423,5381.5940835155125,4929.550450315823,30272.630163177426,15566.105130524396,-864.6774523000713,13663.980955520583,7751.901324277827,6601.076760760523,31435.55722489539,14167.01090678541,26093.840110676167,5397.272735184601,31592.54965452117,8230.60572554086,5489.245632357648,1655.0032906088854,7787.894352435331,10461.070384066927,36179.03574148043,3979.5674270289055,37478.57303557055,11623.834862525138,1187.9839153338744,27773.14018410352,39404.07741360817,16512.68236471776,3611.882397630252,32752.378204399087,7543.199062895095,4202.565024529351,8449.136212567617,13947.321199384973,11945.482584888276,2445.2072183306263,5777.825392794275,3661.0711243160877,5543.944256450173,4148.098784285408,2692.374817537431,7133.1103181164435,7252.348934128571,27493.45906394448,40716.77702854561,27083.252926250265,12486.655200935049,37823.803510527745,14772.597928877542,238.0885411026211,10161.608126654311,27895.70729462753,3354.6558319126707,9578.318131227388,10660.272962729206,10100.357605195306,30220.420801415734,39890.91484602388,8877.616197819985,17054.07138100649,11146.540407024251,36836.040297147556,11505.548239170043,-335.5160786761171,11324.128013282483,2571.0712284724923,9456.490711565679,35701.738204745365,14831.897896349597,13019.839836715477,37576.029346004594,10813.525542382016,11763.47556428087,3603.5092024900514,28184.892163539298,18420.391455849905,-273.7277003070012,32925.86427403872,2725.266975264991,30693.291329146683,4047.11358742624,6104.148086882571,29214.960925897867,35686.72598269266,4858.32078655115,9182.180508469763,6968.354227880431,-398.59275520761,28090.818983650403,4064.8950057177826,10649.972897747728,-1547.2722704907737,38042.4170606932,6741.6453869686375,8319.205835008415,29674.09138558407,3394.1701269041478,41428.3255163935,33894.039658953116,5799.792614234746,9550.796611802132,28532.981559021966,5178.1091993524315,7935.475456613392,2713.2356803548773,1923.8058366075165,4218.490483401678,1385.4161600968,2040.4086353763269,11107.174247283252,18173.989712427712,4631.5290679846585,2187.0742444931384,36450.291338740964,673.285762612426,12167.917455873525,27354.201111088216,11959.519650995917,8094.9736236200915,28051.324326172682,33907.98110655759,10728.076709629666,8807.120576727408,9723.245566565325,3472.9098185060793,11104.836849079584,5415.7750582584595,40435.200540857106,19381.36731466368,9471.761600669768,8001.397310373137,13650.413101908667,16750.70243420879,4526.734507685964,35465.86000869589,30119.147239042555,26428.569968190845,5474.855797703945,5760.905122530909,5595.15953259508,8708.28003106107,1453.244766093343,8230.510626547337,10843.774782784749,3083.39006318881,9651.995724996646,7859.435118394176,13624.267163293109,15182.791654799024,8659.840847820284,34635.92426577353,30683.72801672844,15079.021413920098,10436.193657335702,14082.75233737618,34259.388224631286,9637.99749745628,10933.33585050769,7237.1622967525545,35983.96184602719,8001.106704549726,35275.34357840211,13220.282737998175,12577.641572257977,10668.748673037859,32723.159385998784,13295.926082443286,7786.4296129404975,11220.459303055482,13345.74858436541,6043.842171472235,6698.912051847896,11151.204069700507,6826.009033552436,32776.485714307055,26690.76392978889,9261.051256684572,7641.587674989565,8209.150570880753,11189.672034366304,30079.047162288975,-1684.0350797820065,10653.693596700516,9222.372002525153,27524.83240405259,5490.244587598903,13361.33844623105,8665.613618429612,30707.39882558566,5500.767419363554,37870.46756768759,30869.29259483864,40361.944782025814,12658.934725806554,10526.345191617838,811.8861897829847,8569.004888733209,1534.6459499178163,4857.309873929618,15753.520690388228,13893.558104908763,12803.717507299472,5328.426118958359,4212.347110581708,12019.264243249407,39995.69223478321,14365.845284094248,98.63645235987678,-114.25062765525945,10428.633695537872,7191.595297243737,2720.7071237310483,15436.401586155698,13935.350437854631,3957.1502691157802,3866.8765642192157,12005.029598530422,10801.892364061569,15800.266547784482,7414.754770161911,8438.86440713979,13351.881456646312,4055.06298323151,12588.351693277167,5152.510003645439,9492.970374734508,30126.193240758592,146.84791861694976,6443.239043869813,11500.48862524398,11492.391376079016,2497.9125301833083,32753.641328816855,-366.111588000942,32444.504292701902,17887.40830701369,3245.2884387643207,13699.289809849382,3173.2025722221097,6276.239405948578,17157.455995543038,16162.601510877765,12004.51266459893,12162.304724731639,38310.873752743275,9449.92116601862,11428.121289853036,2135.5640927012646,4103.75842921697,34272.5659131688,27042.372391215045,7003.185649289771,2541.4163094789765,-918.2914305142913,12085.851338692788,10690.394120368757,9650.030750785016,8380.67667952496,2005.6394238745925,11069.421478667238,5796.839782648494,11713.086885352648,11504.486204655186,10882.193662450398,10692.935961434116,31649.540310841534,12538.367158237896,7333.3728268098785,2680.9506884012044,26907.690227125902,7200.432662096897,8932.28571092509,27846.02234894225,3296.2719290448476,17722.12601742483,10885.136151196988,9343.532741594245,14663.210880283212,37636.145585785904,12631.465451090182,11652.041279080011,10323.725124848956,9980.868055748128,13059.173660643137,5004.33108185044,13549.053992490186,406.7774891510344,13090.761816019145,34604.85093984531,17995.442132430617,5444.399537631147,9533.406982977958,4748.107344156364,16984.720983540803,7796.729677921976,39908.672611084316,23691.54945351738,26519.024047744446,7898.167312778809,25164.09249142588,11667.902016353539,4557.274499977193,32141.14375957797,27966.62669712281,15906.193003386383,13004.016495970289,36109.06782376686,24951.759741870563,12054.977606407352,4387.87924187745,27671.777377534432,25916.452949494487,5911.571836833202,13545.498156912494,30633.471325446764,26445.667105269204,35529.3113210876,15103.447771272906,12709.722186899166,14371.089213425155,10655.676584032924,7601.033567571412,31778.166435072955,11985.294303893887,8218.27084349162,15342.47021439498,5641.417391415613,5791.26030634424,5032.779513426973,4900.294235387184,34229.65364230001,14825.730455324978,5758.779139137572,7961.636115276648,8816.477424535857,10979.975212568763,7199.822558023945,15903.662675161206,4656.681787876081,3518.686428976058,9773.830940256186,2759.594615434886,3034.2818407655122,11919.99200974853,4199.683113217521,8585.379353044525,8253.579697820416,9709.93644512198,12380.049903944033,10882.33757506228,2380.7377496441313,4197.987321398599,32827.296120788764,9474.496246097115,33004.28072305555,4039.689106634294,16574.129141029654,34887.48017900891,15418.563588214522,4056.906628410121,4759.157424907571,3697.906039937585,16820.13744176326,2450.86856984083,15428.607942062088,36043.687147774894,14088.863198751169,6297.53295409941,6819.760568755635,36042.431564848775,4092.6573701641755,7222.506271488062,10471.429587574195,14853.955804024921,5090.304056656903,10529.993990415762,4682.252867278363,3754.759014361578,11532.14781140886,28357.92880426204,27856.63349818567,30918.552699321313,16921.296077195264,8171.61465689915,7230.981981796707,14746.306213601405,6554.545718632173,8789.225022386127,3538.825297958563,11590.519774828637,1613.4186753227477,9496.310128637,4848.674596478209,28552.721123482057,4800.160584949677,14488.243941477045,9529.980496570755,8045.447976169115,5675.208538239778,12407.138487059674,15169.96750776677,12302.705563373267,882.5841492535474,27272.90170889189,5679.863073793713,32109.73053903492,118.35636137398978,9956.622545377038,16590.834596025124,33428.54160282257,34998.491534240835,5656.051752782172,9154.842272434049,3333.741679388706,4016.3715207155838,4227.414580759549,1907.088095156032,8094.317440436578,12627.99808674919,40064.750838492735,3612.9153211576922,10374.00882263435,12450.13360003254,37208.10391612489,32080.633021883048,27401.89715729232,11901.418655885798,12713.346759793021,28278.88867187235,1470.3612116097029,13511.444219935096,10068.80436244102,5123.205669925901,40488.13942449111,6225.697456085338,29542.91009196361,33740.19641179823,2024.9143920138577,8410.514929406803,-222.1519575503262,1301.3769440308515,1666.1520710974419,12130.081355210365,29100.28632072878,16300.426103707621,9204.851810161148,17440.49072028627,36242.47925827432,14793.135549109884,10916.85193494373,13005.235724235747,15010.389358855558,35945.19508289884,8295.979050475089,16254.036869707452,10416.280092525834,1699.6252219105372,26817.647138093198,11718.748633904164,31797.70863569068,2571.8700106756205,35525.72166081547,-1303.182342875254,14708.575727273092,34964.73349931919,5285.826321227138,5463.963325676876,13059.580062773719,14318.66391358,2343.832057339958,39089.717939558584,5512.391711685548,7783.073621981817,15723.761266893092,4419.473896877429,14498.573117445943,31162.445080740436,801.8313825813129,1956.6616396744103,11822.888467860546,16379.503632625645,15347.882181893565,33347.636384547965,28963.14708966038,6779.171827476577,14248.998162028813,26738.634928758176,15390.65617040846,1168.8748235274397,39932.2033977303,5337.631792549451,3898.109392173006,13140.120545269623,1225.8765956466686,15668.077315392184,32252.15913052726,7175.418414993277,12220.348732984632,41362.47870913999,6983.320060021862,6991.805698196629,31301.180107916545,11571.130402106224,9264.332788612228,9550.099775224466,15470.550368677046,9449.316519701973,40445.22466971571,14415.196662286991,3532.3325570500965,11751.24662564223,15843.563779420936,3701.5474225605794,32335.81390818357,2872.584100700449,14730.006813728549,8729.240011719368,13409.304970801735,5420.007823991607,6346.264754745538,15348.66780827468,5572.8605295790185,5397.319697768824,12485.277340010012,10951.18017557116,712.7705495751761,8885.032847463473,7411.39094805477,3638.662759499646,3531.8505357403337,10503.753098871759,12473.714738085982,13972.016076685733,37529.2440527135,6700.3899650969615,2694.8043880698933,10160.950218096183,23390.930981995713,6607.244201785143,13232.972030918674,12784.166620969554,12313.415684392457,28838.851260898933,34726.4794215866,8629.598561433973,10148.456721319588,1547.288852927044,12911.037381950504,11097.04568889258,12960.294927557028,5037.1221633185305,28683.419461309575,17403.983463138517,3339.4010724802883,11248.526588638826,25510.75452600982,23432.205336012456,40834.07075174333,6170.218248394463,9286.087514694103,5652.410370159174,14819.685785372327,11624.25072019716,12304.218403726773,12114.702038789066,3220.1884830914296,8850.26567532825,15011.932845137198,3331.7059278377656,13268.455365687973,10589.711941105732,6467.507290921065,4035.4563409011425,38747.46643332536,16364.49141057294,2422.3056657155794,12712.625855010709,8060.829522390821,3655.3857046012345,5397.248666979951,10117.324286118317,10266.108062282263,11591.973523841316,1668.5639005475514,40848.155539286745,32284.950211995347,5637.868799511172,30652.427493326148,257.8238357391565,4503.140910674832,5389.045474404389,12544.123608741622,39114.51080875467,11216.045421432158,4888.961709036765,10485.056639911421,27320.367175096595,4209.12807045244,9428.013498077475,34825.85759849807,10939.206078700445,7749.641821427194,2932.9175921574824,9490.176590781375,25576.201255125074,40131.468990549,1485.3149427791177,8437.34157380298,-1250.2158652623202,11687.399362000702,33773.55659408876,35602.98891485686,31080.491445274856,10254.585687801147,3480.930832575428,4394.0531825260405,11673.096878241478,10764.452659188311,8503.587858080573,8005.435958152033,4134.442728345246,13065.055255610983,12076.159373968378,17085.39581174609,10493.040150067955,33740.60740231912,26102.47836197727,10894.312606416177,5603.773715691532,27624.03360676459,8102.367392991677,12551.500822490263,15577.362592436359,9732.36898865753,17143.12038194907,9242.179862891277,5568.794413137912,7045.832771415444,869.2805929509905,31901.515753588297,14461.240225349273,1440.0879030023207,1894.8242438956677,7229.842069861043,13982.438294349413,5728.833757912384,3658.5533514553827,7265.677904267941,5632.809185585109,12673.165970067974,6110.797549216957,6948.805700140416,33610.20071218268,8421.682323473673,-300.0414264890496,11509.36882644955,5216.000548140735,-2260.5220079138817,779.6576314138583,37021.51520445398,13310.337784411153,19956.541103469324,5119.7134644210455,28809.145016595867,12760.675714758509,7147.195105659848,5831.585340919119,8264.711653777573,6336.755773901481,2393.8946127351555,29099.92553555026,34522.83702510616,34898.99297263188,34281.65667775094,12284.102164039938,5471.077477233863,15437.434982007813,8019.943010089653,5599.607113596125,12100.336272340013,10351.388788901371,1842.3622708230905,29303.15045981807,15821.675766925771,10534.010752948263,12179.903857132489,9169.472632885503,5194.884243097826,13390.853453527787,400.37853364383045,39886.02227843117,11634.168905645161,15988.393685022049,6136.528177829765,3024.6046432737585,8107.727512822683,3575.7790309218835,2034.5660219108722,9641.149615096263,13567.364302093494,16384.540635188256,41386.83001998656,9904.001467320202,11356.103090974735,6823.100322658131,12100.804296902992,5600.992218194417,5884.972264594384,15006.595652763055,3936.5619712163825,28803.229915500327,5231.0910446555645,6668.271021977787,15364.056598169642,37831.35684704209,9786.987803347212,6243.884997746645,11543.638845102148,15298.239156009746,10010.521535832668,32649.3295338772,2712.5388437772126,2902.2740428301404,37935.94753402208,3601.2333795338163,4765.841799863683,9948.386888541463,13157.03301993516,3720.1518637615773,9614.63620581917,9162.120532339148,10264.906578229129],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"markers\",\"x\":[7281.5056,5267.81815,12347.172,24513.09126,3736.4647,7358.17565,9788.8659,17085.2676,8211.1002,19798.05455,3077.0955,3385.39915,6837.3687,8538.28845,26392.26029,13012.20865,3227.1211,15170.069,11073.176,20773.62775,39556.4945,2134.9015,2198.18985,6555.07035,4340.4409,12622.1795,7740.337,12475.3513,3987.926,21082.16,1241.565,40103.89,17929.30337,8302.53565,3471.4096,5846.9176,13352.0998,9144.565,25656.57526,7726.854,13887.9685,30284.64294,5266.3656,12797.20962,1146.7966,7046.7222,8627.5411,39597.4072,12323.936,11454.0215,40904.1995,3171.6149,7445.918,13607.36875,27346.04207,12557.6053,10797.3362,5488.262,6282.235,40941.2854,1708.0014,23306.547,28950.4692,1664.9996,17361.7661,7345.084,18157.876,7256.7231,7626.993,26236.57997,7325.0482,1720.3537,7153.5539,6986.697,8232.6388,10370.91255,4889.0368,6474.013,1625.43375,10115.00885,10264.4421,9386.1613,18223.4512,3561.8889,23887.6627,3392.9768,1135.9407,1880.487,5630.45785,10156.7832,22144.032,62592.87309,9500.57305,46113.511,13390.559,4076.497,6389.37785,9880.068,4751.07,12629.8967,6664.68595,21344.8467,22395.74424,4466.6214,9249.4952,8703.456,38245.59327,36189.1017,19023.26,5080.096,9957.7216,6313.759,4151.0287,17748.5062,37133.8982,21472.4788,4350.5144,41661.602,6985.50695,23241.47453,39983.42595,9174.13565,6710.1919,1633.9618,47055.5321,2899.48935,12231.6136,5138.2567,4646.759,12913.9924,24476.47851,27808.7251,18033.9679,25309.489,6360.9936,20296.86345,10600.5483,9583.8933,34806.4677,5708.867,6457.8434,21259.37795,12928.7911,2904.088,3537.703,36085.219,10713.644,21232.18226,8457.818,18838.70366,3500.6123,6753.038,1256.299,45710.20785,11289.10925,1141.4451,12644.589,1121.8739,12495.29085,27218.43725,13129.60345,1837.2819,6311.952,12333.828,18218.16139,48173.361,6781.3542,3994.1778,1621.8827,11165.41765,9964.06,7441.501,5240.765,4686.3887,18972.495,6393.60345,3732.6251,19144.57652,7633.7206,11093.6229,46889.2612,11931.12525,4454.40265,1149.3959,34779.615,3277.161,27533.9129,10959.33,19594.80965,2483.736,10065.413,7512.267,19442.3535,7209.4918,11299.343,10231.4999,14711.7438,4906.40965,13415.0381,6123.5688,5469.0066,1705.6245,12646.207,6435.6237,11482.63485,44400.4064,17663.1442,14590.63205,38711.0,2166.732,8116.68,38746.3551,5584.3057,6775.961,55135.40209,18903.49141,5920.1041,3378.91,13430.265,11353.2276,5253.524,4738.2682,2196.4732,11987.1682,1515.3449,17178.6824,12268.63225,14133.03775,7421.19455,36021.0112,5966.8874,17081.08,6059.173,37484.4493,10338.9316,2855.43755,13616.3586,29523.1656,1607.5101,9504.3103,25517.11363,18310.742,9869.8102,38415.474,11554.2236,12609.88702,10594.2257,3238.4357,14394.5579,2585.85065,2927.0647,14254.6082,40932.4295,4883.866,7418.522,9630.397,8606.2174,5003.853,2680.9493,9549.5651,3847.674,24873.3849,5699.8375,42856.838,3201.24515,22493.65964,11566.30055,7160.3303,1646.4297,9617.66245,2203.47185,1137.011,9487.6442,7731.85785,7443.64305,2523.1695,34472.841,1969.614,7789.635,41676.0811,1694.7964,1906.35825,11763.0009,9225.2564,21098.55405,2709.1119,4992.3764,6600.20595,18608.262,2459.7201,8252.2843,4441.21315,5425.02335,4058.71245,15518.18025,4544.2348,19199.944,8347.1643,5478.0368,14007.222,42303.69215,5377.4578,1628.4709,16420.49455,10436.096,9778.3472,15612.19335,7261.741,11015.1747,10493.9458,6203.90175,10197.7722,6770.1925,1972.95,9304.7019,29141.3603,46255.1125,11013.7119,8162.71625,12244.531,5662.225,5383.536,12094.478,14478.33015,3208.787,7639.41745,2897.3235,29330.98315,11534.87265,1137.4697,23568.272,13429.0354,9222.4026,36397.576,3956.07145,10965.446,9377.9047,2801.2588,44585.45587,23967.38305,13228.84695,2352.96845,6334.34355,11938.25595,6338.0756,20462.99766,8068.185,4949.7587,9863.4718,39871.7043,19361.9988,6184.2994,1728.897,7323.734819,14418.2804,5246.047,38344.566,7144.86265,2207.69745,3579.8287,10325.206,8310.83915,3481.868,4399.731,5385.3379,4766.022,16232.847,13974.45555,10795.93733,3021.80915,8615.3,9361.3268,35585.576,7147.105,4239.89265,11735.87905,17128.42608,5375.038,10791.96,3443.064,5415.6612,7727.2532,18765.87545,6358.77645,5002.7827,14449.8544,2597.779,13047.33235,51194.55914,2331.519,11946.6259,8964.06055,7160.094,13725.47184,8825.086,11411.685,8930.93455,24106.91255,17878.90068,22462.04375,1391.5287,8240.5896],\"y\":[8662.892823683971,6261.494308989844,15331.327091948988,11299.024976375411,4104.16941973786,9577.526195656203,9565.69869909524,25909.471812969,7622.0820015341815,31079.90834770562,2143.3454503388566,3992.4612279282774,7301.498825377101,11325.651698053058,12904.173104348221,11681.325753562305,5368.230275723967,18548.6139585504,10811.231470787665,31050.837965190316,34319.100971014515,4558.083779589426,1016.5287269640885,7284.109196552927,4224.262463119572,13915.947859276865,11949.123967511274,11048.407864424415,6440.1743845087985,31312.005502884887,-1854.988591396048,34319.56807902654,9330.376795054173,7464.6255661927535,7848.100487560056,11843.55204844015,13982.21419604175,12715.356484720694,9899.510180600751,8632.625842198886,15856.12617816763,10421.930599618965,5904.2014697165,11419.135617247724,5055.377063929358,7227.208500693167,6642.779276837526,34553.35748726812,11109.60679166541,11167.794519280236,32900.13851257704,5252.421989334729,10438.854444189561,14785.974019995134,11033.07922848632,13239.720927187329,12215.90115198358,7650.711097081767,7697.798953576552,35221.44550449744,445.9175921657534,33667.347645702685,37805.54084388565,1231.8177085632124,28599.361634795277,9897.777446645952,6321.081137949754,9810.544873196006,8330.883760914934,7967.683189536016,7254.9677832487505,1871.0034494104657,8695.064076629216,10128.77811371717,8819.456393259825,12907.822772512136,6293.160738827981,10739.647850544177,-754.7777673823948,13438.446384105908,11906.557096198565,12694.047550947435,26755.482191881827,2215.328103722961,34514.61378004331,4243.096460384531,2385.0832125180423,3305.464621320227,9662.50967076284,9686.71661460508,32078.317336424094,32881.184721219775,9521.167199922245,38270.44955627255,13628.810508862407,6223.097993502684,7468.251166152018,14298.762424043882,6680.474277871166,10844.548344401112,8449.264613897558,2315.7754152307953,2862.508721501843,7401.472762606769,10754.945464261196,9795.030404844569,31229.333697519833,30082.469313755435,30628.06955472275,4216.463886757643,10631.795402233034,8088.89351555772,6474.710486387703,26732.34234326944,30319.473678443363,32114.627029443218,5349.576671048264,35609.2947635521,6408.7315985858695,3376.9329676145644,34624.291087453086,11603.68857736771,7198.894633571566,4882.340423206617,39294.104786667966,5271.972621558449,14364.105768624697,9317.19817203347,6701.688509692212,11983.669542863841,11850.792042450921,36297.175843000325,27603.166416824846,35184.71501840801,12794.168499955387,30751.036016954138,12952.532878357444,13035.20560027345,28962.96669707112,5527.484119434037,8556.525224706096,32094.295974488487,14010.511760989975,4111.670843467444,4566.687118268805,30035.430223694042,13929.683231546813,9222.105543005902,12011.547434708948,3932.1553125179416,7254.857233515613,4620.963399867596,1769.2258548216123,38727.95557349546,14102.530593681837,3739.035024501245,14578.008031084919,-1075.0158625501354,15953.064112631237,36356.41937688783,11285.169490789773,6650.784317809244,7763.916836524208,12049.019668959409,4766.473914842543,40258.89671196146,10345.660261452314,10116.720094194123,1911.168391354593,10732.578657755526,12190.891428153082,10144.114593786053,7129.708419423532,11856.67151500284,29448.85654958847,8507.648516765381,595.0955343191927,12121.285337485708,9866.211302175856,15721.181331309745,39146.82505776084,11005.584858499897,5856.527687867871,5694.743197365868,27591.632356345246,7072.066246784525,36168.5497187689,9602.704028749908,30797.085037772726,1285.2242876844575,8364.820212074992,9281.109274396571,7555.365491803124,10010.706305629974,13891.91416075742,11915.754376864257,24790.05074241489,3351.2450472213077,10940.837339354457,7169.342229746493,6299.446080643469,-138.74341755426576,15998.681917939968,13803.023774542198,2851.4127052318145,37390.716045207446,28013.186077279373,15861.502205979086,32530.05294866379,4761.469122433751,9727.868228464413,33377.74288233935,8785.307610738473,9138.563462490354,32736.01281652013,6107.386764525592,9502.628851263464,902.4998364793428,15029.737731175854,13169.774025354163,7212.189904333205,7524.920076147395,594.2735532774077,14273.599776286337,-275.73255495357444,26027.769949164278,14565.881117671637,4355.803607435373,8652.765800234018,29353.896252085455,5883.397186548686,25647.517335301047,7614.161254557832,30275.968982596736,11540.17785506841,5273.080448657005,11011.270278214746,37811.0114483326,-1624.1502288237662,14493.178303554501,10651.530696869573,28455.559198095274,11194.590266772051,32870.661427296625,11547.100769619068,3367.8126950036967,12885.436167625372,8151.353444800545,13026.4066473797,386.6406774672705,9643.89148626463,12837.901900432207,33976.86108566258,5140.769063582509,10483.758095316174,11906.536871209602,9598.87540690571,5666.606068926398,168.2627680162168,16165.999486755092,3138.9162513961346,35235.39096781927,8588.141449201856,36874.17581466304,8144.580025934789,2812.2930985878556,16131.030845607878,10910.411041702839,4409.727818477768,10267.924131414176,2315.7754152307953,2648.3516204036678,10393.71309504981,11712.573949206293,11657.428723981488,3279.876388310804,28205.76645356055,708.0155382193116,9136.793794165722,35407.35820503719,-2802.199128501019,2701.3828196154045,13458.067939734014,9400.749726022685,32563.297005571418,1534.4064659877295,4337.419185152383,9877.32814389238,29456.346206371672,3011.036789661841,7713.663706567433,5010.909513395953,7922.886430785893,6957.547618992081,24666.489689865833,2765.1662961413094,29588.43332063684,12728.317569600033,6907.374152373068,15128.499432837089,36408.22484821014,8968.55229868335,-7.710921629039149,27685.65618727544,11298.122762452876,11416.61784438702,17110.05125002204,11044.829227049377,12272.774032829788,10992.541734417297,7867.842281820562,9003.19436190508,7600.146864930823,1528.5923939667082,13301.942241866062,37987.27098403463,39639.672844835055,8992.61520459699,10869.987432707327,14226.268513045885,11154.299482374143,6152.29443449953,10685.00667546728,9446.550658803513,977.9790905085702,10363.652640161738,3941.5487684366235,38039.48744587783,8400.513050420966,2761.1809380689338,33607.28331870615,14383.841063261236,11073.994708558203,29064.149111050945,5971.991491251963,11107.094930953463,10663.119832972636,3478.0831714405667,31723.67179168362,34811.48696489789,15519.574088396046,1681.7207636385756,10292.010797943418,12759.567887659949,8870.005239788348,15448.196224920335,11826.47754065747,8483.64484578431,9635.494240852004,33658.17210703445,30028.6195518917,7377.286805735002,-1986.9963893314161,1990.9637431641186,17248.291954630113,8428.955107690239,30659.25925948345,8093.209613338018,3355.172765844163,5799.667469770626,14593.406655944222,10654.162208738708,3977.5785275477356,5939.660563779478,6180.446348103887,2323.015090674813,25293.7574513335,14481.480740134233,4138.9386312395945,5929.282224335871,9547.457727265635,8198.636625114555,28664.441282524276,6193.328735043204,4940.4765745356635,12429.285611146319,7294.71824131265,7860.689638704265,8931.866452795508,5915.540305256989,5425.761193261154,11371.676650666992,29719.22384872537,9075.913985751355,5283.835209877783,11414.201147785785,1826.947343790409,12392.18475589026,31150.74692984495,1977.9058518488637,13205.09012335621,11099.277615696512,10971.789298917058,6199.48480616276,10401.101675573547,17571.53354111893,10715.885865509019,34591.93506471054,8013.54753000956,32499.74461692023,2917.9387509157496,10775.164566124447],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predichos\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predichos\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de entrenamiento\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de prueba\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"shapes\":[{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":-2260.5220079138817,\"x1\":41428.3255163935,\"y0\":-2260.5220079138817,\"y1\":41428.3255163935},{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":-2802.199128501019,\"x1\":40258.89671196146,\"xref\":\"x2\",\"y0\":-2802.199128501019,\"y1\":40258.89671196146,\"yref\":\"y2\"}],\"title\":{\"text\":\"Regresi\\u00f3n lineal\"},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5b62a84b-2db1-4672-8494-573d92586afb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la gráfica se observan 3 grupos distintos de puntos, esto nos indica dos cosas: \n",
        "\n",
        "1. Puede que alguno de los supuestos del algoritmo no se cumplan.\n",
        "2. Existen 3 o 4 grupos en los datos. Posiblemente aginando una nueva variable por grupos pueda mejorar las cosas."
      ],
      "metadata": {
        "id": "-owHOIlQj-1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primer Red Neuronal"
      ],
      "metadata": {
        "id": "BDGKZfYhCa7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La arquitectura de la primera red neuronal será muy similar a una regresión lineal, la única diferencia radica en el algoritmo de optimización de la función de pérdida."
      ],
      "metadata": {
        "id": "Ccm5QUM_tRJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación de los datos"
      ],
      "metadata": {
        "id": "5ACuSJgItM-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ocuparemos el mismo transformador que se creo y ajustó cuando usamos la regresión lineal. Los datos *preparados* nos van a servir para ambas redes neuronales."
      ],
      "metadata": {
        "id": "HqpMILowtPK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_transformed\n",
        "X_train_tr = transformer.transform(X_train)\n",
        "X_test_tr = transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "tKkElOVUm4Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "olNiQEns6k-l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "970790ca-fa1f-4147-a051-ec52d17c84ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      age     sex     bmi  children smoker     region\n",
              "7      37  female  27.740         3     no  northwest\n",
              "999    36  female  26.885         0     no  northwest\n",
              "1209   59    male  37.100         1     no  southwest\n",
              "491    61  female  25.080         0     no  southeast\n",
              "625    29  female  26.030         0     no  northwest\n",
              "...   ...     ...     ...       ...    ...        ...\n",
              "854    49  female  23.845         3    yes  northeast\n",
              "554    25  female  41.325         0     no  northeast\n",
              "1278   39    male  29.925         1    yes  northeast\n",
              "374    20    male  33.330         0     no  southeast\n",
              "6      46  female  33.440         1     no  southeast\n",
              "\n",
              "[402 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cad1f1d9-4df3-4c11-86ff-e6a65c3aa235\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>37</td>\n",
              "      <td>female</td>\n",
              "      <td>27.740</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>36</td>\n",
              "      <td>female</td>\n",
              "      <td>26.885</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1209</th>\n",
              "      <td>59</td>\n",
              "      <td>male</td>\n",
              "      <td>37.100</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southwest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>61</td>\n",
              "      <td>female</td>\n",
              "      <td>25.080</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>29</td>\n",
              "      <td>female</td>\n",
              "      <td>26.030</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>854</th>\n",
              "      <td>49</td>\n",
              "      <td>female</td>\n",
              "      <td>23.845</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>25</td>\n",
              "      <td>female</td>\n",
              "      <td>41.325</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1278</th>\n",
              "      <td>39</td>\n",
              "      <td>male</td>\n",
              "      <td>29.925</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>20</td>\n",
              "      <td>male</td>\n",
              "      <td>33.330</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>46</td>\n",
              "      <td>female</td>\n",
              "      <td>33.440</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>402 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cad1f1d9-4df3-4c11-86ff-e6a65c3aa235')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cad1f1d9-4df3-4c11-86ff-e6a65c3aa235 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cad1f1d9-4df3-4c11-86ff-e6a65c3aa235');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tr"
      ],
      "metadata": {
        "id": "PaXam2uL6gVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ab6f26-3cf7-42b7-d52c-eb6a70eb7bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.   ,  0.   ,  1.   , ..., 37.   , 27.74 ,  3.   ],\n",
              "       [ 0.   ,  0.   ,  1.   , ..., 36.   , 26.885,  0.   ],\n",
              "       [ 1.   ,  0.   ,  0.   , ..., 59.   , 37.1  ,  1.   ],\n",
              "       ...,\n",
              "       [ 1.   ,  1.   ,  0.   , ..., 39.   , 29.925,  1.   ],\n",
              "       [ 1.   ,  0.   ,  0.   , ..., 20.   , 33.33 ,  0.   ],\n",
              "       [ 0.   ,  0.   ,  0.   , ..., 46.   , 33.44 ,  1.   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo/arquitectura"
      ],
      "metadata": {
        "id": "Ob7LhoEfstFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La arquitectura sólo consiste en una capa de entrada y una de salida con activación lineal. \n",
        "\n",
        "Al inicio ponemos un par de semillas para asegurarnos que los resultados se puedan replicar."
      ],
      "metadata": {
        "id": "BjreDrilumtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tr.shape[1]"
      ],
      "metadata": {
        "id": "TPqnn-AfVUIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebc50fc-2408-49ca-a2d9-fae2ed0788ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "tf.random.set_seed(10)\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "     layers.Input([X_train_tr.shape[1]]),\n",
        "     layers.Dense(1, activation='linear')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Xic5bz16nbWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos el modelo indicando que el optimizador es RMSprop, la función de pérdida el error cuadrático medio e indicamos que monitoreé el error absoluto medio."
      ],
      "metadata": {
        "id": "rSl0QvNEu2S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = keras.optimizers.RMSprop(learning_rate=.1), \n",
        "    loss = 'mse', \n",
        "    metrics = ['mae'], \n",
        ")"
      ],
      "metadata": {
        "id": "NsMYYB4QpDp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "SlmylUUjqGf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34945d91-ce8e-4e61-dbb6-95878618b8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9\n",
            "Trainable params: 9\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parámetros\n",
        "\n",
        "Algunos ejemplos de otros parametros que podemos utilizar son:\n",
        "\n",
        "- Optimizadores:\n",
        "  - SGD\n",
        "  - RMSprop\n",
        "  - Adam\n",
        "  - Adadelta\n",
        "  - Adagrad\n",
        "  - Adamax\n",
        "  - Nadam\n",
        "  - Ftrl\n",
        "\n",
        "Más detalles [aquí](https://keras.io/api/optimizers/)\n",
        "\n",
        "- Funciones de pérdida:\n",
        " - Binary Crossentropy\n",
        " - Categorical Crossentropy\n",
        " - Poisson\n",
        " - Mean Squared Error\n",
        " - Mean Absolute Error\n",
        " - Cosine Similarity\n",
        "\n",
        "- Métricas:\n",
        "  - Accuracy\n",
        "  - Binary Crossentropy\n",
        "  - AUC\n",
        "  - MAE\n",
        "  - MSE\n"
      ],
      "metadata": {
        "id": "MZa6pUX0BOJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el número de épocas, la ruta donde se deben guardar los checkpoints y creamos el callback que se encargará de guardar los checkpoints."
      ],
      "metadata": {
        "id": "M5NeBPZAvKjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "\n",
        "# weights_filepath = os.path.join('/content/drive/MyDrive/checkpoints/', 'model_1')\n",
        "\n",
        "# model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "#    filepath = weights_filepath , \n",
        "#    save_best_only = True, \n",
        "#    save_weights_only = True, \n",
        "#    monitor = 'val_mae', \n",
        "#    mode = 'min'\n",
        "#)"
      ],
      "metadata": {
        "id": "yCog_zC0tCHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El callback para los checkpoints sólo guardará los pesos (```save_weights_only = True```) del mejor modelo (```save_best_only = True```) considerando el error absoluto medio del conjunto de validación (```monitor = 'val_mae'```) más pequeño (```mode = 'min'```) hasta el momento."
      ],
      "metadata": {
        "id": "lySolqICvVtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "yGCdX2_qs-9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya que esta definida la arquitectura del modelo, se compilo y se crearon los callbacks necesarios, usamos la función ```fit``` para entrenar la red neuronal."
      ],
      "metadata": {
        "id": "jTKapDAMwByU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_tr, y_train, \n",
        "    epochs=epochs, \n",
        "    validation_split=.2, \n",
        "#    callbacks=[model_checkpoint_callback]\n",
        "    )"
      ],
      "metadata": {
        "id": "A5JFd_alqLJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216ae1a3-e411-4965-ec80-33745a4ef6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "24/24 [==============================] - 3s 9ms/step - loss: 337746464.0000 - mae: 13571.1084 - val_loss: 298195904.0000 - val_mae: 12156.6143\n",
            "Epoch 2/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 332252864.0000 - mae: 13382.3047 - val_loss: 293513088.0000 - val_mae: 11980.5703\n",
            "Epoch 3/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 327301600.0000 - mae: 13207.8291 - val_loss: 288939232.0000 - val_mae: 11806.2158\n",
            "Epoch 4/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 322428896.0000 - mae: 13035.0986 - val_loss: 284512512.0000 - val_mae: 11635.0654\n",
            "Epoch 5/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 317614176.0000 - mae: 12863.3105 - val_loss: 280186688.0000 - val_mae: 11465.4971\n",
            "Epoch 6/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 312897440.0000 - mae: 12691.9814 - val_loss: 275836704.0000 - val_mae: 11292.5635\n",
            "Epoch 7/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 308228000.0000 - mae: 12520.0400 - val_loss: 271586528.0000 - val_mae: 11121.1260\n",
            "Epoch 8/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 303601120.0000 - mae: 12348.9131 - val_loss: 267353712.0000 - val_mae: 10947.8848\n",
            "Epoch 9/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 299092640.0000 - mae: 12176.4707 - val_loss: 263219632.0000 - val_mae: 10776.0918\n",
            "Epoch 10/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 294671104.0000 - mae: 12005.5869 - val_loss: 259263904.0000 - val_mae: 10609.2969\n",
            "Epoch 11/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 290273984.0000 - mae: 11838.3447 - val_loss: 255204768.0000 - val_mae: 10435.9316\n",
            "Epoch 12/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 285852000.0000 - mae: 11668.8047 - val_loss: 251243152.0000 - val_mae: 10265.2881\n",
            "Epoch 13/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 281561856.0000 - mae: 11503.1377 - val_loss: 247343264.0000 - val_mae: 10095.9229\n",
            "Epoch 14/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 277302368.0000 - mae: 11340.3721 - val_loss: 243469568.0000 - val_mae: 9927.3262\n",
            "Epoch 15/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 273095072.0000 - mae: 11175.8984 - val_loss: 239711072.0000 - val_mae: 9765.7236\n",
            "Epoch 16/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 269027648.0000 - mae: 11020.0918 - val_loss: 236072592.0000 - val_mae: 9611.0488\n",
            "Epoch 17/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 265007264.0000 - mae: 10869.6162 - val_loss: 232387872.0000 - val_mae: 9453.4336\n",
            "Epoch 18/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 261012000.0000 - mae: 10717.5527 - val_loss: 228795104.0000 - val_mae: 9301.0850\n",
            "Epoch 19/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 257138672.0000 - mae: 10575.6113 - val_loss: 225394512.0000 - val_mae: 9158.1865\n",
            "Epoch 20/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 253347104.0000 - mae: 10438.1582 - val_loss: 222032960.0000 - val_mae: 9016.1035\n",
            "Epoch 21/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 249638272.0000 - mae: 10301.2852 - val_loss: 218606528.0000 - val_mae: 8872.3457\n",
            "Epoch 22/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 245880752.0000 - mae: 10165.2832 - val_loss: 215314496.0000 - val_mae: 8738.1611\n",
            "Epoch 23/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 242281744.0000 - mae: 10033.2529 - val_loss: 212192128.0000 - val_mae: 8612.1133\n",
            "Epoch 24/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 238762448.0000 - mae: 9905.8193 - val_loss: 209016352.0000 - val_mae: 8485.0088\n",
            "Epoch 25/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 235271216.0000 - mae: 9778.1426 - val_loss: 205905088.0000 - val_mae: 8360.9111\n",
            "Epoch 26/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 231797872.0000 - mae: 9652.0586 - val_loss: 202884048.0000 - val_mae: 8242.5137\n",
            "Epoch 27/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 228442768.0000 - mae: 9532.1426 - val_loss: 199911024.0000 - val_mae: 8127.6929\n",
            "Epoch 28/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 225145312.0000 - mae: 9414.8447 - val_loss: 196984528.0000 - val_mae: 8017.1729\n",
            "Epoch 29/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 221861488.0000 - mae: 9303.0693 - val_loss: 194104064.0000 - val_mae: 7912.0298\n",
            "Epoch 30/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 218699904.0000 - mae: 9195.7998 - val_loss: 191381328.0000 - val_mae: 7814.2778\n",
            "Epoch 31/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 215552528.0000 - mae: 9092.1777 - val_loss: 188574896.0000 - val_mae: 7717.3184\n",
            "Epoch 32/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 212470864.0000 - mae: 8989.3398 - val_loss: 185988496.0000 - val_mae: 7630.9692\n",
            "Epoch 33/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 209527904.0000 - mae: 8890.0312 - val_loss: 183402960.0000 - val_mae: 7545.6582\n",
            "Epoch 34/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 206613216.0000 - mae: 8796.3672 - val_loss: 180828608.0000 - val_mae: 7464.4316\n",
            "Epoch 35/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 203743824.0000 - mae: 8706.6543 - val_loss: 178416288.0000 - val_mae: 7396.0273\n",
            "Epoch 36/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 200980096.0000 - mae: 8620.5332 - val_loss: 176027984.0000 - val_mae: 7334.6641\n",
            "Epoch 37/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 198275920.0000 - mae: 8537.3447 - val_loss: 173700256.0000 - val_mae: 7279.7026\n",
            "Epoch 38/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 195529376.0000 - mae: 8456.5908 - val_loss: 171224736.0000 - val_mae: 7227.1079\n",
            "Epoch 39/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 192877456.0000 - mae: 8377.6670 - val_loss: 169109616.0000 - val_mae: 7186.7500\n",
            "Epoch 40/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 190301744.0000 - mae: 8306.1943 - val_loss: 166878688.0000 - val_mae: 7146.1997\n",
            "Epoch 41/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 187785056.0000 - mae: 8239.0449 - val_loss: 164769152.0000 - val_mae: 7111.8535\n",
            "Epoch 42/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 185340080.0000 - mae: 8175.9688 - val_loss: 162680160.0000 - val_mae: 7081.7788\n",
            "Epoch 43/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 182943376.0000 - mae: 8115.8828 - val_loss: 160742544.0000 - val_mae: 7058.4854\n",
            "Epoch 44/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 180641968.0000 - mae: 8060.2734 - val_loss: 158793024.0000 - val_mae: 7045.9351\n",
            "Epoch 45/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 178365232.0000 - mae: 8009.7197 - val_loss: 156969216.0000 - val_mae: 7039.3364\n",
            "Epoch 46/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 176264560.0000 - mae: 7964.7178 - val_loss: 155198624.0000 - val_mae: 7035.4741\n",
            "Epoch 47/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 174081616.0000 - mae: 7925.7397 - val_loss: 153367424.0000 - val_mae: 7037.8252\n",
            "Epoch 48/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 172026464.0000 - mae: 7894.6338 - val_loss: 151714944.0000 - val_mae: 7045.3140\n",
            "Epoch 49/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 170086944.0000 - mae: 7864.4478 - val_loss: 150150928.0000 - val_mae: 7054.4976\n",
            "Epoch 50/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 168148048.0000 - mae: 7844.1323 - val_loss: 148687280.0000 - val_mae: 7065.8130\n",
            "Epoch 51/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 166333280.0000 - mae: 7823.6128 - val_loss: 147155664.0000 - val_mae: 7079.7891\n",
            "Epoch 52/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 164534080.0000 - mae: 7815.0962 - val_loss: 145758272.0000 - val_mae: 7097.2573\n",
            "Epoch 53/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 162836016.0000 - mae: 7808.7104 - val_loss: 144400160.0000 - val_mae: 7118.3921\n",
            "Epoch 54/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 161214832.0000 - mae: 7814.8638 - val_loss: 143174176.0000 - val_mae: 7144.7075\n",
            "Epoch 55/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 159570384.0000 - mae: 7814.9741 - val_loss: 141857008.0000 - val_mae: 7179.3125\n",
            "Epoch 56/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 157989024.0000 - mae: 7827.6597 - val_loss: 140674528.0000 - val_mae: 7212.5371\n",
            "Epoch 57/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 156499888.0000 - mae: 7844.9492 - val_loss: 139547232.0000 - val_mae: 7248.4097\n",
            "Epoch 58/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 155046080.0000 - mae: 7865.5693 - val_loss: 138427824.0000 - val_mae: 7290.9131\n",
            "Epoch 59/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 153651680.0000 - mae: 7893.8389 - val_loss: 137402592.0000 - val_mae: 7339.9399\n",
            "Epoch 60/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 152271456.0000 - mae: 7924.3174 - val_loss: 136405760.0000 - val_mae: 7392.1729\n",
            "Epoch 61/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 150998768.0000 - mae: 7960.8662 - val_loss: 135468608.0000 - val_mae: 7445.0020\n",
            "Epoch 62/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 149812400.0000 - mae: 8006.5630 - val_loss: 134686848.0000 - val_mae: 7496.5791\n",
            "Epoch 63/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 148738160.0000 - mae: 8049.6353 - val_loss: 133901032.0000 - val_mae: 7556.5811\n",
            "Epoch 64/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 147637616.0000 - mae: 8092.3550 - val_loss: 133146976.0000 - val_mae: 7620.6309\n",
            "Epoch 65/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 146642576.0000 - mae: 8141.7949 - val_loss: 132524776.0000 - val_mae: 7677.9355\n",
            "Epoch 66/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 145743808.0000 - mae: 8191.5703 - val_loss: 131914216.0000 - val_mae: 7738.5337\n",
            "Epoch 67/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 144839136.0000 - mae: 8237.6494 - val_loss: 131296320.0000 - val_mae: 7807.1621\n",
            "Epoch 68/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 143999344.0000 - mae: 8291.9619 - val_loss: 130807928.0000 - val_mae: 7866.9297\n",
            "Epoch 69/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 143218640.0000 - mae: 8334.9199 - val_loss: 130311832.0000 - val_mae: 7931.2905\n",
            "Epoch 70/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 142507520.0000 - mae: 8390.4863 - val_loss: 129905424.0000 - val_mae: 7987.8018\n",
            "Epoch 71/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 141842400.0000 - mae: 8424.2959 - val_loss: 129466824.0000 - val_mae: 8053.3257\n",
            "Epoch 72/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 141191504.0000 - mae: 8480.3574 - val_loss: 129105952.0000 - val_mae: 8112.8564\n",
            "Epoch 73/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 140577280.0000 - mae: 8525.7979 - val_loss: 128810520.0000 - val_mae: 8165.8984\n",
            "Epoch 74/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 140058080.0000 - mae: 8578.4717 - val_loss: 128515232.0000 - val_mae: 8223.7627\n",
            "Epoch 75/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 139460384.0000 - mae: 8626.0586 - val_loss: 128208224.0000 - val_mae: 8293.3232\n",
            "Epoch 76/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 138930128.0000 - mae: 8683.1758 - val_loss: 127983816.0000 - val_mae: 8350.5557\n",
            "Epoch 77/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 138505680.0000 - mae: 8731.8184 - val_loss: 127801288.0000 - val_mae: 8402.9434\n",
            "Epoch 78/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 138061776.0000 - mae: 8767.1611 - val_loss: 127623880.0000 - val_mae: 8462.5566\n",
            "Epoch 79/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 137704720.0000 - mae: 8816.7529 - val_loss: 127498608.0000 - val_mae: 8511.1094\n",
            "Epoch 80/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 137386288.0000 - mae: 8862.5537 - val_loss: 127398576.0000 - val_mae: 8556.0674\n",
            "Epoch 81/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 137074160.0000 - mae: 8892.5820 - val_loss: 127306952.0000 - val_mae: 8608.0840\n",
            "Epoch 82/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 136805872.0000 - mae: 8944.6699 - val_loss: 127251640.0000 - val_mae: 8644.9561\n",
            "Epoch 83/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 136604848.0000 - mae: 8974.9248 - val_loss: 127202056.0000 - val_mae: 8684.8076\n",
            "Epoch 84/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 136376688.0000 - mae: 9004.8057 - val_loss: 127166184.0000 - val_mae: 8727.9453\n",
            "Epoch 85/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 136155104.0000 - mae: 9037.1260 - val_loss: 127143192.0000 - val_mae: 8771.9365\n",
            "Epoch 86/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135989888.0000 - mae: 9073.5459 - val_loss: 127135768.0000 - val_mae: 8804.7021\n",
            "Epoch 87/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135848288.0000 - mae: 9097.9170 - val_loss: 127134112.0000 - val_mae: 8835.4111\n",
            "Epoch 88/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135721152.0000 - mae: 9118.2451 - val_loss: 127142648.0000 - val_mae: 8867.5342\n",
            "Epoch 89/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135576672.0000 - mae: 9141.9727 - val_loss: 127169424.0000 - val_mae: 8908.3389\n",
            "Epoch 90/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135462400.0000 - mae: 9181.5342 - val_loss: 127183088.0000 - val_mae: 8931.5400\n",
            "Epoch 91/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135376640.0000 - mae: 9199.3896 - val_loss: 127198640.0000 - val_mae: 8954.2178\n",
            "Epoch 92/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135314336.0000 - mae: 9211.1963 - val_loss: 127212144.0000 - val_mae: 8973.2959\n",
            "Epoch 93/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135245664.0000 - mae: 9237.3047 - val_loss: 127231304.0000 - val_mae: 8992.3174\n",
            "Epoch 94/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135194400.0000 - mae: 9249.1367 - val_loss: 127233104.0000 - val_mae: 9001.0029\n",
            "Epoch 95/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135144720.0000 - mae: 9258.3633 - val_loss: 127257184.0000 - val_mae: 9021.5742\n",
            "Epoch 96/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135081600.0000 - mae: 9276.2021 - val_loss: 127272760.0000 - val_mae: 9036.6230\n",
            "Epoch 97/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135044320.0000 - mae: 9281.9111 - val_loss: 127285784.0000 - val_mae: 9049.2197\n",
            "Epoch 98/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134990640.0000 - mae: 9292.9492 - val_loss: 127328776.0000 - val_mae: 9072.9053\n",
            "Epoch 99/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134939472.0000 - mae: 9318.4248 - val_loss: 127359008.0000 - val_mae: 9090.2676\n",
            "Epoch 100/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134919136.0000 - mae: 9318.5244 - val_loss: 127357008.0000 - val_mae: 9094.8506\n",
            "Epoch 101/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134875264.0000 - mae: 9326.0977 - val_loss: 127377856.0000 - val_mae: 9107.8809\n",
            "Epoch 102/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134840336.0000 - mae: 9336.5352 - val_loss: 127394664.0000 - val_mae: 9118.8799\n",
            "Epoch 103/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134803440.0000 - mae: 9351.4180 - val_loss: 127384456.0000 - val_mae: 9120.4365\n",
            "Epoch 104/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134780896.0000 - mae: 9348.9248 - val_loss: 127405232.0000 - val_mae: 9132.2500\n",
            "Epoch 105/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134751792.0000 - mae: 9350.7236 - val_loss: 127424008.0000 - val_mae: 9143.1055\n",
            "Epoch 106/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134714624.0000 - mae: 9365.4443 - val_loss: 127448536.0000 - val_mae: 9155.2480\n",
            "Epoch 107/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134687264.0000 - mae: 9377.5488 - val_loss: 127472560.0000 - val_mae: 9167.2256\n",
            "Epoch 108/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134669248.0000 - mae: 9384.3193 - val_loss: 127469200.0000 - val_mae: 9170.5000\n",
            "Epoch 109/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134639600.0000 - mae: 9382.0938 - val_loss: 127494448.0000 - val_mae: 9181.9629\n",
            "Epoch 110/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134610448.0000 - mae: 9397.9609 - val_loss: 127490176.0000 - val_mae: 9184.1279\n",
            "Epoch 111/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134587008.0000 - mae: 9397.6514 - val_loss: 127480352.0000 - val_mae: 9185.7021\n",
            "Epoch 112/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134577776.0000 - mae: 9397.4014 - val_loss: 127486832.0000 - val_mae: 9191.6904\n",
            "Epoch 113/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134541968.0000 - mae: 9403.3057 - val_loss: 127475056.0000 - val_mae: 9192.3906\n",
            "Epoch 114/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134534608.0000 - mae: 9396.1562 - val_loss: 127456920.0000 - val_mae: 9193.3271\n",
            "Epoch 115/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134506544.0000 - mae: 9398.9775 - val_loss: 127445424.0000 - val_mae: 9193.9111\n",
            "Epoch 116/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134491664.0000 - mae: 9408.7910 - val_loss: 127436976.0000 - val_mae: 9195.6641\n",
            "Epoch 117/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134466912.0000 - mae: 9407.6680 - val_loss: 127420960.0000 - val_mae: 9195.3193\n",
            "Epoch 118/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134446032.0000 - mae: 9407.0254 - val_loss: 127400416.0000 - val_mae: 9193.4111\n",
            "Epoch 119/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134427008.0000 - mae: 9401.3984 - val_loss: 127388944.0000 - val_mae: 9195.0205\n",
            "Epoch 120/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134406928.0000 - mae: 9405.8379 - val_loss: 127373424.0000 - val_mae: 9194.6621\n",
            "Epoch 121/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134386208.0000 - mae: 9412.4355 - val_loss: 127351000.0000 - val_mae: 9192.3701\n",
            "Epoch 122/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134362928.0000 - mae: 9404.6230 - val_loss: 127317984.0000 - val_mae: 9187.5654\n",
            "Epoch 123/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134355520.0000 - mae: 9403.8799 - val_loss: 127312936.0000 - val_mae: 9189.9326\n",
            "Epoch 124/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134330384.0000 - mae: 9392.7207 - val_loss: 127305424.0000 - val_mae: 9191.7715\n",
            "Epoch 125/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134306608.0000 - mae: 9397.2041 - val_loss: 127318024.0000 - val_mae: 9198.8672\n",
            "Epoch 126/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134286608.0000 - mae: 9402.8271 - val_loss: 127304344.0000 - val_mae: 9199.0410\n",
            "Epoch 127/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134267312.0000 - mae: 9405.2129 - val_loss: 127266840.0000 - val_mae: 9192.7012\n",
            "Epoch 128/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134254256.0000 - mae: 9403.1406 - val_loss: 127252048.0000 - val_mae: 9192.7314\n",
            "Epoch 129/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134227952.0000 - mae: 9407.4473 - val_loss: 127237544.0000 - val_mae: 9192.3115\n",
            "Epoch 130/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134205400.0000 - mae: 9399.8799 - val_loss: 127213904.0000 - val_mae: 9190.0332\n",
            "Epoch 131/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134181664.0000 - mae: 9388.2471 - val_loss: 127214952.0000 - val_mae: 9194.3799\n",
            "Epoch 132/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 134169056.0000 - mae: 9398.8008 - val_loss: 127204920.0000 - val_mae: 9195.2070\n",
            "Epoch 133/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134149080.0000 - mae: 9397.3555 - val_loss: 127193688.0000 - val_mae: 9195.8896\n",
            "Epoch 134/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134122840.0000 - mae: 9397.0771 - val_loss: 127217216.0000 - val_mae: 9205.6748\n",
            "Epoch 135/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134105376.0000 - mae: 9410.5645 - val_loss: 127192752.0000 - val_mae: 9202.7334\n",
            "Epoch 136/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134089824.0000 - mae: 9402.6387 - val_loss: 127189080.0000 - val_mae: 9205.5400\n",
            "Epoch 137/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134059856.0000 - mae: 9409.7549 - val_loss: 127170640.0000 - val_mae: 9204.7109\n",
            "Epoch 138/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134040360.0000 - mae: 9411.9473 - val_loss: 127141912.0000 - val_mae: 9200.8604\n",
            "Epoch 139/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134022392.0000 - mae: 9411.0186 - val_loss: 127107736.0000 - val_mae: 9195.4600\n",
            "Epoch 140/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134002272.0000 - mae: 9396.9014 - val_loss: 127112352.0000 - val_mae: 9199.9883\n",
            "Epoch 141/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 133984768.0000 - mae: 9401.0352 - val_loss: 127112464.0000 - val_mae: 9203.6533\n",
            "Epoch 142/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133961312.0000 - mae: 9401.5537 - val_loss: 127124320.0000 - val_mae: 9210.5869\n",
            "Epoch 143/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133942472.0000 - mae: 9404.8779 - val_loss: 127138544.0000 - val_mae: 9217.6904\n",
            "Epoch 144/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133921976.0000 - mae: 9425.0645 - val_loss: 127118160.0000 - val_mae: 9216.4219\n",
            "Epoch 145/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133903840.0000 - mae: 9418.7871 - val_loss: 127086888.0000 - val_mae: 9212.7256\n",
            "Epoch 146/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133889344.0000 - mae: 9403.9570 - val_loss: 127064144.0000 - val_mae: 9209.9561\n",
            "Epoch 147/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133860064.0000 - mae: 9403.4082 - val_loss: 127053904.0000 - val_mae: 9210.8643\n",
            "Epoch 148/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133842232.0000 - mae: 9408.5361 - val_loss: 127043560.0000 - val_mae: 9212.0703\n",
            "Epoch 149/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133823632.0000 - mae: 9408.2578 - val_loss: 127037840.0000 - val_mae: 9214.5781\n",
            "Epoch 150/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133801248.0000 - mae: 9415.3008 - val_loss: 127026000.0000 - val_mae: 9215.8887\n",
            "Epoch 151/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 133785072.0000 - mae: 9411.7363 - val_loss: 127009184.0000 - val_mae: 9215.0225\n",
            "Epoch 152/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133764176.0000 - mae: 9409.0996 - val_loss: 126992344.0000 - val_mae: 9214.9609\n",
            "Epoch 153/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 133740424.0000 - mae: 9417.4590 - val_loss: 126969136.0000 - val_mae: 9213.1572\n",
            "Epoch 154/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133724400.0000 - mae: 9415.2256 - val_loss: 126956000.0000 - val_mae: 9213.6924\n",
            "Epoch 155/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133701224.0000 - mae: 9421.7070 - val_loss: 126928264.0000 - val_mae: 9210.8174\n",
            "Epoch 156/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133678872.0000 - mae: 9410.5059 - val_loss: 126930768.0000 - val_mae: 9215.1660\n",
            "Epoch 157/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133666080.0000 - mae: 9410.3447 - val_loss: 126899200.0000 - val_mae: 9211.2383\n",
            "Epoch 158/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133647904.0000 - mae: 9400.8516 - val_loss: 126892328.0000 - val_mae: 9212.7842\n",
            "Epoch 159/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133622144.0000 - mae: 9405.3613 - val_loss: 126878928.0000 - val_mae: 9212.9697\n",
            "Epoch 160/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133604376.0000 - mae: 9408.8623 - val_loss: 126870072.0000 - val_mae: 9214.3945\n",
            "Epoch 161/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133588568.0000 - mae: 9412.1123 - val_loss: 126865368.0000 - val_mae: 9217.1543\n",
            "Epoch 162/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133570656.0000 - mae: 9411.6533 - val_loss: 126844216.0000 - val_mae: 9215.2500\n",
            "Epoch 163/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133549576.0000 - mae: 9409.4131 - val_loss: 126820256.0000 - val_mae: 9212.6426\n",
            "Epoch 164/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133522112.0000 - mae: 9407.6787 - val_loss: 126809712.0000 - val_mae: 9212.7490\n",
            "Epoch 165/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133511168.0000 - mae: 9402.9834 - val_loss: 126795560.0000 - val_mae: 9212.4258\n",
            "Epoch 166/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133491992.0000 - mae: 9416.5996 - val_loss: 126778552.0000 - val_mae: 9211.5762\n",
            "Epoch 167/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133477416.0000 - mae: 9396.1816 - val_loss: 126760200.0000 - val_mae: 9210.7451\n",
            "Epoch 168/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133445544.0000 - mae: 9410.4326 - val_loss: 126743224.0000 - val_mae: 9210.0469\n",
            "Epoch 169/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133433176.0000 - mae: 9407.2803 - val_loss: 126739848.0000 - val_mae: 9212.8994\n",
            "Epoch 170/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133411016.0000 - mae: 9408.0010 - val_loss: 126726032.0000 - val_mae: 9212.7529\n",
            "Epoch 171/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133388120.0000 - mae: 9402.2832 - val_loss: 126727648.0000 - val_mae: 9216.5371\n",
            "Epoch 172/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133373624.0000 - mae: 9410.3594 - val_loss: 126714224.0000 - val_mae: 9216.5771\n",
            "Epoch 173/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133353680.0000 - mae: 9408.7949 - val_loss: 126676656.0000 - val_mae: 9210.6113\n",
            "Epoch 174/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133332960.0000 - mae: 9404.8994 - val_loss: 126660256.0000 - val_mae: 9209.8008\n",
            "Epoch 175/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133315840.0000 - mae: 9405.2510 - val_loss: 126660552.0000 - val_mae: 9213.1621\n",
            "Epoch 176/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133292440.0000 - mae: 9399.6816 - val_loss: 126670296.0000 - val_mae: 9218.8984\n",
            "Epoch 177/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133275120.0000 - mae: 9411.2949 - val_loss: 126648792.0000 - val_mae: 9216.7988\n",
            "Epoch 178/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133259936.0000 - mae: 9410.1855 - val_loss: 126630304.0000 - val_mae: 9215.7578\n",
            "Epoch 179/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133238848.0000 - mae: 9409.4033 - val_loss: 126590136.0000 - val_mae: 9209.2158\n",
            "Epoch 180/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133219144.0000 - mae: 9397.9102 - val_loss: 126571000.0000 - val_mae: 9207.5840\n",
            "Epoch 181/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133196064.0000 - mae: 9395.7617 - val_loss: 126569264.0000 - val_mae: 9210.3896\n",
            "Epoch 182/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133183096.0000 - mae: 9393.9209 - val_loss: 126524632.0000 - val_mae: 9202.5322\n",
            "Epoch 183/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133158120.0000 - mae: 9396.0312 - val_loss: 126516248.0000 - val_mae: 9203.5420\n",
            "Epoch 184/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133136856.0000 - mae: 9392.4102 - val_loss: 126513488.0000 - val_mae: 9205.9473\n",
            "Epoch 185/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133116072.0000 - mae: 9392.2930 - val_loss: 126521232.0000 - val_mae: 9211.0234\n",
            "Epoch 186/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133095272.0000 - mae: 9399.1611 - val_loss: 126517664.0000 - val_mae: 9213.7295\n",
            "Epoch 187/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133078720.0000 - mae: 9403.3086 - val_loss: 126509336.0000 - val_mae: 9214.9639\n",
            "Epoch 188/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133059696.0000 - mae: 9408.1670 - val_loss: 126505240.0000 - val_mae: 9217.1338\n",
            "Epoch 189/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133044432.0000 - mae: 9402.7715 - val_loss: 126491344.0000 - val_mae: 9216.6963\n",
            "Epoch 190/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133016392.0000 - mae: 9404.5713 - val_loss: 126475024.0000 - val_mae: 9215.7617\n",
            "Epoch 191/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133006232.0000 - mae: 9398.1299 - val_loss: 126467184.0000 - val_mae: 9217.3594\n",
            "Epoch 192/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132981536.0000 - mae: 9406.5254 - val_loss: 126437880.0000 - val_mae: 9213.6680\n",
            "Epoch 193/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132956776.0000 - mae: 9399.5381 - val_loss: 126424264.0000 - val_mae: 9213.9600\n",
            "Epoch 194/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132948504.0000 - mae: 9400.0195 - val_loss: 126397624.0000 - val_mae: 9210.5000\n",
            "Epoch 195/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132923872.0000 - mae: 9393.5225 - val_loss: 126383984.0000 - val_mae: 9210.5156\n",
            "Epoch 196/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132901576.0000 - mae: 9400.1689 - val_loss: 126344736.0000 - val_mae: 9204.3770\n",
            "Epoch 197/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132887232.0000 - mae: 9401.7402 - val_loss: 126323160.0000 - val_mae: 9202.4883\n",
            "Epoch 198/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132863168.0000 - mae: 9393.8145 - val_loss: 126318792.0000 - val_mae: 9204.9844\n",
            "Epoch 199/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132843136.0000 - mae: 9390.8389 - val_loss: 126300560.0000 - val_mae: 9203.6904\n",
            "Epoch 200/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132822472.0000 - mae: 9388.8994 - val_loss: 126271216.0000 - val_mae: 9199.8682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x = list(range(epochs)), y = history.history['mae'], name = 'entrenamiento')\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x = list(range(epochs)), y = history.history['val_mae'], name = 'validación')\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    hovermode = 'x unified', \n",
        "    legend_title = 'Conjunto de', \n",
        "    xaxis_title = 'Época', \n",
        "    yaxis_title = 'Error Absoluto Medio (MAE)', \n",
        "    title = 'MAE en cada época del entrenamiento<br><sup>Primera red neuronal</sup>', \n",
        ")\n"
      ],
      "metadata": {
        "id": "xZJqyyCarj1r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "a0b83cf2-1abf-44a1-afce-0af0ae8eba34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"ab296ae9-4cc5-4eb6-a767-dd75d116eebb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ab296ae9-4cc5-4eb6-a767-dd75d116eebb\")) {                    Plotly.newPlot(                        \"ab296ae9-4cc5-4eb6-a767-dd75d116eebb\",                        [{\"name\":\"entrenamiento\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[13571.1083984375,13382.3046875,13207.8291015625,13035.0986328125,12863.310546875,12691.9814453125,12520.0400390625,12348.9130859375,12176.470703125,12005.5869140625,11838.3447265625,11668.8046875,11503.1376953125,11340.3720703125,11175.8984375,11020.091796875,10869.6162109375,10717.552734375,10575.611328125,10438.158203125,10301.28515625,10165.283203125,10033.2529296875,9905.8193359375,9778.142578125,9652.05859375,9532.142578125,9414.8447265625,9303.0693359375,9195.7998046875,9092.177734375,8989.33984375,8890.03125,8796.3671875,8706.654296875,8620.533203125,8537.3447265625,8456.5908203125,8377.6669921875,8306.1943359375,8239.044921875,8175.96875,8115.8828125,8060.2734375,8009.7197265625,7964.7177734375,7925.73974609375,7894.6337890625,7864.44775390625,7844.13232421875,7823.61279296875,7815.09619140625,7808.71044921875,7814.86376953125,7814.97412109375,7827.65966796875,7844.94921875,7865.5693359375,7893.8388671875,7924.3173828125,7960.8662109375,8006.56298828125,8049.63525390625,8092.35498046875,8141.794921875,8191.5703125,8237.6494140625,8291.9619140625,8334.919921875,8390.486328125,8424.2958984375,8480.357421875,8525.7978515625,8578.4716796875,8626.05859375,8683.17578125,8731.818359375,8767.1611328125,8816.7529296875,8862.5537109375,8892.58203125,8944.669921875,8974.9248046875,9004.8056640625,9037.1259765625,9073.5458984375,9097.9169921875,9118.2451171875,9141.97265625,9181.5341796875,9199.3896484375,9211.1962890625,9237.3046875,9249.13671875,9258.36328125,9276.2021484375,9281.9111328125,9292.94921875,9318.4248046875,9318.5244140625,9326.09765625,9336.53515625,9351.41796875,9348.9248046875,9350.7236328125,9365.4443359375,9377.548828125,9384.3193359375,9382.09375,9397.9609375,9397.6513671875,9397.4013671875,9403.3056640625,9396.15625,9398.9775390625,9408.791015625,9407.66796875,9407.025390625,9401.3984375,9405.837890625,9412.435546875,9404.623046875,9403.8798828125,9392.720703125,9397.2041015625,9402.8271484375,9405.212890625,9403.140625,9407.447265625,9399.8798828125,9388.2470703125,9398.80078125,9397.35546875,9397.0771484375,9410.564453125,9402.638671875,9409.7548828125,9411.947265625,9411.0185546875,9396.9013671875,9401.03515625,9401.5537109375,9404.8779296875,9425.064453125,9418.787109375,9403.95703125,9403.408203125,9408.5361328125,9408.2578125,9415.30078125,9411.736328125,9409.099609375,9417.458984375,9415.2255859375,9421.70703125,9410.505859375,9410.3447265625,9400.8515625,9405.361328125,9408.8623046875,9412.1123046875,9411.6533203125,9409.4130859375,9407.6787109375,9402.9833984375,9416.599609375,9396.181640625,9410.4326171875,9407.2802734375,9408.0009765625,9402.283203125,9410.359375,9408.794921875,9404.8994140625,9405.2509765625,9399.681640625,9411.294921875,9410.185546875,9409.4033203125,9397.91015625,9395.76171875,9393.9208984375,9396.03125,9392.41015625,9392.29296875,9399.1611328125,9403.30859375,9408.1669921875,9402.771484375,9404.5712890625,9398.1298828125,9406.525390625,9399.5380859375,9400.01953125,9393.5224609375,9400.1689453125,9401.740234375,9393.814453125,9390.8388671875,9388.8994140625],\"type\":\"scatter\"},{\"name\":\"validaci\\u00f3n\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[12156.6142578125,11980.5703125,11806.2158203125,11635.0654296875,11465.4970703125,11292.5634765625,11121.1259765625,10947.884765625,10776.091796875,10609.296875,10435.931640625,10265.2880859375,10095.9228515625,9927.326171875,9765.7236328125,9611.048828125,9453.43359375,9301.0849609375,9158.1865234375,9016.103515625,8872.345703125,8738.1611328125,8612.11328125,8485.0087890625,8360.9111328125,8242.513671875,8127.69287109375,8017.1728515625,7912.02978515625,7814.27783203125,7717.318359375,7630.96923828125,7545.658203125,7464.431640625,7396.02734375,7334.6640625,7279.70263671875,7227.10791015625,7186.75,7146.19970703125,7111.853515625,7081.77880859375,7058.4853515625,7045.93505859375,7039.33642578125,7035.47412109375,7037.8251953125,7045.31396484375,7054.49755859375,7065.81298828125,7079.7890625,7097.25732421875,7118.39208984375,7144.70751953125,7179.3125,7212.537109375,7248.40966796875,7290.9130859375,7339.93994140625,7392.1728515625,7445.001953125,7496.5791015625,7556.5810546875,7620.630859375,7677.935546875,7738.53369140625,7807.162109375,7866.9296875,7931.29052734375,7987.8017578125,8053.32568359375,8112.8564453125,8165.8984375,8223.7626953125,8293.3232421875,8350.5556640625,8402.943359375,8462.556640625,8511.109375,8556.0673828125,8608.083984375,8644.9560546875,8684.8076171875,8727.9453125,8771.9365234375,8804.7021484375,8835.4111328125,8867.5341796875,8908.3388671875,8931.5400390625,8954.2177734375,8973.2958984375,8992.3173828125,9001.0029296875,9021.57421875,9036.623046875,9049.2197265625,9072.9052734375,9090.267578125,9094.8505859375,9107.880859375,9118.8798828125,9120.4365234375,9132.25,9143.10546875,9155.248046875,9167.2255859375,9170.5,9181.962890625,9184.1279296875,9185.7021484375,9191.6904296875,9192.390625,9193.3271484375,9193.9111328125,9195.6640625,9195.3193359375,9193.4111328125,9195.0205078125,9194.662109375,9192.3701171875,9187.5654296875,9189.9326171875,9191.771484375,9198.8671875,9199.041015625,9192.701171875,9192.7314453125,9192.3115234375,9190.033203125,9194.3798828125,9195.20703125,9195.8896484375,9205.6748046875,9202.7333984375,9205.5400390625,9204.7109375,9200.8603515625,9195.4599609375,9199.98828125,9203.6533203125,9210.5869140625,9217.6904296875,9216.421875,9212.7255859375,9209.9560546875,9210.8642578125,9212.0703125,9214.578125,9215.888671875,9215.0224609375,9214.9609375,9213.1572265625,9213.6923828125,9210.8173828125,9215.166015625,9211.23828125,9212.7841796875,9212.9697265625,9214.39453125,9217.154296875,9215.25,9212.642578125,9212.7490234375,9212.42578125,9211.576171875,9210.7451171875,9210.046875,9212.8994140625,9212.7529296875,9216.537109375,9216.5771484375,9210.611328125,9209.80078125,9213.162109375,9218.8984375,9216.798828125,9215.7578125,9209.2158203125,9207.583984375,9210.3896484375,9202.5322265625,9203.5419921875,9205.947265625,9211.0234375,9213.7294921875,9214.9638671875,9217.1337890625,9216.6962890625,9215.76171875,9217.359375,9213.66796875,9213.9599609375,9210.5,9210.515625,9204.376953125,9202.48828125,9204.984375,9203.6904296875,9199.8681640625],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"hovermode\":\"x unified\",\"legend\":{\"title\":{\"text\":\"Conjunto de\"}},\"xaxis\":{\"title\":{\"text\":\"\\u00c9poca\"}},\"yaxis\":{\"title\":{\"text\":\"Error Absoluto Medio (MAE)\"}},\"title\":{\"text\":\"MAE en cada \\u00e9poca del entrenamiento<br><sup>Primera red neuronal</sup>\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ab296ae9-4cc5-4eb6-a767-dd75d116eebb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La gráfica muestra que el MAE llegó a un mínimo alrededor de la época 40 y luego el desempeño empeoró. Gracias a que agregamos el callback para guardar únicamente el mejor modelo basado en esta métrica, basta que lo carguemos con la función ```load_weights```."
      ],
      "metadata": {
        "id": "rqjmieF5xtpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_weights(weights_filepath)"
      ],
      "metadata": {
        "id": "g9e_GZ-e62Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "eBO1-tkuChQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos la función ```predict``` para predecir usando el modelo que acabamos de entrenar."
      ],
      "metadata": {
        "id": "pAVPqecfylK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_nn1 = model.predict(X_train_tr)[:, 0]\n",
        "y_pred_test_nn1 = model.predict(X_test_tr)[:, 0]"
      ],
      "metadata": {
        "id": "CaMHcouE7_DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_nn1"
      ],
      "metadata": {
        "id": "Ey2e_VT_WbWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métricas"
      ],
      "metadata": {
        "id": "BKOPr_ZCCy_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"MSE train: {mean_squared_error(y_train, y_pred_train_nn1)}\", \n",
        "    f\"MSE test: {mean_squared_error(y_test, y_pred_test_nn1)}\", \n",
        "    \"\",\n",
        "    f\"MAE train: {mean_absolute_error(y_train, y_pred_train_nn1)}\", \n",
        "    f\"MAE test: {mean_absolute_error(y_test, y_pred_test_nn1)}\", \n",
        "    sep = '\\n'\n",
        ")"
      ],
      "metadata": {
        "id": "gV4if7ut7V2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e695c24c-331d-4395-f906-43259a173c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE train: 131487903.45923795\n",
            "MSE test: 117553262.1523611\n",
            "\n",
            "MAE train: 9349.837116070214\n",
            "MAE test: 8802.052779360774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización"
      ],
      "metadata": {
        "id": "O6iJ9GhSCkws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_evaluation(y_train, y_pred_train_nn1, y_test, y_pred_test_nn1, 'Primera red neuronal')"
      ],
      "metadata": {
        "id": "0vvHsvgcgf5J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "72303695-11b4-4900-c42e-fd4797be469d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"1dbd5ffa-4075-480f-a2e2-f44b0d47c7eb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1dbd5ffa-4075-480f-a2e2-f44b0d47c7eb\")) {                    Plotly.newPlot(                        \"1dbd5ffa-4075-480f-a2e2-f44b0d47c7eb\",                        [{\"mode\":\"markers\",\"x\":[3167.45585,2689.4954,11576.13,16586.49771,6746.7425,5976.8311,5649.715,15161.5344,2007.945,19214.70553,16455.70785,10594.50155,27117.99378,4296.2712,7151.092,8410.04685,22331.5668,37165.1638,17468.9839,2494.022,6356.2707,11305.93455,12235.8392,14256.1928,13822.803,7173.35995,9140.951,1711.0268,5926.846,32108.66282,17942.106,43896.3763,8671.19125,6112.35295,15019.76005,11365.952,2709.24395,43254.41795,1263.249,3757.8448,20420.60465,2302.3,11658.11505,11394.06555,1967.0227,3353.4703,10269.46,18963.17192,31620.00106,30259.99556,6571.544,1632.56445,2755.02095,5934.3798,14319.031,19964.7463,6849.026,2498.4144,1631.6683,3766.8838,9625.92,3309.7926,37079.372,15359.1045,36124.5737,2842.76075,1622.1885,46599.1084,6948.7008,4500.33925,2128.43105,9095.06825,19673.33573,48824.45,8688.85885,3645.0894,2480.9791,2154.361,33307.5508,12643.3778,27375.90478,8556.907,37270.1512,6593.5083,9101.798,13019.16105,28868.6639,1621.3402,6571.02435,34828.654,4260.744,8765.249,4040.55825,9875.6804,4337.7352,11356.6609,9290.1395,34439.8559,8891.1395,7162.0122,8026.6666,35160.13457,13555.0049,4719.73655,7222.78625,2534.39375,1719.4363,1635.73365,1725.5523,1824.2854,8342.90875,12829.4551,12979.358,4391.652,19933.458,7623.518,7640.3092,28101.33305,8442.667,2721.3208,2803.69785,12044.342,1131.5066,11743.9341,5974.3847,2731.9122,26140.3603,4243.59005,4846.92015,28287.89766,20630.28351,2741.948,29186.48236,1261.859,6877.9801,14283.4594,4402.233,37742.5757,13217.0945,11163.568,13451.122,26467.09737,58571.07448,13204.28565,1639.5631,2473.3341,48970.2476,9910.35985,3044.2133,60021.39897,41034.2214,9877.6077,8125.7845,2020.5523,25382.297,34303.1672,7742.1098,10702.6424,8516.829,27322.73386,37701.8768,26125.67477,30184.9367,10107.2206,3070.8087,5152.134,4746.344,5910.944,4415.1588,14043.4767,16085.1275,8428.0693,39727.614,8283.6807,14210.53595,10355.641,12265.5069,6455.86265,11488.31695,4134.08245,9563.029,18804.7524,11856.4115,11264.541,1631.8212,38792.6856,42560.4304,2150.469,7050.642,42112.2356,41949.2441,4462.7218,5484.4673,20234.85475,1877.9294,11085.5868,8219.2039,13126.67745,5227.98875,3268.84665,12479.70895,6250.435,28923.13692,2136.88225,13937.6665,36580.28216,6652.5288,3947.4131,19444.2658,12741.16745,1832.094,13770.0979,6113.23105,6198.7518,38511.6283,14382.70905,16450.8947,7133.9025,37829.7242,6406.4107,6933.24225,2137.6536,12404.8791,10096.97,26109.32905,3906.127,44423.803,12124.9924,1253.936,34166.273,45702.02235,13470.86,2632.992,38709.176,9182.17,1826.843,7243.8136,12430.95335,10118.424,4827.90495,5312.16985,3213.62205,4779.6023,4931.647,23082.95533,4433.9159,7985.815,33750.2918,48675.5177,33475.81715,9872.701,44260.7499,10601.412,2117.33885,11070.535,33900.653,2639.0429,5428.7277,11837.16,9301.89355,35491.64,47291.055,9566.9909,14410.9321,20878.78443,42983.4585,10796.35025,11884.04858,11244.3769,3279.86855,6796.86325,27037.9141,10848.1343,10560.4917,44202.6536,9288.0267,9620.3307,5458.04645,36149.4835,11381.3254,21595.38229,21880.82,1977.815,36837.467,6640.54485,4347.02335,36219.40545,26926.5144,4435.0942,6128.79745,3597.596,1704.5681,16776.30405,2727.3951,7348.142,1242.816,45863.205,24671.66334,6186.127,19539.243,2566.4707,49577.6624,39836.519,6402.29135,8827.2099,17904.52705,6272.4772,6500.2359,2257.47525,2155.6815,2362.22905,1815.8759,1875.344,9724.53,11538.421,2219.4451,2156.7518,42969.8527,1708.92575,9722.7695,15820.699,11842.442,8582.3023,33907.548,38998.546,7345.7266,4463.2051,15828.82173,2867.1196,5438.7491,4564.19145,46718.16325,12592.5345,23563.01618,6185.3208,10976.24575,27941.28758,4189.1131,41097.16175,36197.699,15817.9857,26018.95052,3857.75925,3490.5491,6238.298,2396.0959,7419.4779,8413.46305,1880.07,7050.0213,4518.82625,11552.904,10982.5013,4673.3922,23807.2406,20149.3229,12949.1554,4753.6368,27000.98473,24180.9335,7731.4271,7749.1564,9644.2525,42211.1382,6940.90985,24869.8368,13457.9608,12224.35085,8551.347,22478.6,12925.886,5012.471,9414.92,14988.432,4618.0799,2730.10785,19749.38338,3556.9223,39241.442,17043.3414,8277.523,4830.63,7518.02535,10214.636,18767.7377,1242.26,4536.259,8965.79575,34617.84065,16796.41194,11848.141,8932.084,20009.63365,1759.338,45008.9555,37465.34375,46661.4424,11881.9696,5972.378,1252.407,3875.7341,1743.214,3981.9768,13635.6379,10381.4787,9048.0273,3704.3545,5209.57885,7265.7025,47462.894,11674.13,1737.376,2775.19215,3693.428,5028.1466,1639.5631,9058.7303,10226.2842,4529.477,1629.8335,12815.44495,24915.04626,14692.66935,7986.47525,8017.06115,12105.32,3554.203,10461.9794,5615.369,4795.6568,19515.5416,2103.08,4349.462,12523.6048,10072.05505,1136.3994,21677.28345,1704.70015,21774.32215,13063.883,5116.5004,10825.2537,3353.284,4149.736,13405.3903,10977.2063,14349.8544,11741.726,44641.1974,4762.329,7371.772,3176.8159,2850.68375,40273.6455,17560.37975,4433.3877,1526.312,1702.4553,6373.55735,10942.13205,8547.6913,8604.48365,3176.2877,11150.78,4527.18295,12233.828,9850.432,7196.867,11729.6795,21223.6758,10435.06525,5272.1758,2250.8352,33732.6867,14358.36437,8233.0975,17496.306,2789.0574,14901.5167,5124.1887,8027.968,13143.33665,43578.9394,10807.4863,8522.003,5400.9805,9447.3824,10601.63225,4237.12655,14001.2867,1615.7667,11840.77505,40182.246,15230.32405,4877.98105,28340.18885,2699.56835,9748.9106,8539.671,47403.88,20167.33603,17352.6803,4058.1161,16297.846,8596.8278,2497.0383,38282.7495,36898.73308,14474.675,13224.693,42124.5153,14571.8908,11253.421,1534.3045,18246.4955,32548.3405,3877.30425,11842.62375,38126.2465,16577.7795,24915.22085,8569.8618,7804.1605,10736.87075,8968.33,9991.03765,21195.818,11658.37915,8603.8234,13919.8229,4915.05985,2217.6012,1634.5734,4934.705,32787.45859,12269.68865,2217.46915,5373.36425,8601.3293,12142.5786,7077.1894,20709.02034,1633.0444,2130.6759,7537.1639,14426.07385,1261.442,5757.41345,2211.13075,8782.469,6748.5912,11833.7823,10928.849,5709.1644,2203.73595,5354.07465,23065.4207,6875.961,22412.6485,2055.3249,12950.0712,40003.33225,12363.547,4032.2407,5855.9025,17626.23951,11033.6617,3594.17085,11396.9002,41999.52,9391.346,3577.999,4504.6624,40720.55105,3392.3652,9264.797,22192.43711,10450.552,2416.955,5989.52365,20177.67113,21984.47061,8835.26495,36307.7983,18648.4217,39722.7462,12485.8009,5327.40025,6496.886,10579.711,6686.4313,10577.087,1980.07,11455.28,2221.56445,5397.6167,1986.9334,17179.522,3761.292,8733.22925,6082.405,9855.1314,6117.4945,9866.30485,13844.7972,7935.29115,2913.569,18328.2381,1682.597,39611.7577,2201.0971,8823.279,30063.58055,39047.285,24535.69855,3659.346,1163.4627,4718.20355,2020.177,24059.68019,3206.49135,7201.70085,11881.358,46130.5265,23288.9284,8520.026,8023.13545,42760.5022,21348.706,35147.52848,14451.83515,12096.6512,34254.05335,4005.4225,14001.1338,9715.841,12890.05765,48673.5588,7228.21565,19719.6947,39774.2763,2974.126,6548.19505,1964.78,3861.20965,2200.83085,20781.48892,19521.9682,13981.85035,8515.7587,14235.072,41919.097,13143.86485,25992.82104,12638.195,10959.6947,42111.6647,7729.64575,24227.33724,10106.13425,1842.519,16138.76205,9283.562,43943.8761,3484.331,48885.13561,1731.677,14394.39815,44501.3982,2404.7338,3591.48,10806.839,11944.59435,2464.6188,46151.1245,4894.7533,9193.8385,14119.62,4133.64165,11945.1327,20745.9891,25081.76784,2710.82855,11657.7189,13470.8044,9800.8882,23244.7902,19350.3689,4449.462,13880.949,16115.3045,33471.97189,2304.0022,47305.305,2138.0707,3056.3881,28468.91901,2395.17155,13887.204,21771.3423,8269.044,12222.8983,48549.17835,8605.3615,5325.651,20984.0936,10407.08585,12032.326,10422.91665,12648.7034,9282.4806,47896.79135,13393.756,1981.5819,9411.005,9541.69555,2902.9065,21978.6769,3161.454,9704.66805,8444.474,11187.6567,20277.80751,3366.6697,11436.73815,4747.0529,1917.3184,13041.921,18806.14547,2457.502,7337.748,4922.9159,1727.54,3756.6216,7954.517,25333.33284,11362.755,43813.8661,5969.723,1769.53165,2438.0552,13844.506,5261.46945,9634.538,10923.9332,6610.1097,34838.873,24393.6224,3410.324,12957.118,3260.199,28476.73499,8116.26885,8978.1851,4661.28635,19107.7796,12574.049,2866.091,8334.5896,16884.924,13747.87235,47496.49445,4357.04365,5836.5204,5472.449,21797.0004,8823.98575,19496.71917,13112.6048,2690.1138,4667.60765,10602.385,4428.88785,11512.405,11830.6072,3393.35635,2719.27975,46200.9851,12982.8747,18955.22017,10564.8845,8534.6718,2254.7967,4571.41305,11326.71487,6551.7501,11520.09985,2045.68525,48517.56315,21659.9301,4438.2634,36950.2567,2527.81865,4687.797,5125.2157,7448.40395,30942.1918,8334.45755,3972.9247,6196.448,16657.71745,27724.28875,6289.7549,39725.51805,11743.299,7624.63,2205.9808,6653.7886,14455.64405,47928.03,1712.227,4889.9995,2585.269,12629.1656,40419.0191,25678.77845,35069.37452,7682.67,1909.52745,2643.2685,10704.47,8988.15875,4320.41085,24603.04837,5693.4305,12730.9996,11286.5387,13831.1152,8062.764,39125.33225,15006.57945,8871.1517,3943.5954,32734.1863,4670.64,8798.593,8083.9198,11345.519,9432.9253,7441.053,2026.9741,7045.499,1632.03625,37607.5277,13462.52,2102.2647,2104.1134,6414.178,11363.2832,4562.8421,4719.52405,6067.12675,5257.50795,6666.243,1984.4533,3935.1799,23401.30575,6600.361,1627.28245,8124.4084,3062.50825,1727.785,3180.5101,43753.33705,30166.61817,15555.18875,4234.927,18259.216,12129.61415,5148.5526,11737.84884,5594.8455,3292.52985,2457.21115,35595.5898,24667.419,40974.1649,24520.264,10043.249,3172.018,36910.60803,6799.458,3866.8552,10141.1362,6079.6715,1744.465,19040.876,14313.8463,4266.1658,12146.971,3989.841,4561.1885,10085.846,1837.237,47269.854,23045.56616,12981.3457,3925.7582,3558.62025,5245.2269,4185.0979,11272.33139,5729.0053,11879.10405,16069.08475,63770.42801,8280.6227,11082.5772,4837.5823,8527.532,4074.4537,5031.26955,11090.7178,1532.4697,34672.1472,2322.6218,6858.4796,13224.05705,52590.82939,8059.6791,7526.70645,7152.6714,8944.1151,12029.2867,22218.1149,3046.062,1748.774,43921.1837,1674.6323,4137.5227,9447.25035,7650.77375,2261.5688,9861.025,5979.731,7147.4728],\"y\":[7536.82080078125,13140.8154296875,19727.021484375,10473.216796875,13036.109375,13653.875,13797.775390625,19641.75,9007.947265625,14534.693359375,18804.865234375,16287.865234375,15095.974609375,9026.3828125,13254.26171875,14969.884765625,13769.421875,12525.4296875,9657.822265625,11276.517578125,17007.611328125,16045.0986328125,18284.859375,17340.306640625,19185.65625,12725.748046875,17049.208984375,8638.640625,13071.994140625,14810.66015625,10393.24609375,16861.060546875,16366.490234375,13957.990234375,18306.861328125,18676.453125,8702.130859375,17451.16015625,10641.076171875,11358.2958984375,13082.10546875,9274.70703125,16327.41015625,17785.853515625,9464.3466796875,9484.96484375,16538.84375,13853.193359375,19211.380859375,17332.623046875,14340.2431640625,8928.380859375,11834.4287109375,14343.76171875,19924.21875,13089.552734375,14481.302734375,12126.9921875,10994.458984375,13009.634765625,16016.41796875,10224.408203125,13213.275390625,8922.0654296875,12488.16796875,9417.7080078125,9723.125,19566.423828125,16371.828125,10952.921875,9618.4755859375,13923.443359375,12211.55078125,19804.443359375,14069.55078125,10939.986328125,12402.1708984375,9443.8916015625,9976.587890625,17765.037109375,11510.9736328125,16124.78515625,13502.830078125,13387.21875,14180.76953125,17604.6171875,18201.759765625,7423.099609375,14338.287109375,10994.52734375,11784.109375,15981.986328125,11583.2431640625,17608.439453125,11141.142578125,17861.595703125,15898.130859375,10691.4765625,13522.421875,16050.53125,14332.3193359375,15958.6513671875,18375.080078125,10484.552734375,13182.98046875,11967.2021484375,9766.4375,9353.4013671875,10586.65234375,11147.5068359375,16822.587890625,7903.60888671875,19339.037109375,10047.634765625,11731.703125,13493.423828125,14384.37109375,17812.314453125,15021.5634765625,10113.6201171875,8370.7529296875,16237.513671875,9728.349609375,18126.041015625,13325.7890625,8735.3203125,14852.9287109375,11999.994140625,12232.2265625,18806.42578125,17965.140625,10081.21875,16800.494140625,10454.6640625,10904.716796875,8124.1982421875,13205.400390625,14459.658203125,16907.0390625,17449.416015625,16796.73828125,17770.375,13886.416015625,15189.248046875,9866.9677734375,11376.90234375,20345.091796875,15731.10546875,9512.1376953125,17973.63671875,15856.08984375,17483.123046875,16981.021484375,10960.12109375,16488.83203125,10825.33203125,14674.822265625,15868.794921875,14801.15234375,17340.41015625,13316.142578125,9265.16015625,18360.767578125,14825.9453125,10465.41796875,12246.533203125,12373.2021484375,13394.86328125,12144.173828125,18073.458984375,20609.337890625,13415.3466796875,16057.111328125,15900.4580078125,19942.927734375,15475.064453125,18591.775390625,13395.19921875,17001.5859375,10312.5234375,16595.5234375,11694.50390625,18902.814453125,17992.310546875,11014.9638671875,12199.81640625,16854.83984375,8921.935546875,13851.8564453125,14764.265625,16129.94140625,13167.607421875,13433.5390625,12702.419921875,10130.845703125,15722.462890625,14833.1044921875,9805.26953125,11801.20703125,12412.236328125,15841.654296875,13013.4306640625,16688.646484375,10751.86328125,17807.03515625,19143.861328125,12463.939453125,11706.349609375,13090.458984375,18891.033203125,8119.71875,17960.693359375,12386.2734375,12546.3515625,12253.5400390625,17199.294921875,9645.208984375,11594.1826171875,14317.5859375,13473.11328125,11606.0078125,10263.388671875,13344.8564453125,14987.0498046875,16932.6640625,10744.44921875,18050.9609375,16654.986328125,9392.111328125,10885.73046875,18150.966796875,19443.796875,10621.234375,14784.8203125,13771.7353515625,11229.0947265625,13503.9521484375,17529.248046875,16067.017578125,9682.103515625,11126.6171875,10325.56640625,12234.716796875,10897.16796875,10212.326171875,12449.642578125,12500.69140625,10561.5546875,20142.423828125,10424.5703125,16563.673828125,18266.837890625,18587.498046875,8130.9033203125,15691.015625,11049.251953125,10204.396484375,13716.21875,16059.58984375,14736.0859375,13134.3525390625,19168.259765625,14138.47265625,19895.68359375,15470.3994140625,16481.447265625,16435.421875,8533.1904296875,15933.3076171875,9613.8935546875,13949.91015625,17517.333984375,17328.2734375,17256.939453125,17793.314453125,15614.783203125,15691.189453125,10826.0078125,11195.0517578125,20999.587890625,8427.75,13994.7109375,10471.998046875,13198.77734375,11240.5625,11947.6533203125,11500.1416015625,17891.654296875,11199.3671875,13176.109375,12277.283203125,8156.2529296875,10913.41796875,10928.2431640625,15425.455078125,7900.8115234375,17778.205078125,11440.4755859375,13961.296875,12925.58984375,10663.36328125,20715.55078125,14593.29296875,11668.599609375,15089.6064453125,11843.5029296875,12804.5458984375,13877.046875,9528.736328125,9882.3955078125,10578.67578125,10019.7109375,9522.70703125,16242.4140625,19656.279296875,11012.041015625,10025.93359375,16950.294921875,8740.65625,16651.51171875,11765.0390625,16767.95703125,13302.6484375,10628.248046875,15321.833984375,15544.65625,14146.404296875,14065.4384765625,11447.927734375,16140.787109375,11024.09765625,19236.763671875,20084.669921875,14895.5576171875,14114.58984375,17614.861328125,19447.96875,10452.71484375,16445.31640625,12837.3984375,10271.046875,10776.251953125,12143.017578125,11657.66015625,13923.4765625,9407.638671875,13697.17578125,15246.3125,10417.921875,14413.8134765625,13432.208984375,16474.20703125,19186.056640625,14114.453125,15599.6513671875,13306.2060546875,18468.119140625,14603.744140625,18674.783203125,15497.623046875,15227.931640625,14719.24609375,13494.021484375,17078.55078125,13704.6591796875,15978.380859375,17975.30078125,16744.16015625,15379.134765625,14244.998046875,17947.5,13179.1328125,15325.080078125,17889.8359375,11078.26171875,11500.18359375,15807.279296875,12157.859375,14028.05859375,11149.71875,14813.23828125,11810.2841796875,13914.7841796875,16585.689453125,13512.462890625,7826.24609375,15124.912109375,13229.9501953125,10568.630859375,11266.30078125,17532.248046875,14585.29296875,12885.80078125,11360.99609375,18506.365234375,11990.6953125,18379.189453125,17024.306640625,14318.12109375,9187.0576171875,14169.833984375,9198.6103515625,11368.66796875,18659.056640625,17254.103515625,17397.263671875,12652.97265625,10742.267578125,15693.8154296875,19305.939453125,17829.97265625,8415.677734375,8361.4462890625,14510.51953125,13174.81640625,9866.9677734375,18832.642578125,18409.236328125,10696.365234375,10748.39453125,16518.630859375,14395.814453125,18791.267578125,12589.236328125,13407.572265625,16873.943359375,11104.6015625,16737.513671875,11222.4912109375,14387.7822265625,13796.326171875,8767.912109375,12330.03515625,16636.08203125,16331.552734375,10384.521484375,14199.107421875,8173.9619140625,14038.771484375,20144.970703125,10773.287109375,17391.5625,10575.451171875,12162.365234375,20552.7578125,18337.96484375,16902.673828125,16177.021484375,17355.794921875,14516.9462890625,15349.8154296875,9503.8671875,10480.259765625,15855.38671875,10210.7109375,12378.806640625,10226.716796875,7872.90625,15355.8232421875,15320.21484375,15150.28125,13941.7158203125,9433.03125,15734.419921875,11540.1845703125,16681.947265625,16708.982421875,15233.07421875,15206.66796875,14916.9619140625,16950.181640625,12821.2626953125,10028.654296875,10232.15625,13242.1455078125,14710.6083984375,11360.595703125,10220.912109375,19877.412109375,15424.412109375,14789.166015625,18447.111328125,17546.1953125,17545.087890625,15415.34375,15235.712890625,14729.953125,17244.162109375,11528.921875,17549.75,8861.8994140625,16975.572265625,15056.8369140625,19623.2265625,11029.037109375,14302.6279296875,11310.76953125,19773.501953125,13813.26953125,19568.658203125,8515.3466796875,10030.2783203125,13554.134765625,9652.3564453125,15250.6591796875,11942.443359375,14306.5234375,10437.0263671875,19334.79296875,17280.875,16646.86328125,9073.3359375,16501.01171875,11560.0009765625,11732.9296875,10389.1201171875,11638.599609375,17223.501953125,13202.64453125,10681.9306640625,15836.791015625,18123.560546875,17402.57421875,17046.310546875,15649.607421875,13740.7568359375,15228.8291015625,16362.828125,13853.169921875,18958.3984375,10779.7822265625,11148.54296875,11384.060546875,11307.275390625,14886.4609375,17518.30078125,11130.833984375,13393.052734375,14945.7646484375,17039.6640625,12826.90625,18930.03125,11179.0078125,9919.53125,13788.771484375,9392.548828125,10398.7392578125,15887.8203125,10280.79296875,14972.4013671875,13284.0390625,15868.01953125,16977.16015625,16067.82421875,9289.078125,10851.830078125,15128.23828125,14667.177734375,15393.3154296875,10731.138671875,19450.65234375,14919.1689453125,18479.9296875,10974.0537109375,11743.712890625,11179.8828125,17960.740234375,9474.17578125,17740.275390625,17449.546875,17819.7265625,12028.931640625,12466.275390625,16068.203125,11105.021484375,13846.837890625,16081.595703125,18381.908203125,11559.849609375,14160.904296875,11043.353515625,11107.615234375,16169.8671875,11671.8994140625,11039.619140625,13116.3359375,19194.953125,12730.041015625,13166.380859375,18218.927734375,12138.4765625,15325.279296875,10568.91796875,16518.42578125,9110.005859375,14784.59375,11956.2763671875,12688.4990234375,11559.1875,17281.67578125,14552.3857421875,14468.8310546875,12291.50390625,16351.0859375,18350.923828125,16004.0615234375,8536.591796875,10786.306640625,11986.19140625,14152.564453125,8551.39453125,14789.271484375,18709.9140625,14552.76953125,15318.126953125,12206.736328125,14013.9736328125,10235.587890625,10648.376953125,9999.4345703125,9369.2705078125,13074.677734375,16680.873046875,18938.095703125,10291.1015625,14968.490234375,15844.521484375,16808.6484375,13792.03515625,10791.69140625,16655.53125,16356.572265625,11269.83984375,8908.865234375,17529.244140625,14576.296875,11166.8154296875,20316.0625,11795.90234375,12841.58203125,14625.9921875,9852.693359375,13331.283203125,8518.3798828125,8768.384765625,8899.4755859375,17424.978515625,12121.056640625,19436.439453125,14657.615234375,19594.798828125,16715.486328125,18517.947265625,16158.708984375,17434.791015625,17675.537109375,16130.759765625,13750.494140625,19435.955078125,15064.041015625,9517.8125,10714.8876953125,16624.818359375,13097.748046875,10178.5537109375,16454.416015625,7651.38671875,17282.935546875,15314.162109375,10979.697265625,11457.068359375,16771.134765625,18114.21484375,10208.0947265625,18416.998046875,11960.99609375,13243.416015625,18680.513671875,10692.529296875,17972.384765625,12634.001953125,9367.3740234375,8914.640625,16274.283203125,19697.751953125,18200.2734375,15548.767578125,12630.392578125,12742.443359375,18257.951171875,10289.302734375,17681.263671875,9353.40625,19142.423828125,10911.24609375,10555.61328125,16796.607421875,9283.6748046875,19358.220703125,13818.96875,13676.12109375,16549.357421875,19393.8671875,13675.658203125,12245.99609375,13142.953125,15471.3857421875,15096.9033203125,15320.9365234375,19105.482421875,14870.9970703125,19059.505859375,18731.072265625,11238.587890625,15445.240234375,18176.326171875,11274.2890625,15089.52734375,10363.2021484375,16867.701171875,15002.48828125,16601.001953125,11707.20703125,12632.015625,17456.771484375,12729.68359375,11422.990234375,17199.650390625,15895.548828125,8992.25,14213.236328125,12218.859375,10853.22265625,11194.2529296875,14692.583984375,16415.03515625,18247.705078125,18202.935546875,13509.548828125,9901.193359375,14651.00390625,8303.09765625,12356.32421875,17172.17578125,16701.689453125,15997.416015625,11660.162109375,16100.572265625,13252.060546875,16356.4365234375,10168.181640625,15902.421875,15336.052734375,17232.685546875,11046.904296875,11845.427734375,20263.90625,11048.5703125,15706.908203125,9458.85546875,8337.2724609375,19414.095703125,11993.7041015625,14218.4296875,12112.330078125,18758.21484375,15529.251953125,15290.9033203125,16817.70703125,9950.1767578125,13722.4755859375,18717.986328125,10367.03515625,16812.478515625,15611.96484375,11210.736328125,10048.029296875,17904.451171875,20072.072265625,9924.3125,16614.3359375,13788.025390625,10559.9296875,11620.6171875,15110.6943359375,13853.75,15637.8212890625,9438.3671875,19375.884765625,13336.951171875,11624.388671875,13503.064453125,8286.7060546875,10050.810546875,11942.09375,16518.19140625,18773.224609375,15689.19921875,11982.5166015625,14343.95703125,10453.21875,11260.943359375,15121.654296875,15280.5625,16969.087890625,13642.552734375,9590.1337890625,14155.751953125,9203.9794921875,19452.666015625,9183.38671875,14041.4912109375,7623.70703125,16591.306640625,15160.67578125,16534.384765625,13812.61328125,14834.953125,10378.1484375,10771.08984375,16466.751953125,15320.455078125,13745.0908203125,14414.46875,10327.31640625,17527.42578125,16364.021484375,20561.81640625,15665.841796875,14867.955078125,9539.2607421875,15877.029296875,11578.146484375,10572.2529296875,13483.9453125,17134.787109375,18764.451171875,15936.1884765625,18384.966796875,14887.1591796875,11821.34765625,13162.1298828125,8857.5439453125,13478.474609375,18325.322265625,9303.771484375,9551.69921875,12617.515625,18579.953125,12287.173828125,10412.6796875,13148.6220703125,11825.048828125,15564.59765625,11801.9609375,13584.7734375,14752.783203125,14044.8212890625,8220.0126953125,16796.47265625,11662.56640625,7129.43115234375,8903.736328125,16156.85546875,17662.9765625,20931.728515625,10935.21875,11711.755859375,17274.806640625,12411.431640625,12342.578125,13117.4130859375,12023.669921875,9392.9462890625,11427.57421875,15987.9033203125,16306.103515625,15586.3994140625,15832.455078125,11779.9375,18406.0078125,12608.412109375,12065.103515625,15961.515625,14830.9951171875,9366.3818359375,12432.6435546875,19490.3125,15039.58203125,17367.314453125,13989.7451171875,10210.3779296875,17535.85546875,8809.4453125,19294.494140625,15977.9990234375,19867.01953125,11223.673828125,10075.251953125,13729.2236328125,11314.8984375,9329.5927734375,14107.1484375,17023.794921875,18841.634765625,20133.451171875,15490.349609375,15933.1171875,12863.087890625,16236.529296875,12599.978515625,11904.1552734375,18659.138671875,11313.9365234375,12398.1611328125,12261.470703125,12967.951171875,17946.818359375,18540.98046875,13892.640625,13093.240234375,16420.166015625,18257.416015625,15334.501953125,15453.8515625,9760.06640625,9944.2607421875,18043.810546875,11179.458984375,11169.6416015625,14712.2431640625,16171.556640625,10077.720703125,14997.8076171875,14851.71875,15722.982421875],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"markers\",\"x\":[7281.5056,5267.81815,12347.172,24513.09126,3736.4647,7358.17565,9788.8659,17085.2676,8211.1002,19798.05455,3077.0955,3385.39915,6837.3687,8538.28845,26392.26029,13012.20865,3227.1211,15170.069,11073.176,20773.62775,39556.4945,2134.9015,2198.18985,6555.07035,4340.4409,12622.1795,7740.337,12475.3513,3987.926,21082.16,1241.565,40103.89,17929.30337,8302.53565,3471.4096,5846.9176,13352.0998,9144.565,25656.57526,7726.854,13887.9685,30284.64294,5266.3656,12797.20962,1146.7966,7046.7222,8627.5411,39597.4072,12323.936,11454.0215,40904.1995,3171.6149,7445.918,13607.36875,27346.04207,12557.6053,10797.3362,5488.262,6282.235,40941.2854,1708.0014,23306.547,28950.4692,1664.9996,17361.7661,7345.084,18157.876,7256.7231,7626.993,26236.57997,7325.0482,1720.3537,7153.5539,6986.697,8232.6388,10370.91255,4889.0368,6474.013,1625.43375,10115.00885,10264.4421,9386.1613,18223.4512,3561.8889,23887.6627,3392.9768,1135.9407,1880.487,5630.45785,10156.7832,22144.032,62592.87309,9500.57305,46113.511,13390.559,4076.497,6389.37785,9880.068,4751.07,12629.8967,6664.68595,21344.8467,22395.74424,4466.6214,9249.4952,8703.456,38245.59327,36189.1017,19023.26,5080.096,9957.7216,6313.759,4151.0287,17748.5062,37133.8982,21472.4788,4350.5144,41661.602,6985.50695,23241.47453,39983.42595,9174.13565,6710.1919,1633.9618,47055.5321,2899.48935,12231.6136,5138.2567,4646.759,12913.9924,24476.47851,27808.7251,18033.9679,25309.489,6360.9936,20296.86345,10600.5483,9583.8933,34806.4677,5708.867,6457.8434,21259.37795,12928.7911,2904.088,3537.703,36085.219,10713.644,21232.18226,8457.818,18838.70366,3500.6123,6753.038,1256.299,45710.20785,11289.10925,1141.4451,12644.589,1121.8739,12495.29085,27218.43725,13129.60345,1837.2819,6311.952,12333.828,18218.16139,48173.361,6781.3542,3994.1778,1621.8827,11165.41765,9964.06,7441.501,5240.765,4686.3887,18972.495,6393.60345,3732.6251,19144.57652,7633.7206,11093.6229,46889.2612,11931.12525,4454.40265,1149.3959,34779.615,3277.161,27533.9129,10959.33,19594.80965,2483.736,10065.413,7512.267,19442.3535,7209.4918,11299.343,10231.4999,14711.7438,4906.40965,13415.0381,6123.5688,5469.0066,1705.6245,12646.207,6435.6237,11482.63485,44400.4064,17663.1442,14590.63205,38711.0,2166.732,8116.68,38746.3551,5584.3057,6775.961,55135.40209,18903.49141,5920.1041,3378.91,13430.265,11353.2276,5253.524,4738.2682,2196.4732,11987.1682,1515.3449,17178.6824,12268.63225,14133.03775,7421.19455,36021.0112,5966.8874,17081.08,6059.173,37484.4493,10338.9316,2855.43755,13616.3586,29523.1656,1607.5101,9504.3103,25517.11363,18310.742,9869.8102,38415.474,11554.2236,12609.88702,10594.2257,3238.4357,14394.5579,2585.85065,2927.0647,14254.6082,40932.4295,4883.866,7418.522,9630.397,8606.2174,5003.853,2680.9493,9549.5651,3847.674,24873.3849,5699.8375,42856.838,3201.24515,22493.65964,11566.30055,7160.3303,1646.4297,9617.66245,2203.47185,1137.011,9487.6442,7731.85785,7443.64305,2523.1695,34472.841,1969.614,7789.635,41676.0811,1694.7964,1906.35825,11763.0009,9225.2564,21098.55405,2709.1119,4992.3764,6600.20595,18608.262,2459.7201,8252.2843,4441.21315,5425.02335,4058.71245,15518.18025,4544.2348,19199.944,8347.1643,5478.0368,14007.222,42303.69215,5377.4578,1628.4709,16420.49455,10436.096,9778.3472,15612.19335,7261.741,11015.1747,10493.9458,6203.90175,10197.7722,6770.1925,1972.95,9304.7019,29141.3603,46255.1125,11013.7119,8162.71625,12244.531,5662.225,5383.536,12094.478,14478.33015,3208.787,7639.41745,2897.3235,29330.98315,11534.87265,1137.4697,23568.272,13429.0354,9222.4026,36397.576,3956.07145,10965.446,9377.9047,2801.2588,44585.45587,23967.38305,13228.84695,2352.96845,6334.34355,11938.25595,6338.0756,20462.99766,8068.185,4949.7587,9863.4718,39871.7043,19361.9988,6184.2994,1728.897,7323.734819,14418.2804,5246.047,38344.566,7144.86265,2207.69745,3579.8287,10325.206,8310.83915,3481.868,4399.731,5385.3379,4766.022,16232.847,13974.45555,10795.93733,3021.80915,8615.3,9361.3268,35585.576,7147.105,4239.89265,11735.87905,17128.42608,5375.038,10791.96,3443.064,5415.6612,7727.2532,18765.87545,6358.77645,5002.7827,14449.8544,2597.779,13047.33235,51194.55914,2331.519,11946.6259,8964.06055,7160.094,13725.47184,8825.086,11411.685,8930.93455,24106.91255,17878.90068,22462.04375,1391.5287,8240.5896],\"y\":[13134.724609375,12236.859375,18883.916015625,16879.455078125,10722.2216796875,14171.611328125,14777.1005859375,9897.9033203125,13746.3193359375,13026.587890625,9342.3720703125,10554.59375,13563.142578125,14818.208984375,16323.779296875,16352.1708984375,11752.134765625,20524.91015625,16045.19921875,13208.064453125,15376.748046875,10486.2265625,8545.291015625,12776.1650390625,11120.216796875,17522.171875,16161.4228515625,15257.25,11905.1015625,13817.986328125,7733.0400390625,15304.498046875,15032.55859375,13393.923828125,13680.08984375,15612.794921875,18085.849609375,17272.470703125,14935.54296875,14353.220703125,19460.748046875,15258.0556640625,12042.05859375,15473.978515625,11778.8876953125,13587.5859375,13244.1376953125,15052.919921875,16412.939453125,15878.796875,13205.3037109375,11987.2900390625,15539.603515625,17679.98828125,16730.18359375,18320.056640625,16141.9150390625,13681.0341796875,13851.75,15780.412109375,8616.6923828125,15432.375,17928.638671875,9887.619140625,11389.826171875,15015.34765625,12006.9140625,14489.58984375,13959.455078125,13906.474609375,13155.267578125,9889.4697265625,13652.162109375,14614.748046875,14649.091796875,16220.865234375,12941.49609375,15329.3037109375,7972.08447265625,16221.900390625,15865.89453125,17385.818359375,9955.958984375,10129.822265625,15449.30078125,11187.04296875,10323.005859375,10212.43359375,13889.541015625,14426.3525390625,14534.0859375,15443.4296875,14458.62109375,16979.373046875,18302.322265625,12612.59375,13317.443359375,17551.66015625,13007.0048828125,16583.322265625,13900.654296875,9253.6591796875,9486.978515625,11770.11328125,15113.669921875,15381.97265625,14244.521484375,11745.0703125,12449.5751953125,10567.1025390625,15612.396484375,13662.396484375,12597.1416015625,9764.076171875,12793.9853515625,13096.5869140625,12032.0498046875,16116.85546875,12326.548828125,10246.650390625,14380.68359375,16257.220703125,12968.986328125,11302.0390625,19810.1015625,11107.158203125,17718.1640625,13792.052734375,13268.5185546875,16736.23828125,16536.98828125,17745.279296875,10432.2529296875,16009.716796875,15193.763671875,12710.44921875,16484.5234375,16405.248046875,12283.650390625,11246.0263671875,13660.837890625,13913.8046875,18337.1015625,11171.328125,10808.654296875,12340.20703125,17697.076171875,15174.509765625,16792.048828125,9685.201171875,12530.8017578125,10824.283203125,9709.0126953125,18804.029296875,17208.212890625,11061.19921875,18292.2890625,8436.5107421875,17931.337890625,17269.294921875,16605.353515625,12890.4638671875,13158.6484375,17094.35546875,11424.048828125,19864.509765625,15239.462890625,14832.7646484375,9682.1142578125,16062.3359375,16462.4375,15128.9580078125,12869.427734375,15857.41015625,12733.74609375,13884.1376953125,8798.9970703125,15501.947265625,15123.103515625,19048.740234375,19778.150390625,16307.876953125,12051.927734375,12127.478515625,10975.884765625,12882.072265625,17433.412109375,15817.146484375,13073.990234375,10158.4755859375,14795.5908203125,14158.5380859375,12679.86328125,14119.51953125,16821.734375,16572.40625,8753.4599609375,10005.2109375,15996.7939453125,13106.126953125,12295.8701171875,8297.9267578125,18615.3125,17007.07421875,10194.7490234375,17809.669921875,11507.658203125,18161.4765625,13942.0478515625,11102.962890625,15498.607421875,13890.162109375,14376.25,14254.76953125,13714.23046875,11602.775390625,14884.734375,9039.1357421875,17562.943359375,17401.158203125,12845.353515625,12530.7060546875,8315.0712890625,18133.896484375,9017.333984375,9599.158203125,17376.626953125,11014.9638671875,13927.3955078125,11767.6416015625,13390.6796875,9533.419921875,12959.6083984375,12604.52734375,16672.501953125,11117.791015625,16073.515625,17969.939453125,7754.60888671875,18616.677734375,15768.19921875,11472.88671875,16130.869140625,14369.4912109375,17050.572265625,10308.2646484375,17885.158203125,13269.533203125,17699.744140625,8346.91015625,14514.138671875,17127.794921875,14473.328125,11986.62890625,14584.986328125,16428.2890625,14355.947265625,12023.375,8813.7724609375,18847.923828125,10584.41015625,15622.439453125,14816.99609375,17229.130859375,12780.068359375,9610.396484375,19053.99609375,15824.97265625,10787.845703125,15365.66015625,9253.6591796875,10466.54296875,16381.591796875,15643.9931640625,15315.6318359375,10461.919921875,11552.4580078125,9372.1640625,15454.5634765625,16917.4296875,6845.77294921875,9953.126953125,17395.05859375,14594.5693359375,14276.234375,8684.421875,11129.4501953125,14669.2265625,12125.4521484375,9729.4208984375,12800.5546875,11580.857421875,12823.68359375,12257.228515625,9010.22265625,10175.541015625,12184.982421875,17009.51171875,13123.12109375,18084.318359375,17200.84375,13942.0390625,8379.396484375,10210.859375,16443.216796875,15561.091796875,18948.92578125,15162.5400390625,15481.4521484375,15186.654296875,13237.0107421875,14543.5546875,13742.5693359375,9819.5546875,17467.302734375,17931.025390625,17927.021484375,15359.53515625,14712.3583984375,18378.736328125,15736.900390625,12634.822265625,15419.92578125,14615.4443359375,9031.9404296875,14648.56640625,10909.3408203125,18104.537109375,14839.2080078125,10528.0595703125,15504.53125,17873.96875,14757.0009765625,11089.7109375,11490.63671875,16637.361328125,16278.529296875,10229.912109375,13405.7021484375,15792.06640625,18483.158203125,9542.236328125,13819.5234375,17264.173828125,14567.4716796875,18295.103515625,16392.849609375,13587.53515625,15280.828125,15327.01953125,12944.935546875,12976.123046875,7278.5615234375,9076.5673828125,20497.376953125,13577.794921875,12390.2412109375,13064.044921875,9820.353515625,10953.34375,17989.904296875,14663.73046875,10082.4453125,13131.26953125,11942.72265625,10152.0546875,9771.3984375,18444.724609375,10811.8984375,11523.908203125,14747.095703125,14439.056640625,11705.8388671875,13426.482421875,11504.13671875,17045.779296875,12027.8291015625,13018.0205078125,15201.4482421875,11654.92578125,13044.474609375,15208.18359375,11464.6328125,14443.912109375,12141.25,16389.89453125,9301.419921875,16076.564453125,13198.2412109375,9287.029296875,18002.87890625,15722.23046875,15531.8701171875,12329.85546875,15031.6083984375,19077.86328125,15044.02734375,15078.228515625,12698.599609375,14143.0712890625,10710.2216796875,15715.37109375],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predichos\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predichos\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de entrenamiento\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de prueba\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"shapes\":[{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":7129.43115234375,\"x1\":20999.587890625,\"y0\":7129.43115234375,\"y1\":20999.587890625},{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":6845.77294921875,\"x1\":20524.91015625,\"xref\":\"x2\",\"y0\":6845.77294921875,\"y1\":20524.91015625,\"yref\":\"y2\"}],\"title\":{\"text\":\"Primera red neuronal\"},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1dbd5ffa-4075-480f-a2e2-f44b0d47c7eb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualmente, se confirma lo que la $R^2$ nos indicó, se observan tres grupos de datos, y la red sólo aprendio a \"predecir correctamente\" uno de ellos.\n",
        "\n",
        "Crearemos otra red con más capas esperando que se comporte mejor."
      ],
      "metadata": {
        "id": "HFWclg-Ty9kQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segunda Red Neuronal"
      ],
      "metadata": {
        "id": "nQTwmhQpCn3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo/Aquitectura"
      ],
      "metadata": {
        "id": "UCrkPP1Wz17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para esta segunda red neuronal, estamos agregando 3 capas densas con 64 neuronas cada una, es decir, el modelo va a tener 5 capas en total:\n",
        "\n",
        "1. Una capa de entrada,\n",
        "2. tres capas ocultas, y\n",
        "3. una capa de salida."
      ],
      "metadata": {
        "id": "2R06qLjozZOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "tf.random.set_seed(10)\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "     layers.Input([X_train_tr.shape[1]]),\n",
        "     layers.Dense(64, activation='relu'),\n",
        "     layers.Dense(64, activation='relu'),\n",
        "     layers.Dense(64, activation='relu'),     \n",
        "     layers.Dense(1)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "FtTZuFszsFtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = keras.optimizers.Adam(.01), \n",
        "    loss = 'mse', \n",
        "    metrics = ['mae']\n",
        ")"
      ],
      "metadata": {
        "id": "WoVJrNCTy8Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "xiGlEe4Cy__e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b45676c-1d18-42ac-b7a0-c7b2f572f9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 64)                576       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,961\n",
            "Trainable params: 8,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "\n",
        "# weights_filepath = os.path.join('/content/drive/MyDrive/checkpoints/', 'model_2')\n",
        "\n",
        "#model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "#    filepath = weights_filepath, \n",
        "#    save_best_only = True, \n",
        "#    save_weights_only = True, \n",
        "#    monitor = 'val_mae', \n",
        "#    mode = 'min'\n",
        "#)\n"
      ],
      "metadata": {
        "id": "W61odIylz73J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "qQNwN-yQz8aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_tr, y_train, \n",
        "    epochs=epochs, \n",
        "    validation_split=.2, \n",
        "#    callbacks=[model_checkpoint_callback]\n",
        "    )"
      ],
      "metadata": {
        "id": "jG40cMrgzB4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c422839a-9d2a-4731-feab-c124db90f6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "24/24 [==============================] - 1s 9ms/step - loss: 290971008.0000 - mae: 11875.2041 - val_loss: 127459592.0000 - val_mae: 8538.9385\n",
            "Epoch 2/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 143272096.0000 - mae: 9402.0283 - val_loss: 126440896.0000 - val_mae: 8402.3633\n",
            "Epoch 3/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135644624.0000 - mae: 9458.7236 - val_loss: 124784616.0000 - val_mae: 8468.6592\n",
            "Epoch 4/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132211128.0000 - mae: 9474.9141 - val_loss: 123875232.0000 - val_mae: 8269.6895\n",
            "Epoch 5/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 130426632.0000 - mae: 9338.9160 - val_loss: 121591816.0000 - val_mae: 8537.9717\n",
            "Epoch 6/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 128815632.0000 - mae: 9182.7051 - val_loss: 119524736.0000 - val_mae: 8380.2158\n",
            "Epoch 7/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 127964120.0000 - mae: 9010.1777 - val_loss: 117587360.0000 - val_mae: 8158.5078\n",
            "Epoch 8/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 122557048.0000 - mae: 8895.3213 - val_loss: 113477968.0000 - val_mae: 8499.4482\n",
            "Epoch 9/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 117473096.0000 - mae: 8618.9053 - val_loss: 109928232.0000 - val_mae: 8838.5508\n",
            "Epoch 10/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 109192120.0000 - mae: 8256.0332 - val_loss: 102645784.0000 - val_mae: 8660.7705\n",
            "Epoch 11/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 94656104.0000 - mae: 7799.3774 - val_loss: 81963152.0000 - val_mae: 7231.5586\n",
            "Epoch 12/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 74334648.0000 - mae: 6578.0527 - val_loss: 62294584.0000 - val_mae: 6625.8403\n",
            "Epoch 13/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 53812792.0000 - mae: 5678.7295 - val_loss: 43987244.0000 - val_mae: 4955.0713\n",
            "Epoch 14/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 49828164.0000 - mae: 5120.2217 - val_loss: 42016540.0000 - val_mae: 3828.5090\n",
            "Epoch 15/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 43198720.0000 - mae: 4685.2803 - val_loss: 39131064.0000 - val_mae: 4148.8306\n",
            "Epoch 16/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 41276268.0000 - mae: 4474.3511 - val_loss: 38639868.0000 - val_mae: 4287.2476\n",
            "Epoch 17/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 40177420.0000 - mae: 4380.0078 - val_loss: 39673588.0000 - val_mae: 3535.9810\n",
            "Epoch 18/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 39828760.0000 - mae: 4381.0186 - val_loss: 38783492.0000 - val_mae: 3501.0715\n",
            "Epoch 19/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 40657136.0000 - mae: 4426.6465 - val_loss: 38304480.0000 - val_mae: 3681.2126\n",
            "Epoch 20/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 39664452.0000 - mae: 4247.8213 - val_loss: 39414912.0000 - val_mae: 4493.8066\n",
            "Epoch 21/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 40735672.0000 - mae: 4350.5103 - val_loss: 38200684.0000 - val_mae: 3574.7012\n",
            "Epoch 22/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 40491412.0000 - mae: 4257.0684 - val_loss: 38035300.0000 - val_mae: 4235.4839\n",
            "Epoch 23/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 38351972.0000 - mae: 4309.7334 - val_loss: 37535576.0000 - val_mae: 3630.5669\n",
            "Epoch 24/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 38015992.0000 - mae: 4084.6216 - val_loss: 39384260.0000 - val_mae: 4542.8994\n",
            "Epoch 25/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 39313836.0000 - mae: 4211.6143 - val_loss: 38474748.0000 - val_mae: 3385.9900\n",
            "Epoch 26/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 37742260.0000 - mae: 3991.9236 - val_loss: 38578000.0000 - val_mae: 4061.4873\n",
            "Epoch 27/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36802752.0000 - mae: 4095.2937 - val_loss: 41556908.0000 - val_mae: 4637.4072\n",
            "Epoch 28/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 38669468.0000 - mae: 4185.3125 - val_loss: 37553104.0000 - val_mae: 4158.3311\n",
            "Epoch 29/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36160184.0000 - mae: 4055.7625 - val_loss: 37564536.0000 - val_mae: 4250.5732\n",
            "Epoch 30/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 38942876.0000 - mae: 4077.9170 - val_loss: 36361528.0000 - val_mae: 3676.2783\n",
            "Epoch 31/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 35772296.0000 - mae: 3949.7498 - val_loss: 38866136.0000 - val_mae: 4428.4380\n",
            "Epoch 32/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35807612.0000 - mae: 4009.0146 - val_loss: 39435996.0000 - val_mae: 3254.4331\n",
            "Epoch 33/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36954876.0000 - mae: 3982.2969 - val_loss: 36152448.0000 - val_mae: 4038.1899\n",
            "Epoch 34/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36158800.0000 - mae: 3861.6023 - val_loss: 35687540.0000 - val_mae: 3968.6323\n",
            "Epoch 35/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36089692.0000 - mae: 3857.4404 - val_loss: 37069616.0000 - val_mae: 4483.2930\n",
            "Epoch 36/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36240292.0000 - mae: 3961.8625 - val_loss: 37640844.0000 - val_mae: 3229.1719\n",
            "Epoch 37/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36167852.0000 - mae: 3983.5762 - val_loss: 43367336.0000 - val_mae: 4851.7822\n",
            "Epoch 38/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36948000.0000 - mae: 4094.3394 - val_loss: 40606220.0000 - val_mae: 4677.6143\n",
            "Epoch 39/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 34714696.0000 - mae: 3889.8223 - val_loss: 39286480.0000 - val_mae: 3156.6831\n",
            "Epoch 40/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36794748.0000 - mae: 3932.9653 - val_loss: 37092912.0000 - val_mae: 3751.8877\n",
            "Epoch 41/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35821452.0000 - mae: 3952.2427 - val_loss: 34471444.0000 - val_mae: 3743.6055\n",
            "Epoch 42/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 34274072.0000 - mae: 3827.2654 - val_loss: 35923036.0000 - val_mae: 3178.4626\n",
            "Epoch 43/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36675432.0000 - mae: 3974.7688 - val_loss: 34140584.0000 - val_mae: 3351.4868\n",
            "Epoch 44/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 34089268.0000 - mae: 3816.9351 - val_loss: 34361444.0000 - val_mae: 3852.0388\n",
            "Epoch 45/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33263184.0000 - mae: 3744.3240 - val_loss: 33517020.0000 - val_mae: 3755.9436\n",
            "Epoch 46/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33477852.0000 - mae: 3670.9980 - val_loss: 35945460.0000 - val_mae: 4307.3018\n",
            "Epoch 47/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33774872.0000 - mae: 3752.7878 - val_loss: 33764420.0000 - val_mae: 4005.5127\n",
            "Epoch 48/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 32755032.0000 - mae: 3766.7063 - val_loss: 33505084.0000 - val_mae: 3312.5591\n",
            "Epoch 49/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 32814126.0000 - mae: 3677.7869 - val_loss: 34384072.0000 - val_mae: 3743.6431\n",
            "Epoch 50/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35025508.0000 - mae: 3841.4846 - val_loss: 34845488.0000 - val_mae: 4380.5752\n",
            "Epoch 51/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 32724118.0000 - mae: 3691.4529 - val_loss: 32663350.0000 - val_mae: 3435.2356\n",
            "Epoch 52/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 32659760.0000 - mae: 3714.0945 - val_loss: 40424384.0000 - val_mae: 4729.4302\n",
            "Epoch 53/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33386408.0000 - mae: 3719.1548 - val_loss: 32150542.0000 - val_mae: 3628.0127\n",
            "Epoch 54/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 32716260.0000 - mae: 3696.0486 - val_loss: 31702304.0000 - val_mae: 3244.1616\n",
            "Epoch 55/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33730396.0000 - mae: 3695.3091 - val_loss: 32737504.0000 - val_mae: 3090.8457\n",
            "Epoch 56/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31890448.0000 - mae: 3655.8931 - val_loss: 31532494.0000 - val_mae: 3348.6936\n",
            "Epoch 57/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 32765604.0000 - mae: 3553.3096 - val_loss: 37619120.0000 - val_mae: 4352.1616\n",
            "Epoch 58/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33803016.0000 - mae: 3634.9023 - val_loss: 38278808.0000 - val_mae: 4433.9111\n",
            "Epoch 59/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30761384.0000 - mae: 3618.2656 - val_loss: 31009896.0000 - val_mae: 3508.4087\n",
            "Epoch 60/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 31274282.0000 - mae: 3439.4375 - val_loss: 32687246.0000 - val_mae: 3908.5854\n",
            "Epoch 61/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30414032.0000 - mae: 3603.6506 - val_loss: 30030162.0000 - val_mae: 3413.7878\n",
            "Epoch 62/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 32190392.0000 - mae: 3569.9473 - val_loss: 34792428.0000 - val_mae: 3735.9819\n",
            "Epoch 63/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36175464.0000 - mae: 3806.4519 - val_loss: 33717676.0000 - val_mae: 3978.8430\n",
            "Epoch 64/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31177726.0000 - mae: 3541.1887 - val_loss: 31245404.0000 - val_mae: 3320.6360\n",
            "Epoch 65/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30515936.0000 - mae: 3483.9485 - val_loss: 32405964.0000 - val_mae: 3564.1270\n",
            "Epoch 66/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31847612.0000 - mae: 3588.1279 - val_loss: 31505320.0000 - val_mae: 2884.2778\n",
            "Epoch 67/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36081780.0000 - mae: 3702.8105 - val_loss: 31039014.0000 - val_mae: 3244.8318\n",
            "Epoch 68/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 46220144.0000 - mae: 4209.0059 - val_loss: 47126392.0000 - val_mae: 5379.1963\n",
            "Epoch 69/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35917188.0000 - mae: 3908.8857 - val_loss: 29374214.0000 - val_mae: 3682.0984\n",
            "Epoch 70/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30920690.0000 - mae: 3525.7239 - val_loss: 30683288.0000 - val_mae: 4230.0537\n",
            "Epoch 71/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31210690.0000 - mae: 3507.4492 - val_loss: 34510128.0000 - val_mae: 4438.1084\n",
            "Epoch 72/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30423448.0000 - mae: 3588.9839 - val_loss: 28063768.0000 - val_mae: 3426.3794\n",
            "Epoch 73/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30744152.0000 - mae: 3476.8984 - val_loss: 31393150.0000 - val_mae: 4141.7256\n",
            "Epoch 74/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30016010.0000 - mae: 3486.2920 - val_loss: 28601420.0000 - val_mae: 3571.0227\n",
            "Epoch 75/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29798876.0000 - mae: 3376.6387 - val_loss: 28739180.0000 - val_mae: 3875.4116\n",
            "Epoch 76/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31953076.0000 - mae: 3688.8694 - val_loss: 38177208.0000 - val_mae: 4379.3613\n",
            "Epoch 77/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30951500.0000 - mae: 3471.0137 - val_loss: 28000218.0000 - val_mae: 3584.5085\n",
            "Epoch 78/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28219382.0000 - mae: 3279.5474 - val_loss: 29441496.0000 - val_mae: 4130.4448\n",
            "Epoch 79/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29207678.0000 - mae: 3444.1057 - val_loss: 30083070.0000 - val_mae: 3050.7803\n",
            "Epoch 80/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 29315526.0000 - mae: 3392.5034 - val_loss: 29232462.0000 - val_mae: 3746.4980\n",
            "Epoch 81/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28311980.0000 - mae: 3407.7852 - val_loss: 27545166.0000 - val_mae: 3482.2354\n",
            "Epoch 82/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27432684.0000 - mae: 3263.0269 - val_loss: 25939062.0000 - val_mae: 2982.4722\n",
            "Epoch 83/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27891154.0000 - mae: 3386.6567 - val_loss: 27653784.0000 - val_mae: 2604.4646\n",
            "Epoch 84/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28053706.0000 - mae: 3238.2920 - val_loss: 25955254.0000 - val_mae: 3313.8752\n",
            "Epoch 85/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27173330.0000 - mae: 3257.5498 - val_loss: 26161934.0000 - val_mae: 3496.3796\n",
            "Epoch 86/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26337466.0000 - mae: 3247.8716 - val_loss: 26236010.0000 - val_mae: 2571.4702\n",
            "Epoch 87/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27799410.0000 - mae: 3170.7207 - val_loss: 25633650.0000 - val_mae: 3173.9275\n",
            "Epoch 88/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29859778.0000 - mae: 3412.4907 - val_loss: 26225528.0000 - val_mae: 3533.9014\n",
            "Epoch 89/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 26987602.0000 - mae: 3256.8823 - val_loss: 25556632.0000 - val_mae: 3429.7371\n",
            "Epoch 90/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26983364.0000 - mae: 3292.4998 - val_loss: 25101712.0000 - val_mae: 3429.6409\n",
            "Epoch 91/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27323454.0000 - mae: 3284.2019 - val_loss: 26968516.0000 - val_mae: 2730.2656\n",
            "Epoch 92/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26789752.0000 - mae: 3154.9839 - val_loss: 24654658.0000 - val_mae: 2790.7893\n",
            "Epoch 93/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31055256.0000 - mae: 3411.7029 - val_loss: 25166034.0000 - val_mae: 3121.6335\n",
            "Epoch 94/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31129860.0000 - mae: 3472.3821 - val_loss: 36602028.0000 - val_mae: 4692.0903\n",
            "Epoch 95/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29341806.0000 - mae: 3502.9304 - val_loss: 24088690.0000 - val_mae: 3040.1150\n",
            "Epoch 96/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26290516.0000 - mae: 3087.4426 - val_loss: 25061420.0000 - val_mae: 3063.1794\n",
            "Epoch 97/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26883712.0000 - mae: 3177.8843 - val_loss: 25415890.0000 - val_mae: 2914.8008\n",
            "Epoch 98/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25976564.0000 - mae: 3015.9692 - val_loss: 26331658.0000 - val_mae: 3795.5312\n",
            "Epoch 99/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26486022.0000 - mae: 3273.8367 - val_loss: 25171620.0000 - val_mae: 2749.1497\n",
            "Epoch 100/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26034598.0000 - mae: 3113.1177 - val_loss: 27245310.0000 - val_mae: 4092.6221\n",
            "Epoch 101/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25990580.0000 - mae: 3136.6052 - val_loss: 24634308.0000 - val_mae: 3546.1970\n",
            "Epoch 102/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 26421090.0000 - mae: 3251.9446 - val_loss: 23911394.0000 - val_mae: 2846.9617\n",
            "Epoch 103/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26825200.0000 - mae: 3246.8521 - val_loss: 24027552.0000 - val_mae: 3161.0027\n",
            "Epoch 104/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25186824.0000 - mae: 3002.1550 - val_loss: 24195186.0000 - val_mae: 3069.1587\n",
            "Epoch 105/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25911194.0000 - mae: 3186.8823 - val_loss: 23209166.0000 - val_mae: 3020.2759\n",
            "Epoch 106/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24956898.0000 - mae: 3140.9148 - val_loss: 24006196.0000 - val_mae: 2784.2041\n",
            "Epoch 107/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25982376.0000 - mae: 3069.2507 - val_loss: 23450182.0000 - val_mae: 2746.3291\n",
            "Epoch 108/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25271244.0000 - mae: 3091.6846 - val_loss: 25746342.0000 - val_mae: 3504.3137\n",
            "Epoch 109/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27763690.0000 - mae: 3135.6475 - val_loss: 23809212.0000 - val_mae: 3335.3677\n",
            "Epoch 110/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28028066.0000 - mae: 3339.4827 - val_loss: 27172630.0000 - val_mae: 3306.4238\n",
            "Epoch 111/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26977290.0000 - mae: 3187.5427 - val_loss: 24521952.0000 - val_mae: 3424.4456\n",
            "Epoch 112/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25148898.0000 - mae: 3169.8525 - val_loss: 22616292.0000 - val_mae: 2684.1194\n",
            "Epoch 113/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27803156.0000 - mae: 3162.5034 - val_loss: 23557890.0000 - val_mae: 2332.9568\n",
            "Epoch 114/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26686462.0000 - mae: 3162.5552 - val_loss: 23070398.0000 - val_mae: 2362.5122\n",
            "Epoch 115/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25989538.0000 - mae: 3174.1375 - val_loss: 36018936.0000 - val_mae: 3207.7112\n",
            "Epoch 116/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30320602.0000 - mae: 3301.4919 - val_loss: 24530350.0000 - val_mae: 3168.7644\n",
            "Epoch 117/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25201368.0000 - mae: 3169.7576 - val_loss: 24329374.0000 - val_mae: 2916.3210\n",
            "Epoch 118/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 26170628.0000 - mae: 3129.9043 - val_loss: 23623700.0000 - val_mae: 2598.3813\n",
            "Epoch 119/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27484480.0000 - mae: 3206.5427 - val_loss: 22531768.0000 - val_mae: 2880.5854\n",
            "Epoch 120/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24894692.0000 - mae: 3021.9185 - val_loss: 24241974.0000 - val_mae: 3215.5872\n",
            "Epoch 121/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24800530.0000 - mae: 3060.9382 - val_loss: 22563058.0000 - val_mae: 2712.4873\n",
            "Epoch 122/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24484128.0000 - mae: 3021.1252 - val_loss: 22630662.0000 - val_mae: 2691.0085\n",
            "Epoch 123/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28833444.0000 - mae: 3422.0227 - val_loss: 23882738.0000 - val_mae: 2493.1162\n",
            "Epoch 124/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33338642.0000 - mae: 3379.6287 - val_loss: 29319552.0000 - val_mae: 3895.9744\n",
            "Epoch 125/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 27075554.0000 - mae: 3171.4856 - val_loss: 24257816.0000 - val_mae: 3351.0085\n",
            "Epoch 126/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 27239214.0000 - mae: 3269.6404 - val_loss: 34177340.0000 - val_mae: 2956.9355\n",
            "Epoch 127/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31086740.0000 - mae: 3523.7407 - val_loss: 34112520.0000 - val_mae: 2837.7163\n",
            "Epoch 128/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 34627052.0000 - mae: 3636.4197 - val_loss: 23914754.0000 - val_mae: 3033.8909\n",
            "Epoch 129/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 27234616.0000 - mae: 3370.8584 - val_loss: 26979062.0000 - val_mae: 3416.0977\n",
            "Epoch 130/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 28593012.0000 - mae: 3171.5869 - val_loss: 27791768.0000 - val_mae: 2931.2361\n",
            "Epoch 131/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25522308.0000 - mae: 3131.4141 - val_loss: 24098708.0000 - val_mae: 3211.9551\n",
            "Epoch 132/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25676148.0000 - mae: 3105.1868 - val_loss: 27329972.0000 - val_mae: 3781.2346\n",
            "Epoch 133/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26524100.0000 - mae: 3173.2634 - val_loss: 23569912.0000 - val_mae: 3080.9771\n",
            "Epoch 134/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26224760.0000 - mae: 3094.6582 - val_loss: 23214344.0000 - val_mae: 3120.0107\n",
            "Epoch 135/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27302094.0000 - mae: 3302.0178 - val_loss: 23048712.0000 - val_mae: 2709.7571\n",
            "Epoch 136/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25761330.0000 - mae: 3154.2249 - val_loss: 23844004.0000 - val_mae: 3278.6057\n",
            "Epoch 137/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24568394.0000 - mae: 3127.2900 - val_loss: 23372310.0000 - val_mae: 2573.8701\n",
            "Epoch 138/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25463286.0000 - mae: 2947.2878 - val_loss: 28015290.0000 - val_mae: 3518.7795\n",
            "Epoch 139/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28277708.0000 - mae: 3298.3215 - val_loss: 24629134.0000 - val_mae: 3236.4468\n",
            "Epoch 140/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26181142.0000 - mae: 3169.1121 - val_loss: 26016082.0000 - val_mae: 3311.5251\n",
            "Epoch 141/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27339222.0000 - mae: 3216.4961 - val_loss: 22814548.0000 - val_mae: 2648.6575\n",
            "Epoch 142/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26249188.0000 - mae: 3135.9753 - val_loss: 24764150.0000 - val_mae: 3498.1157\n",
            "Epoch 143/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26121126.0000 - mae: 3204.0220 - val_loss: 22827752.0000 - val_mae: 2974.1726\n",
            "Epoch 144/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27886590.0000 - mae: 3378.4436 - val_loss: 31157156.0000 - val_mae: 2833.5266\n",
            "Epoch 145/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29296714.0000 - mae: 3320.3938 - val_loss: 24132554.0000 - val_mae: 2846.2444\n",
            "Epoch 146/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26310450.0000 - mae: 3111.9219 - val_loss: 22535906.0000 - val_mae: 2889.8521\n",
            "Epoch 147/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25013770.0000 - mae: 3072.1062 - val_loss: 23614900.0000 - val_mae: 3216.2317\n",
            "Epoch 148/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26007546.0000 - mae: 3190.0166 - val_loss: 24860560.0000 - val_mae: 2431.8342\n",
            "Epoch 149/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26796448.0000 - mae: 3158.4150 - val_loss: 22925890.0000 - val_mae: 2574.7051\n",
            "Epoch 150/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24522960.0000 - mae: 3141.4121 - val_loss: 22805412.0000 - val_mae: 2877.9973\n",
            "Epoch 151/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26862704.0000 - mae: 3077.7817 - val_loss: 27943400.0000 - val_mae: 4013.4814\n",
            "Epoch 152/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26163274.0000 - mae: 3136.2085 - val_loss: 22691576.0000 - val_mae: 2929.2976\n",
            "Epoch 153/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 23921392.0000 - mae: 3082.3503 - val_loss: 22924072.0000 - val_mae: 2824.6912\n",
            "Epoch 154/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24945790.0000 - mae: 3025.3430 - val_loss: 22700660.0000 - val_mae: 2968.3208\n",
            "Epoch 155/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24258514.0000 - mae: 2951.5425 - val_loss: 22317160.0000 - val_mae: 2698.0386\n",
            "Epoch 156/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 26193476.0000 - mae: 3184.7361 - val_loss: 22550414.0000 - val_mae: 2578.9800\n",
            "Epoch 157/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26555234.0000 - mae: 3241.6338 - val_loss: 22445134.0000 - val_mae: 2914.5642\n",
            "Epoch 158/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24872094.0000 - mae: 2941.5044 - val_loss: 25232958.0000 - val_mae: 3649.9751\n",
            "Epoch 159/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26517928.0000 - mae: 3137.7173 - val_loss: 25769924.0000 - val_mae: 3997.9507\n",
            "Epoch 160/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25348840.0000 - mae: 3195.7053 - val_loss: 24260324.0000 - val_mae: 2896.4653\n",
            "Epoch 161/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 25577406.0000 - mae: 3051.3223 - val_loss: 24701998.0000 - val_mae: 2799.2939\n",
            "Epoch 162/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24212882.0000 - mae: 3032.8489 - val_loss: 22686700.0000 - val_mae: 2759.6084\n",
            "Epoch 163/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25523766.0000 - mae: 3145.9746 - val_loss: 23033458.0000 - val_mae: 2991.9409\n",
            "Epoch 164/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28278440.0000 - mae: 3253.9268 - val_loss: 27805020.0000 - val_mae: 2644.9934\n",
            "Epoch 165/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 25272356.0000 - mae: 3060.1646 - val_loss: 23993374.0000 - val_mae: 2704.9414\n",
            "Epoch 166/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 26005528.0000 - mae: 3140.7156 - val_loss: 22562752.0000 - val_mae: 3007.6799\n",
            "Epoch 167/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24730738.0000 - mae: 3056.5742 - val_loss: 22294234.0000 - val_mae: 2750.4412\n",
            "Epoch 168/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24839886.0000 - mae: 3190.5930 - val_loss: 23636714.0000 - val_mae: 2579.8823\n",
            "Epoch 169/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25435990.0000 - mae: 3085.2209 - val_loss: 22443514.0000 - val_mae: 2949.9587\n",
            "Epoch 170/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25752700.0000 - mae: 3188.9202 - val_loss: 22471370.0000 - val_mae: 2593.5366\n",
            "Epoch 171/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24769114.0000 - mae: 3147.5444 - val_loss: 24034486.0000 - val_mae: 2317.8398\n",
            "Epoch 172/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26713112.0000 - mae: 3180.4602 - val_loss: 24871560.0000 - val_mae: 2381.2732\n",
            "Epoch 173/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 25991690.0000 - mae: 3109.1321 - val_loss: 22191588.0000 - val_mae: 2646.1443\n",
            "Epoch 174/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25838828.0000 - mae: 3138.6887 - val_loss: 29090004.0000 - val_mae: 2711.9736\n",
            "Epoch 175/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25200620.0000 - mae: 3018.5601 - val_loss: 22801970.0000 - val_mae: 3074.0156\n",
            "Epoch 176/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 26486140.0000 - mae: 3218.4817 - val_loss: 23482366.0000 - val_mae: 2970.6721\n",
            "Epoch 177/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25241016.0000 - mae: 3049.1440 - val_loss: 22604610.0000 - val_mae: 2740.9019\n",
            "Epoch 178/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25845328.0000 - mae: 3094.9211 - val_loss: 23464822.0000 - val_mae: 2822.6377\n",
            "Epoch 179/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25003240.0000 - mae: 3153.0452 - val_loss: 23019574.0000 - val_mae: 3014.9216\n",
            "Epoch 180/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25452278.0000 - mae: 3018.2437 - val_loss: 25550426.0000 - val_mae: 2912.7051\n",
            "Epoch 181/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25387720.0000 - mae: 3135.8523 - val_loss: 23582006.0000 - val_mae: 2483.6279\n",
            "Epoch 182/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24767324.0000 - mae: 3005.7908 - val_loss: 23334670.0000 - val_mae: 3312.3374\n",
            "Epoch 183/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28152416.0000 - mae: 3289.0898 - val_loss: 22910480.0000 - val_mae: 3007.9841\n",
            "Epoch 184/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 32250004.0000 - mae: 3566.4983 - val_loss: 27025416.0000 - val_mae: 2777.7944\n",
            "Epoch 185/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31090532.0000 - mae: 3354.0198 - val_loss: 23604240.0000 - val_mae: 2925.2363\n",
            "Epoch 186/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24830626.0000 - mae: 3182.1541 - val_loss: 23178644.0000 - val_mae: 2630.2510\n",
            "Epoch 187/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25761196.0000 - mae: 3071.6902 - val_loss: 22702924.0000 - val_mae: 2771.3667\n",
            "Epoch 188/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24250414.0000 - mae: 3024.9324 - val_loss: 23807428.0000 - val_mae: 2894.6284\n",
            "Epoch 189/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24122838.0000 - mae: 3177.5293 - val_loss: 22906224.0000 - val_mae: 2400.1912\n",
            "Epoch 190/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24999556.0000 - mae: 2997.0691 - val_loss: 22568386.0000 - val_mae: 2963.5471\n",
            "Epoch 191/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24263786.0000 - mae: 2865.0139 - val_loss: 24206286.0000 - val_mae: 3544.5559\n",
            "Epoch 192/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 26454716.0000 - mae: 3156.0803 - val_loss: 26546688.0000 - val_mae: 3409.7556\n",
            "Epoch 193/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24775576.0000 - mae: 3173.4626 - val_loss: 22357548.0000 - val_mae: 2823.5303\n",
            "Epoch 194/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24724078.0000 - mae: 3084.8240 - val_loss: 23550278.0000 - val_mae: 3200.3545\n",
            "Epoch 195/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25108702.0000 - mae: 3091.5317 - val_loss: 22621186.0000 - val_mae: 2781.6689\n",
            "Epoch 196/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 32843302.0000 - mae: 3598.9219 - val_loss: 36181916.0000 - val_mae: 2943.2905\n",
            "Epoch 197/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28523528.0000 - mae: 3309.3938 - val_loss: 23114608.0000 - val_mae: 2871.7783\n",
            "Epoch 198/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24953538.0000 - mae: 3324.2783 - val_loss: 23997080.0000 - val_mae: 2265.4299\n",
            "Epoch 199/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25498600.0000 - mae: 2978.1387 - val_loss: 23608522.0000 - val_mae: 3024.4348\n",
            "Epoch 200/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26581654.0000 - mae: 3271.7959 - val_loss: 22517316.0000 - val_mae: 2793.6782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x = list(range(epochs)), y = history.history['mae'], name = 'entrenamiento')\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x = list(range(epochs)), y = history.history['val_mae'], name = 'validación')\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    hovermode = 'x unified', \n",
        "    legend_title = 'Conjunto de', \n",
        "    xaxis_title = 'Época', \n",
        "    yaxis_title = 'Error Absoluto Medio (MAE)', \n",
        "    title = 'MAE en cada época del entrenamiento<br><sup>Segunda red neuronal</sup>', \n",
        ")\n"
      ],
      "metadata": {
        "id": "t2WacT42zDVE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "1acb9d1f-e3e1-4aaf-a9d2-807601fae84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"dc99b3aa-c3f9-4d94-b308-e6c4b167c21f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"dc99b3aa-c3f9-4d94-b308-e6c4b167c21f\")) {                    Plotly.newPlot(                        \"dc99b3aa-c3f9-4d94-b308-e6c4b167c21f\",                        [{\"name\":\"entrenamiento\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[11875.2041015625,9402.0283203125,9458.7236328125,9474.9140625,9338.916015625,9182.705078125,9010.177734375,8895.3212890625,8618.9052734375,8256.033203125,7799.37744140625,6578.052734375,5678.7294921875,5120.2216796875,4685.2802734375,4474.35107421875,4380.0078125,4381.0185546875,4426.646484375,4247.8212890625,4350.51025390625,4257.068359375,4309.7333984375,4084.62158203125,4211.6142578125,3991.923583984375,4095.293701171875,4185.3125,4055.762451171875,4077.9169921875,3949.749755859375,4009.0146484375,3982.296875,3861.602294921875,3857.4404296875,3961.862548828125,3983.576171875,4094.33935546875,3889.822265625,3932.96533203125,3952.24267578125,3827.265380859375,3974.768798828125,3816.93505859375,3744.323974609375,3670.998046875,3752.787841796875,3766.706298828125,3677.786865234375,3841.484619140625,3691.452880859375,3714.094482421875,3719.15478515625,3696.048583984375,3695.30908203125,3655.89306640625,3553.3095703125,3634.90234375,3618.265625,3439.4375,3603.650634765625,3569.947265625,3806.451904296875,3541.188720703125,3483.948486328125,3588.1279296875,3702.810546875,4209.005859375,3908.8857421875,3525.723876953125,3507.44921875,3588.98388671875,3476.8984375,3486.2919921875,3376.638671875,3688.869384765625,3471.013671875,3279.54736328125,3444.105712890625,3392.50341796875,3407.78515625,3263.02685546875,3386.65673828125,3238.2919921875,3257.5498046875,3247.87158203125,3170.720703125,3412.49072265625,3256.88232421875,3292.499755859375,3284.201904296875,3154.98388671875,3411.702880859375,3472.382080078125,3502.930419921875,3087.442626953125,3177.88427734375,3015.96923828125,3273.836669921875,3113.11767578125,3136.605224609375,3251.944580078125,3246.85205078125,3002.155029296875,3186.88232421875,3140.914794921875,3069.250732421875,3091.6845703125,3135.6474609375,3339.482666015625,3187.542724609375,3169.8525390625,3162.50341796875,3162.55517578125,3174.137451171875,3301.491943359375,3169.757568359375,3129.904296875,3206.542724609375,3021.91845703125,3060.938232421875,3021.125244140625,3422.022705078125,3379.628662109375,3171.485595703125,3269.640380859375,3523.74072265625,3636.419677734375,3370.8583984375,3171.5869140625,3131.4140625,3105.186767578125,3173.263427734375,3094.658203125,3302.017822265625,3154.224853515625,3127.2900390625,2947.287841796875,3298.321533203125,3169.112060546875,3216.49609375,3135.975341796875,3204.02197265625,3378.443603515625,3320.393798828125,3111.921875,3072.106201171875,3190.0166015625,3158.4150390625,3141.412109375,3077.78173828125,3136.20849609375,3082.350341796875,3025.343017578125,2951.54248046875,3184.736083984375,3241.6337890625,2941.50439453125,3137.71728515625,3195.705322265625,3051.322265625,3032.848876953125,3145.974609375,3253.9267578125,3060.16455078125,3140.715576171875,3056.57421875,3190.593017578125,3085.220947265625,3188.920166015625,3147.54443359375,3180.460205078125,3109.132080078125,3138.688720703125,3018.56005859375,3218.481689453125,3049.14404296875,3094.921142578125,3153.045166015625,3018.24365234375,3135.852294921875,3005.790771484375,3289.08984375,3566.498291015625,3354.019775390625,3182.154052734375,3071.690185546875,3024.932373046875,3177.529296875,2997.069091796875,2865.013916015625,3156.080322265625,3173.462646484375,3084.823974609375,3091.53173828125,3598.921875,3309.393798828125,3324.2783203125,2978.138671875,3271.7958984375],\"type\":\"scatter\"},{\"name\":\"validaci\\u00f3n\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[8538.9384765625,8402.36328125,8468.6591796875,8269.689453125,8537.9716796875,8380.2158203125,8158.5078125,8499.4482421875,8838.55078125,8660.7705078125,7231.55859375,6625.84033203125,4955.0712890625,3828.509033203125,4148.83056640625,4287.24755859375,3535.98095703125,3501.071533203125,3681.212646484375,4493.806640625,3574.701171875,4235.48388671875,3630.56689453125,4542.8994140625,3385.989990234375,4061.4873046875,4637.4072265625,4158.3310546875,4250.5732421875,3676.2783203125,4428.43798828125,3254.43310546875,4038.18994140625,3968.63232421875,4483.29296875,3229.171875,4851.7822265625,4677.6142578125,3156.68310546875,3751.8876953125,3743.60546875,3178.462646484375,3351.48681640625,3852.038818359375,3755.943603515625,4307.3017578125,4005.5126953125,3312.55908203125,3743.64306640625,4380.5751953125,3435.235595703125,4729.43017578125,3628.0126953125,3244.16162109375,3090.845703125,3348.693603515625,4352.16162109375,4433.9111328125,3508.40869140625,3908.58544921875,3413.787841796875,3735.98193359375,3978.843017578125,3320.635986328125,3564.126953125,2884.27783203125,3244.831787109375,5379.1962890625,3682.098388671875,4230.0537109375,4438.1083984375,3426.37939453125,4141.7255859375,3571.022705078125,3875.41162109375,4379.361328125,3584.508544921875,4130.44482421875,3050.7802734375,3746.498046875,3482.2353515625,2982.47216796875,2604.464599609375,3313.875244140625,3496.379638671875,2571.47021484375,3173.927490234375,3533.9013671875,3429.737060546875,3429.640869140625,2730.265625,2790.789306640625,3121.633544921875,4692.09033203125,3040.114990234375,3063.179443359375,2914.80078125,3795.53125,2749.149658203125,4092.6220703125,3546.197021484375,2846.961669921875,3161.002685546875,3069.15869140625,3020.27587890625,2784.2041015625,2746.3291015625,3504.313720703125,3335.36767578125,3306.423828125,3424.445556640625,2684.119384765625,2332.956787109375,2362.51220703125,3207.711181640625,3168.764404296875,2916.321044921875,2598.38134765625,2880.58544921875,3215.587158203125,2712.4873046875,2691.008544921875,2493.1162109375,3895.974365234375,3351.008544921875,2956.935546875,2837.71630859375,3033.890869140625,3416.09765625,2931.236083984375,3211.955078125,3781.234619140625,3080.97705078125,3120.0107421875,2709.757080078125,3278.605712890625,2573.8701171875,3518.779541015625,3236.44677734375,3311.525146484375,2648.657470703125,3498.11572265625,2974.172607421875,2833.526611328125,2846.244384765625,2889.85205078125,3216.231689453125,2431.834228515625,2574.705078125,2877.997314453125,4013.4814453125,2929.297607421875,2824.691162109375,2968.32080078125,2698.03857421875,2578.97998046875,2914.564208984375,3649.97509765625,3997.95068359375,2896.46533203125,2799.2939453125,2759.6083984375,2991.94091796875,2644.993408203125,2704.94140625,3007.679931640625,2750.441162109375,2579.88232421875,2949.958740234375,2593.53662109375,2317.83984375,2381.273193359375,2646.144287109375,2711.9736328125,3074.015625,2970.672119140625,2740.90185546875,2822.6376953125,3014.921630859375,2912.705078125,2483.6279296875,3312.33740234375,3007.984130859375,2777.79443359375,2925.236328125,2630.2509765625,2771.36669921875,2894.62841796875,2400.191162109375,2963.547119140625,3544.555908203125,3409.755615234375,2823.5302734375,3200.3544921875,2781.6689453125,2943.29052734375,2871.7783203125,2265.429931640625,3024.434814453125,2793.67822265625],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"hovermode\":\"x unified\",\"legend\":{\"title\":{\"text\":\"Conjunto de\"}},\"xaxis\":{\"title\":{\"text\":\"\\u00c9poca\"}},\"yaxis\":{\"title\":{\"text\":\"Error Absoluto Medio (MAE)\"}},\"title\":{\"text\":\"MAE en cada \\u00e9poca del entrenamiento<br><sup>Segunda red neuronal</sup>\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dc99b3aa-c3f9-4d94-b308-e6c4b167c21f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos un comportamiento \"errático\" en los valores del MAE a lo largo de cada época. Lo importante es que la gráfica muestra que el modelo sí aprendió (el valor de la métrica fue disminuyendo conforme avanzan las épocas de entrenamiento)."
      ],
      "metadata": {
        "id": "xqfE75-Z0AaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_weights(weights_filepath)"
      ],
      "metadata": {
        "id": "q1LAcmNC_DeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "2VPw8OQGCq-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_nn2 = model.predict(X_train_tr)[:, 0]\n",
        "y_pred_test_nn2 = model.predict(X_test_tr)[:, 0]"
      ],
      "metadata": {
        "id": "jas0YHGp_GlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métricas"
      ],
      "metadata": {
        "id": "aGTEa5WICt-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"MSE train: {mean_squared_error(y_train, y_pred_train_nn2)}\", \n",
        "    f\"MSE test: {mean_squared_error(y_test, y_pred_test_nn2)}\", \n",
        "    \"\",\n",
        "    f\"MAE train: {mean_absolute_error(y_train, y_pred_train_nn2)}\", \n",
        "    f\"MAE test: {mean_absolute_error(y_test, y_pred_test_nn2)}\", \n",
        "    sep = '\\n'\n",
        ")"
      ],
      "metadata": {
        "id": "2bvw6OYH_Ll8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1441f21b-f068-4140-9d1b-cc21f0e6cc5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE train: 23372883.4040356\n",
            "MSE test: 23407418.93006112\n",
            "\n",
            "MAE train: 2878.041126533704\n",
            "MAE test: 2947.652786269376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos una $R^2$ mejor que cuando ajustamos el modelo con la regresión lineal y el MAE se redujo casi a la mitad. Parece que esta red aprendió mejor a predecir los datos."
      ],
      "metadata": {
        "id": "yr7KSkge0aAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización"
      ],
      "metadata": {
        "id": "6kTOfNLmCvDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_evaluation(y_train, y_pred_train_nn2, y_test, y_pred_test_nn2, 'Segunda red neuronal')"
      ],
      "metadata": {
        "id": "OEvX6SnLztxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "728253c2-3e9d-422d-84b3-e7465849f0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"e5e11df9-2cd0-4129-b1d4-1ef3ff8bf84d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e5e11df9-2cd0-4129-b1d4-1ef3ff8bf84d\")) {                    Plotly.newPlot(                        \"e5e11df9-2cd0-4129-b1d4-1ef3ff8bf84d\",                        [{\"mode\":\"markers\",\"x\":[3167.45585,2689.4954,11576.13,16586.49771,6746.7425,5976.8311,5649.715,15161.5344,2007.945,19214.70553,16455.70785,10594.50155,27117.99378,4296.2712,7151.092,8410.04685,22331.5668,37165.1638,17468.9839,2494.022,6356.2707,11305.93455,12235.8392,14256.1928,13822.803,7173.35995,9140.951,1711.0268,5926.846,32108.66282,17942.106,43896.3763,8671.19125,6112.35295,15019.76005,11365.952,2709.24395,43254.41795,1263.249,3757.8448,20420.60465,2302.3,11658.11505,11394.06555,1967.0227,3353.4703,10269.46,18963.17192,31620.00106,30259.99556,6571.544,1632.56445,2755.02095,5934.3798,14319.031,19964.7463,6849.026,2498.4144,1631.6683,3766.8838,9625.92,3309.7926,37079.372,15359.1045,36124.5737,2842.76075,1622.1885,46599.1084,6948.7008,4500.33925,2128.43105,9095.06825,19673.33573,48824.45,8688.85885,3645.0894,2480.9791,2154.361,33307.5508,12643.3778,27375.90478,8556.907,37270.1512,6593.5083,9101.798,13019.16105,28868.6639,1621.3402,6571.02435,34828.654,4260.744,8765.249,4040.55825,9875.6804,4337.7352,11356.6609,9290.1395,34439.8559,8891.1395,7162.0122,8026.6666,35160.13457,13555.0049,4719.73655,7222.78625,2534.39375,1719.4363,1635.73365,1725.5523,1824.2854,8342.90875,12829.4551,12979.358,4391.652,19933.458,7623.518,7640.3092,28101.33305,8442.667,2721.3208,2803.69785,12044.342,1131.5066,11743.9341,5974.3847,2731.9122,26140.3603,4243.59005,4846.92015,28287.89766,20630.28351,2741.948,29186.48236,1261.859,6877.9801,14283.4594,4402.233,37742.5757,13217.0945,11163.568,13451.122,26467.09737,58571.07448,13204.28565,1639.5631,2473.3341,48970.2476,9910.35985,3044.2133,60021.39897,41034.2214,9877.6077,8125.7845,2020.5523,25382.297,34303.1672,7742.1098,10702.6424,8516.829,27322.73386,37701.8768,26125.67477,30184.9367,10107.2206,3070.8087,5152.134,4746.344,5910.944,4415.1588,14043.4767,16085.1275,8428.0693,39727.614,8283.6807,14210.53595,10355.641,12265.5069,6455.86265,11488.31695,4134.08245,9563.029,18804.7524,11856.4115,11264.541,1631.8212,38792.6856,42560.4304,2150.469,7050.642,42112.2356,41949.2441,4462.7218,5484.4673,20234.85475,1877.9294,11085.5868,8219.2039,13126.67745,5227.98875,3268.84665,12479.70895,6250.435,28923.13692,2136.88225,13937.6665,36580.28216,6652.5288,3947.4131,19444.2658,12741.16745,1832.094,13770.0979,6113.23105,6198.7518,38511.6283,14382.70905,16450.8947,7133.9025,37829.7242,6406.4107,6933.24225,2137.6536,12404.8791,10096.97,26109.32905,3906.127,44423.803,12124.9924,1253.936,34166.273,45702.02235,13470.86,2632.992,38709.176,9182.17,1826.843,7243.8136,12430.95335,10118.424,4827.90495,5312.16985,3213.62205,4779.6023,4931.647,23082.95533,4433.9159,7985.815,33750.2918,48675.5177,33475.81715,9872.701,44260.7499,10601.412,2117.33885,11070.535,33900.653,2639.0429,5428.7277,11837.16,9301.89355,35491.64,47291.055,9566.9909,14410.9321,20878.78443,42983.4585,10796.35025,11884.04858,11244.3769,3279.86855,6796.86325,27037.9141,10848.1343,10560.4917,44202.6536,9288.0267,9620.3307,5458.04645,36149.4835,11381.3254,21595.38229,21880.82,1977.815,36837.467,6640.54485,4347.02335,36219.40545,26926.5144,4435.0942,6128.79745,3597.596,1704.5681,16776.30405,2727.3951,7348.142,1242.816,45863.205,24671.66334,6186.127,19539.243,2566.4707,49577.6624,39836.519,6402.29135,8827.2099,17904.52705,6272.4772,6500.2359,2257.47525,2155.6815,2362.22905,1815.8759,1875.344,9724.53,11538.421,2219.4451,2156.7518,42969.8527,1708.92575,9722.7695,15820.699,11842.442,8582.3023,33907.548,38998.546,7345.7266,4463.2051,15828.82173,2867.1196,5438.7491,4564.19145,46718.16325,12592.5345,23563.01618,6185.3208,10976.24575,27941.28758,4189.1131,41097.16175,36197.699,15817.9857,26018.95052,3857.75925,3490.5491,6238.298,2396.0959,7419.4779,8413.46305,1880.07,7050.0213,4518.82625,11552.904,10982.5013,4673.3922,23807.2406,20149.3229,12949.1554,4753.6368,27000.98473,24180.9335,7731.4271,7749.1564,9644.2525,42211.1382,6940.90985,24869.8368,13457.9608,12224.35085,8551.347,22478.6,12925.886,5012.471,9414.92,14988.432,4618.0799,2730.10785,19749.38338,3556.9223,39241.442,17043.3414,8277.523,4830.63,7518.02535,10214.636,18767.7377,1242.26,4536.259,8965.79575,34617.84065,16796.41194,11848.141,8932.084,20009.63365,1759.338,45008.9555,37465.34375,46661.4424,11881.9696,5972.378,1252.407,3875.7341,1743.214,3981.9768,13635.6379,10381.4787,9048.0273,3704.3545,5209.57885,7265.7025,47462.894,11674.13,1737.376,2775.19215,3693.428,5028.1466,1639.5631,9058.7303,10226.2842,4529.477,1629.8335,12815.44495,24915.04626,14692.66935,7986.47525,8017.06115,12105.32,3554.203,10461.9794,5615.369,4795.6568,19515.5416,2103.08,4349.462,12523.6048,10072.05505,1136.3994,21677.28345,1704.70015,21774.32215,13063.883,5116.5004,10825.2537,3353.284,4149.736,13405.3903,10977.2063,14349.8544,11741.726,44641.1974,4762.329,7371.772,3176.8159,2850.68375,40273.6455,17560.37975,4433.3877,1526.312,1702.4553,6373.55735,10942.13205,8547.6913,8604.48365,3176.2877,11150.78,4527.18295,12233.828,9850.432,7196.867,11729.6795,21223.6758,10435.06525,5272.1758,2250.8352,33732.6867,14358.36437,8233.0975,17496.306,2789.0574,14901.5167,5124.1887,8027.968,13143.33665,43578.9394,10807.4863,8522.003,5400.9805,9447.3824,10601.63225,4237.12655,14001.2867,1615.7667,11840.77505,40182.246,15230.32405,4877.98105,28340.18885,2699.56835,9748.9106,8539.671,47403.88,20167.33603,17352.6803,4058.1161,16297.846,8596.8278,2497.0383,38282.7495,36898.73308,14474.675,13224.693,42124.5153,14571.8908,11253.421,1534.3045,18246.4955,32548.3405,3877.30425,11842.62375,38126.2465,16577.7795,24915.22085,8569.8618,7804.1605,10736.87075,8968.33,9991.03765,21195.818,11658.37915,8603.8234,13919.8229,4915.05985,2217.6012,1634.5734,4934.705,32787.45859,12269.68865,2217.46915,5373.36425,8601.3293,12142.5786,7077.1894,20709.02034,1633.0444,2130.6759,7537.1639,14426.07385,1261.442,5757.41345,2211.13075,8782.469,6748.5912,11833.7823,10928.849,5709.1644,2203.73595,5354.07465,23065.4207,6875.961,22412.6485,2055.3249,12950.0712,40003.33225,12363.547,4032.2407,5855.9025,17626.23951,11033.6617,3594.17085,11396.9002,41999.52,9391.346,3577.999,4504.6624,40720.55105,3392.3652,9264.797,22192.43711,10450.552,2416.955,5989.52365,20177.67113,21984.47061,8835.26495,36307.7983,18648.4217,39722.7462,12485.8009,5327.40025,6496.886,10579.711,6686.4313,10577.087,1980.07,11455.28,2221.56445,5397.6167,1986.9334,17179.522,3761.292,8733.22925,6082.405,9855.1314,6117.4945,9866.30485,13844.7972,7935.29115,2913.569,18328.2381,1682.597,39611.7577,2201.0971,8823.279,30063.58055,39047.285,24535.69855,3659.346,1163.4627,4718.20355,2020.177,24059.68019,3206.49135,7201.70085,11881.358,46130.5265,23288.9284,8520.026,8023.13545,42760.5022,21348.706,35147.52848,14451.83515,12096.6512,34254.05335,4005.4225,14001.1338,9715.841,12890.05765,48673.5588,7228.21565,19719.6947,39774.2763,2974.126,6548.19505,1964.78,3861.20965,2200.83085,20781.48892,19521.9682,13981.85035,8515.7587,14235.072,41919.097,13143.86485,25992.82104,12638.195,10959.6947,42111.6647,7729.64575,24227.33724,10106.13425,1842.519,16138.76205,9283.562,43943.8761,3484.331,48885.13561,1731.677,14394.39815,44501.3982,2404.7338,3591.48,10806.839,11944.59435,2464.6188,46151.1245,4894.7533,9193.8385,14119.62,4133.64165,11945.1327,20745.9891,25081.76784,2710.82855,11657.7189,13470.8044,9800.8882,23244.7902,19350.3689,4449.462,13880.949,16115.3045,33471.97189,2304.0022,47305.305,2138.0707,3056.3881,28468.91901,2395.17155,13887.204,21771.3423,8269.044,12222.8983,48549.17835,8605.3615,5325.651,20984.0936,10407.08585,12032.326,10422.91665,12648.7034,9282.4806,47896.79135,13393.756,1981.5819,9411.005,9541.69555,2902.9065,21978.6769,3161.454,9704.66805,8444.474,11187.6567,20277.80751,3366.6697,11436.73815,4747.0529,1917.3184,13041.921,18806.14547,2457.502,7337.748,4922.9159,1727.54,3756.6216,7954.517,25333.33284,11362.755,43813.8661,5969.723,1769.53165,2438.0552,13844.506,5261.46945,9634.538,10923.9332,6610.1097,34838.873,24393.6224,3410.324,12957.118,3260.199,28476.73499,8116.26885,8978.1851,4661.28635,19107.7796,12574.049,2866.091,8334.5896,16884.924,13747.87235,47496.49445,4357.04365,5836.5204,5472.449,21797.0004,8823.98575,19496.71917,13112.6048,2690.1138,4667.60765,10602.385,4428.88785,11512.405,11830.6072,3393.35635,2719.27975,46200.9851,12982.8747,18955.22017,10564.8845,8534.6718,2254.7967,4571.41305,11326.71487,6551.7501,11520.09985,2045.68525,48517.56315,21659.9301,4438.2634,36950.2567,2527.81865,4687.797,5125.2157,7448.40395,30942.1918,8334.45755,3972.9247,6196.448,16657.71745,27724.28875,6289.7549,39725.51805,11743.299,7624.63,2205.9808,6653.7886,14455.64405,47928.03,1712.227,4889.9995,2585.269,12629.1656,40419.0191,25678.77845,35069.37452,7682.67,1909.52745,2643.2685,10704.47,8988.15875,4320.41085,24603.04837,5693.4305,12730.9996,11286.5387,13831.1152,8062.764,39125.33225,15006.57945,8871.1517,3943.5954,32734.1863,4670.64,8798.593,8083.9198,11345.519,9432.9253,7441.053,2026.9741,7045.499,1632.03625,37607.5277,13462.52,2102.2647,2104.1134,6414.178,11363.2832,4562.8421,4719.52405,6067.12675,5257.50795,6666.243,1984.4533,3935.1799,23401.30575,6600.361,1627.28245,8124.4084,3062.50825,1727.785,3180.5101,43753.33705,30166.61817,15555.18875,4234.927,18259.216,12129.61415,5148.5526,11737.84884,5594.8455,3292.52985,2457.21115,35595.5898,24667.419,40974.1649,24520.264,10043.249,3172.018,36910.60803,6799.458,3866.8552,10141.1362,6079.6715,1744.465,19040.876,14313.8463,4266.1658,12146.971,3989.841,4561.1885,10085.846,1837.237,47269.854,23045.56616,12981.3457,3925.7582,3558.62025,5245.2269,4185.0979,11272.33139,5729.0053,11879.10405,16069.08475,63770.42801,8280.6227,11082.5772,4837.5823,8527.532,4074.4537,5031.26955,11090.7178,1532.4697,34672.1472,2322.6218,6858.4796,13224.05705,52590.82939,8059.6791,7526.70645,7152.6714,8944.1151,12029.2867,22218.1149,3046.062,1748.774,43921.1837,1674.6323,4137.5227,9447.25035,7650.77375,2261.5688,9861.025,5979.731,7147.4728],\"y\":[3395.2197265625,4499.1220703125,12477.6435546875,2679.577880859375,8659.619140625,7797.34716796875,6965.578125,17082.80078125,2352.241943359375,6875.80859375,16912.119140625,11814.3525390625,11704.4638671875,5097.8857421875,8223.9755859375,10440.0283203125,29891.634765625,33500.4609375,24779.447265625,3808.007568359375,8621.5224609375,12233.16796875,12960.1875,14458.177734375,14141.640625,9236.9609375,11478.8896484375,2256.207275390625,7203.07861328125,10450.802734375,20968.8359375,44147.4140625,10438.328125,8209.6982421875,15442.265625,12189.4609375,2774.931396484375,40123.91796875,2015.4278564453125,5283.13671875,6295.4375,3431.62353515625,13123.9140625,13268.982421875,2742.69091796875,4305.28955078125,12334.8935546875,5509.07470703125,15674.0576171875,14295.30859375,7754.6328125,2242.543212890625,3928.149658203125,8056.8369140625,14215.3603515625,24006.58984375,8366.55078125,4188.09326171875,2110.913330078125,5104.0927734375,10512.96484375,4430.87353515625,35951.28515625,17128.681640625,28514.25,3547.95556640625,2006.4805908203125,41830.99609375,9272.3291015625,6286.49658203125,2714.14404296875,10922.1796875,4778.09716796875,49695.59765625,10781.439453125,5263.12060546875,3578.63232421875,2604.542236328125,27310.91015625,13923.021484375,7413.58740234375,9722.3544921875,32645.43359375,9005.486328125,10226.419921875,14410.6875,32316.2734375,2118.8935546875,8222.970703125,32913.48046875,5102.3974609375,10415.8798828125,5516.560546875,11448.091796875,5422.2734375,12409.267578125,11364.7333984375,28372.921875,11169.162109375,9542.0888671875,9450.25,12092.2734375,14908.6376953125,5620.7138671875,8916.345703125,4284.373046875,2348.848876953125,2277.456298828125,2416.224365234375,3167.945068359375,10393.7001953125,11040.615234375,13463.91015625,4636.92236328125,27051.21875,8756.0517578125,9738.0859375,32588.02734375,10930.662109375,3958.694580078125,3034.103759765625,12845.89453125,1968.923095703125,12712.48046875,7781.7314453125,3251.525146484375,10171.828125,5796.39111328125,6271.6455078125,14528.8623046875,11264.1728515625,3331.0673828125,13464.96484375,2000.1148681640625,8420.107421875,15866.2109375,6061.048828125,33004.37890625,14436.216796875,12021.2998046875,13720.6650390625,12704.345703125,42425.0703125,14533.955078125,2393.62353515625,3518.044921875,48432.53515625,11143.048828125,3799.67578125,38291.7421875,38537.45703125,11911.9794921875,10480.00390625,2764.54736328125,28796.861328125,24790.072265625,10271.978515625,13035.8125,10723.3349609375,14305.626953125,33887.80078125,16568.669921875,36044.0703125,12403.529296875,4481.71923828125,6048.94873046875,6214.89697265625,7192.88720703125,6123.2041015625,15042.4931640625,17499.71875,10037.794921875,35214.0859375,10313.4052734375,14543.369140625,11384.4267578125,14225.0244140625,8136.884765625,13683.2314453125,5358.67919921875,11036.7314453125,7839.1279296875,13505.7890625,12572.671875,2112.59765625,41677.20703125,39871.94921875,2561.666015625,8250.76171875,46166.734375,38954.41796875,6512.3232421875,7705.47998046875,26519.908203125,2473.451171875,12579.0751953125,10385.197265625,2856.01904296875,6789.9404296875,4847.88916015625,13568.154296875,7796.06640625,13486.291015625,3996.529296875,14656.7783203125,17255.533203125,9063.5498046875,5286.98388671875,19673.736328125,14206.6416015625,2185.110595703125,14306.18359375,7566.19677734375,7831.4833984375,39735.10546875,15178.40234375,19483.34375,8718.6376953125,29368.80078125,8310.3466796875,8226.9541015625,3579.51953125,6171.15966796875,11741.3173828125,28790.4140625,4540.06884765625,40712.65234375,13041.578125,1913.0618896484375,30604.822265625,45718.40234375,13570.0537109375,3376.92578125,34060.36328125,10198.7783203125,2929.6875,9149.5283203125,13728.173828125,11098.951171875,5931.65576171875,7050.126953125,4098.86962890625,6376.8818359375,5716.47998046875,1980.208740234375,6389.64208984375,9681.033203125,27697.9296875,49219.4453125,26970.6484375,10813.7822265625,37700.26171875,11559.8916015625,2220.217529296875,11766.5712890625,29459.0078125,3481.19189453125,7620.99755859375,12417.322265625,11004.8349609375,30954.677734375,41349.265625,10774.4951171875,15494.76171875,10223.6171875,43871.9296875,12664.75390625,2658.70556640625,13378.2548828125,3855.955810546875,9011.58984375,26925.33984375,12948.9736328125,13176.5458984375,42164.5546875,11374.9443359375,11503.98828125,6993.19189453125,33739.9453125,13636.638671875,3110.7060546875,24281.935546875,3083.986572265625,36117.75,8641.0712890625,5743.96142578125,35151.67578125,26933.474609375,5753.64990234375,9163.2705078125,4177.4326171875,2202.259765625,17142.794921875,4006.833740234375,8629.9267578125,1793.51513671875,52274.640625,7750.81103515625,7484.00244140625,23852.748046875,3279.9267578125,44175.33203125,38636.0078125,7856.1708984375,10350.41796875,22023.080078125,8702.02734375,8416.1376953125,2739.07177734375,2928.288330078125,3509.03564453125,3081.10107421875,2178.037109375,10986.234375,12965.240234375,2489.155029296875,2937.00439453125,40455.00390625,2250.26513671875,11645.380859375,16598.12890625,12377.017578125,10855.046875,28236.08203125,32919.12890625,9330.2177734375,6296.31787109375,9347.302734375,4650.201171875,7766.77490234375,5425.689453125,36089.66015625,15784.0810546875,8626.595703125,8105.31005859375,12108.0380859375,15151.318359375,5330.51025390625,36214.40234375,34639.6328125,17041.984375,6113.46240234375,5585.33935546875,4766.69873046875,7463.90576171875,3419.818115234375,8877.708984375,10123.6875,2496.533203125,8814.91015625,6440.34765625,12777.4501953125,13036.6591796875,6560.0322265625,26172.55078125,25860.802734375,15484.7001953125,6410.55029296875,15425.5263671875,26217.892578125,10246.0400390625,9962.7783203125,10725.029296875,38283.12890625,8674.2587890625,27242.005859375,14903.845703125,12811.9169921875,9674.2353515625,29340.095703125,15136.1162109375,6139.0595703125,10519.9375,15570.7626953125,7388.3193359375,6004.71923828125,13038.74609375,5338.9345703125,39373.75,20283.56640625,9383.236328125,6408.0849609375,9219.37109375,11303.8515625,18351.3046875,1787.5379638671875,6136.7197265625,11142.068359375,30892.0859375,5164.57421875,12333.5322265625,9896.625,24258.193359375,2168.9921875,38373.6328125,34818.296875,44133.1484375,13964.77734375,7156.404296875,1896.6241455078125,5780.35107421875,1934.9249267578125,4999.98583984375,14674.4765625,12634.748046875,11462.3623046875,5725.88330078125,6532.22900390625,9920.51171875,48621.00390625,12322.1220703125,1870.6116943359375,3572.396484375,5487.63623046875,6906.900390625,2393.62353515625,11384.556640625,12408.216796875,5202.37158203125,2090.700439453125,14121.99609375,8637.2685546875,14711.94140625,9678.6572265625,9406.3193359375,13060.2216796875,4572.982421875,11837.951171875,6306.8076171875,6515.828125,21423.767578125,2671.334716796875,5593.580078125,13372.5869140625,11998.0234375,2022.8236083984375,23483.97265625,2203.714599609375,26839.87890625,13345.958984375,6728.37451171875,12136.8720703125,4296.705078125,5076.89306640625,14976.8232421875,13356.1142578125,15044.5546875,12763.552734375,42186.6953125,6341.58154296875,8617.11328125,4055.28662109375,3800.102294921875,36182.58203125,22557.5859375,6343.3251953125,2414.369873046875,2178.984375,9260.234375,11935.029296875,10697.9404296875,10803.3798828125,4051.099853515625,12110.5634765625,5834.0498046875,12718.2314453125,10977.0556640625,8671.421875,13540.9736328125,22784.52734375,12299.1455078125,6762.5478515625,3229.7646484375,28228.609375,6015.12841796875,10386.6357421875,23534.35546875,3682.284423828125,15621.37109375,7182.50537109375,9497.4482421875,14568.2177734375,36479.53125,13135.556640625,9692.318359375,7370.1142578125,11202.4111328125,11818.33984375,5776.9560546875,15970.63671875,1935.736083984375,12544.3662109375,36291.40234375,16345.337890625,6781.78955078125,9721.984375,4034.428466796875,12009.68359375,9573.18359375,45838.16796875,16735.744140625,23478.826171875,5726.544921875,23027.380859375,10762.14453125,4177.18798828125,33976.69140625,30967.4609375,15843.662109375,13698.6484375,37061.5234375,15308.9091796875,12628.427734375,2768.850341796875,23384.7578125,22740.564453125,5518.73046875,12560.365234375,37514.8515625,18864.763671875,28470.548828125,10541.4404296875,10120.6611328125,13305.3388671875,10406.34765625,11006.7294921875,21095.064453125,13121.8984375,10806.5732421875,16110.3916015625,7003.173828125,3359.92578125,2142.916748046875,5740.71728515625,27500.9453125,13612.1259765625,3338.21484375,7135.2060546875,10704.046875,14167.74609375,8720.375,13195.4580078125,2126.072998046875,2976.106201171875,9720.474609375,4331.1728515625,1995.5213623046875,7996.72998046875,2589.872314453125,10088.09765625,8659.1708984375,13678.4609375,12948.2421875,8026.01318359375,2333.302001953125,6732.48291015625,28543.322265625,8361.2509765625,23031.822265625,3189.0625,13616.2666015625,35360.93359375,13527.263671875,5484.63818359375,7539.8994140625,6045.615234375,14853.3173828125,4268.0556640625,13275.759765625,33075.28515625,10683.849609375,4347.439453125,5688.63330078125,31335.23828125,4469.83984375,10204.515625,14008.1611328125,11572.1982421875,3333.654296875,8939.3134765625,4971.75830078125,5780.2490234375,10420.1298828125,33322.48046875,26435.912109375,43238.95703125,14552.181640625,7494.26806640625,7680.69091796875,12546.8857421875,7995.53955078125,11696.365234375,2602.787353515625,12054.89453125,2634.843505859375,7342.12109375,3451.201904296875,16499.068359375,4845.45361328125,11602.326171875,7562.68115234375,12081.9970703125,7826.7626953125,11440.9619140625,14064.353515625,9714.8642578125,2653.768798828125,22832.91796875,2775.38916015625,40964.71484375,2316.6220703125,9909.818359375,14608.060546875,35855.6015625,29089.14453125,4977.6123046875,8371.205078125,5895.38671875,2486.993408203125,5371.5751953125,4042.35302734375,8619.0888671875,12729.3564453125,36180.33984375,5367.416015625,9655.607421875,10334.828125,39259.94921875,26178.8515625,24617.33203125,15459.490234375,13821.1337890625,28124.58984375,4557.8564453125,15971.6162109375,11561.0419921875,2659.15673828125,48395.8359375,8754.6982421875,24481.046875,38677.328125,3782.57861328125,8474.7158203125,2434.347900390625,4296.0751953125,2302.657470703125,12464.216796875,24614.36328125,15184.5712890625,10731.5029296875,14581.486328125,36871.578125,14564.1875,13563.919921875,13011.7177734375,12677.9951171875,41548.55078125,9838.10546875,13475.453125,11856.0419921875,2299.955810546875,18351.5390625,11281.1552734375,30128.73828125,4312.81396484375,41545.9765625,1807.82958984375,16163.7958984375,55530.40234375,4018.766357421875,4117.76953125,11721.6884765625,13578.197265625,3448.9755859375,46513.8046875,6547.447265625,10458.828125,14958.7744140625,5096.30419921875,13048.8525390625,26007.25390625,3399.29443359375,2890.791259765625,13126.935546875,14805.8427734375,11898.701171875,23751.232421875,25379.953125,5950.1845703125,13973.8466796875,18367.142578125,13308.9267578125,2709.89208984375,42290.234375,4191.9296875,3397.6953125,10780.0634765625,3411.446533203125,15161.482421875,26865.9453125,9312.673828125,12821.57421875,49177.95703125,11348.853515625,6037.9443359375,26585.91796875,12772.1337890625,12813.94921875,12391.84375,14166.900390625,11414.01171875,46817.73046875,13834.3203125,3400.739013671875,11184.6044921875,11524.1240234375,4352.1943359375,23868.333984375,4050.489501953125,12350.9443359375,9783.57421875,12589.4619140625,5524.2529296875,4847.8037109375,13786.525390625,6802.62744140625,3578.775390625,13361.4189453125,9871.732421875,3027.659912109375,8536.9541015625,7071.83837890625,2438.121826171875,5272.958984375,9114.1259765625,12350.283203125,12205.9775390625,34980.15625,7574.2666015625,2538.943603515625,5455.6806640625,16351.5078125,6807.2626953125,10584.6865234375,12429.9775390625,8888.548828125,31594.833984375,28535.32421875,4406.49853515625,13641.0078125,4655.80419921875,9873.984375,10056.787109375,11481.7822265625,5756.3681640625,23743.79296875,13094.45703125,4252.982421875,10400.0693359375,24829.541015625,13723.6943359375,43191.19921875,5710.32080078125,7581.20751953125,6578.2529296875,12508.359375,10510.6123046875,10065.5009765625,14040.4267578125,3374.107666015625,6452.86474609375,11558.1298828125,5699.93701171875,13108.470703125,12538.5888671875,5699.9150390625,4254.01611328125,49457.5546875,14667.8525390625,3095.455322265625,12098.6259765625,10241.5078125,3268.408447265625,5938.228515625,6366.2685546875,8833.181640625,13180.775390625,2936.135986328125,46855.48046875,25742.33203125,5781.078125,32837.04296875,2978.88427734375,4430.31201171875,6797.451171875,9536.2353515625,35274.15234375,10400.265625,5585.638671875,7300.2451171875,18300.986328125,2904.59228515625,8595.552734375,32320.95703125,13835.439453125,8765.3056640625,2358.03173828125,8754.67578125,13998.8935546875,44713.98046875,2286.633056640625,6821.10009765625,2642.349853515625,14657.33203125,40061.9375,26927.05859375,22777.48828125,9735.1259765625,2883.13916015625,3836.0849609375,11386.19921875,10311.3447265625,6446.130859375,8643.9228515625,6570.662109375,14284.2255859375,12785.509765625,15312.62890625,9495.125,34756.36328125,14846.720703125,10992.912109375,5193.498046875,15750.7470703125,5992.814453125,10131.5615234375,10405.8251953125,12328.71484375,12859.158203125,8919.2685546875,2828.3056640625,8207.9599609375,2236.724609375,33734.2578125,13633.6923828125,2946.2607421875,2966.35546875,7385.55615234375,13407.4658203125,6114.4912109375,5906.814453125,7997.77783203125,6774.546875,8794.7197265625,3212.0087890625,6032.0654296875,29124.7578125,8057.94775390625,2184.355224609375,10490.9619140625,4557.45263671875,1764.9542236328125,3549.7880859375,47672.4921875,15337.6171875,17283.7265625,4438.44873046875,21232.86328125,13027.9833984375,6567.35400390625,6427.6015625,7144.46630859375,5098.06787109375,2801.5244140625,27473.25,26531.755859375,35957.05078125,32471.931640625,11836.134765625,4153.53857421875,13976.4833984375,8657.2958984375,5585.71435546875,12392.8369140625,8106.34765625,1948.7064208984375,25680.13671875,15490.2880859375,6096.310546875,12912.3623046875,5270.71484375,6945.80029296875,11275.39453125,2241.767822265625,47409.65625,11798.5107421875,14679.5185546875,5333.51904296875,4586.31689453125,7004.880859375,5807.97607421875,2675.444091796875,8156.73095703125,13433.3916015625,17052.611328125,56943.53125,10336.740234375,11945.537109375,6535.494140625,10652.3564453125,5946.70068359375,6596.46044921875,12847.0107421875,2748.63818359375,27080.78515625,3946.78173828125,8899.6142578125,14750.2919921875,40815.78515625,10039.3564453125,9167.8896484375,9510.939453125,11453.8359375,13516.7275390625,22607.203125,3814.328369140625,1996.17578125,38114.63671875,2954.08056640625,5431.9150390625,11203.4169921875,9941.9775390625,3103.263671875,10801.66015625,7653.580078125,9539.69921875],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"markers\",\"x\":[7281.5056,5267.81815,12347.172,24513.09126,3736.4647,7358.17565,9788.8659,17085.2676,8211.1002,19798.05455,3077.0955,3385.39915,6837.3687,8538.28845,26392.26029,13012.20865,3227.1211,15170.069,11073.176,20773.62775,39556.4945,2134.9015,2198.18985,6555.07035,4340.4409,12622.1795,7740.337,12475.3513,3987.926,21082.16,1241.565,40103.89,17929.30337,8302.53565,3471.4096,5846.9176,13352.0998,9144.565,25656.57526,7726.854,13887.9685,30284.64294,5266.3656,12797.20962,1146.7966,7046.7222,8627.5411,39597.4072,12323.936,11454.0215,40904.1995,3171.6149,7445.918,13607.36875,27346.04207,12557.6053,10797.3362,5488.262,6282.235,40941.2854,1708.0014,23306.547,28950.4692,1664.9996,17361.7661,7345.084,18157.876,7256.7231,7626.993,26236.57997,7325.0482,1720.3537,7153.5539,6986.697,8232.6388,10370.91255,4889.0368,6474.013,1625.43375,10115.00885,10264.4421,9386.1613,18223.4512,3561.8889,23887.6627,3392.9768,1135.9407,1880.487,5630.45785,10156.7832,22144.032,62592.87309,9500.57305,46113.511,13390.559,4076.497,6389.37785,9880.068,4751.07,12629.8967,6664.68595,21344.8467,22395.74424,4466.6214,9249.4952,8703.456,38245.59327,36189.1017,19023.26,5080.096,9957.7216,6313.759,4151.0287,17748.5062,37133.8982,21472.4788,4350.5144,41661.602,6985.50695,23241.47453,39983.42595,9174.13565,6710.1919,1633.9618,47055.5321,2899.48935,12231.6136,5138.2567,4646.759,12913.9924,24476.47851,27808.7251,18033.9679,25309.489,6360.9936,20296.86345,10600.5483,9583.8933,34806.4677,5708.867,6457.8434,21259.37795,12928.7911,2904.088,3537.703,36085.219,10713.644,21232.18226,8457.818,18838.70366,3500.6123,6753.038,1256.299,45710.20785,11289.10925,1141.4451,12644.589,1121.8739,12495.29085,27218.43725,13129.60345,1837.2819,6311.952,12333.828,18218.16139,48173.361,6781.3542,3994.1778,1621.8827,11165.41765,9964.06,7441.501,5240.765,4686.3887,18972.495,6393.60345,3732.6251,19144.57652,7633.7206,11093.6229,46889.2612,11931.12525,4454.40265,1149.3959,34779.615,3277.161,27533.9129,10959.33,19594.80965,2483.736,10065.413,7512.267,19442.3535,7209.4918,11299.343,10231.4999,14711.7438,4906.40965,13415.0381,6123.5688,5469.0066,1705.6245,12646.207,6435.6237,11482.63485,44400.4064,17663.1442,14590.63205,38711.0,2166.732,8116.68,38746.3551,5584.3057,6775.961,55135.40209,18903.49141,5920.1041,3378.91,13430.265,11353.2276,5253.524,4738.2682,2196.4732,11987.1682,1515.3449,17178.6824,12268.63225,14133.03775,7421.19455,36021.0112,5966.8874,17081.08,6059.173,37484.4493,10338.9316,2855.43755,13616.3586,29523.1656,1607.5101,9504.3103,25517.11363,18310.742,9869.8102,38415.474,11554.2236,12609.88702,10594.2257,3238.4357,14394.5579,2585.85065,2927.0647,14254.6082,40932.4295,4883.866,7418.522,9630.397,8606.2174,5003.853,2680.9493,9549.5651,3847.674,24873.3849,5699.8375,42856.838,3201.24515,22493.65964,11566.30055,7160.3303,1646.4297,9617.66245,2203.47185,1137.011,9487.6442,7731.85785,7443.64305,2523.1695,34472.841,1969.614,7789.635,41676.0811,1694.7964,1906.35825,11763.0009,9225.2564,21098.55405,2709.1119,4992.3764,6600.20595,18608.262,2459.7201,8252.2843,4441.21315,5425.02335,4058.71245,15518.18025,4544.2348,19199.944,8347.1643,5478.0368,14007.222,42303.69215,5377.4578,1628.4709,16420.49455,10436.096,9778.3472,15612.19335,7261.741,11015.1747,10493.9458,6203.90175,10197.7722,6770.1925,1972.95,9304.7019,29141.3603,46255.1125,11013.7119,8162.71625,12244.531,5662.225,5383.536,12094.478,14478.33015,3208.787,7639.41745,2897.3235,29330.98315,11534.87265,1137.4697,23568.272,13429.0354,9222.4026,36397.576,3956.07145,10965.446,9377.9047,2801.2588,44585.45587,23967.38305,13228.84695,2352.96845,6334.34355,11938.25595,6338.0756,20462.99766,8068.185,4949.7587,9863.4718,39871.7043,19361.9988,6184.2994,1728.897,7323.734819,14418.2804,5246.047,38344.566,7144.86265,2207.69745,3579.8287,10325.206,8310.83915,3481.868,4399.731,5385.3379,4766.022,16232.847,13974.45555,10795.93733,3021.80915,8615.3,9361.3268,35585.576,7147.105,4239.89265,11735.87905,17128.42608,5375.038,10791.96,3443.064,5415.6612,7727.2532,18765.87545,6358.77645,5002.7827,14449.8544,2597.779,13047.33235,51194.55914,2331.519,11946.6259,8964.06055,7160.094,13725.47184,8825.086,11411.685,8930.93455,24106.91255,17878.90068,22462.04375,1391.5287,8240.5896],\"y\":[9160.0087890625,6728.009765625,13156.13671875,14629.8740234375,4880.65234375,9072.9296875,12666.658203125,19832.330078125,10447.03125,20964.177734375,3593.76708984375,4363.52587890625,8872.8955078125,10468.529296875,11128.8271484375,13505.43359375,4256.96337890625,15764.349609375,11745.541015625,27231.92578125,33932.859375,3670.863037109375,2276.895263671875,8185.548828125,5515.5048828125,13111.19140625,9255.009765625,13596.0263671875,4524.10888671875,28956.828125,1780.0660400390625,38435.1015625,9353.82421875,10499.06640625,5247.72412109375,7716.587890625,13962.7421875,10403.873046875,11185.1796875,9196.125,15155.6494140625,11576.677734375,6716.4970703125,9414.8798828125,2137.362548828125,9105.248046875,10833.9345703125,32407.037109375,14011.318359375,12207.810546875,48156.99609375,4558.5869140625,8959.7568359375,15098.015625,11832.14453125,14431.216796875,12470.451171875,7045.53955078125,7808.99072265625,33879.8984375,2240.08203125,28079.103515625,32483.03515625,2847.96337890625,18410.3125,8604.478515625,6100.1572265625,9120.7783203125,8785.287109375,9579.197265625,8999.466796875,2358.955078125,8616.9013671875,9076.9736328125,10390.13671875,12152.611328125,6700.2138671875,7832.66015625,2163.9892578125,12464.6650390625,11943.0556640625,11799.333984375,21349.05078125,4663.326171875,25727.169921875,4474.6875,2017.7703857421875,2234.69384765625,7937.8740234375,11361.1767578125,32067.228515625,29983.8046875,11794.513671875,56784.82421875,13859.779296875,5486.12646484375,8143.06689453125,10877.5419921875,6252.35107421875,14310.404296875,8432.1123046875,2330.392333984375,4526.0693359375,8327.8916015625,10571.6513671875,10085.4970703125,23371.994140625,30668.5625,19187.470703125,5507.0546875,12044.4716796875,8109.3916015625,5606.0283203125,25623.265625,34244.5078125,26393.044921875,5834.72900390625,40832.234375,8965.708984375,4904.33740234375,37083.64453125,10691.5439453125,8368.880859375,2136.17919921875,41341.5078125,3660.66357421875,12858.0654296875,6520.29150390625,6307.7216796875,14671.0947265625,11506.92578125,30597.44921875,25262.953125,33220.6015625,11554.6904296875,25596.818359375,12468.0390625,11332.5439453125,26076.52734375,6241.818359375,8203.41015625,24322.3671875,15114.099609375,3976.145751953125,4748.99267578125,31683.720703125,11415.826171875,9218.6171875,9846.52734375,6192.4326171875,4284.36962890625,8146.8486328125,1938.864013671875,39780.09765625,13225.740234375,2078.408935546875,12962.9287109375,1862.805908203125,14544.1494140625,34291.6640625,14673.01171875,3311.118896484375,7412.60302734375,13256.24609375,4237.57421875,50414.22265625,8697.185546875,5762.50927734375,2003.112060546875,13022.3017578125,11996.10546875,8922.6044921875,6371.8759765625,6803.87890625,20219.212890625,8177.96484375,4762.0107421875,10386.0517578125,9690.5234375,12824.8447265625,39981.45703125,13680.97265625,6050.58056640625,2165.9970703125,32775.828125,4562.13623046875,33977.4296875,12001.470703125,20888.091796875,4071.766357421875,11373.4091796875,8949.248046875,6540.1748046875,9313.642578125,12409.361328125,11509.2109375,16232.5458984375,5802.396484375,13861.9765625,7876.9248046875,7084.4033203125,2213.897216796875,13291.9326171875,8962.65625,2045.221923828125,43476.125,17452.447265625,16100.9755859375,37581.1015625,2740.824462890625,9502.49609375,34481.625,7713.9384765625,8011.9970703125,37291.06640625,5186.28857421875,7875.86767578125,3901.91796875,15124.521484375,12414.3408203125,6428.6787109375,6595.24755859375,2260.1494140625,14474.7197265625,2559.9853515625,22324.876953125,13528.490234375,2112.59765625,8892.5654296875,29578.77734375,8353.0048828125,25377.822265625,7203.69970703125,33318.765625,12287.404296875,4215.37646484375,14819.962890625,35704.8203125,1844.7786865234375,11723.7724609375,13044.5693359375,23633.919921875,11214.9423828125,35039.296875,13836.3349609375,3938.567626953125,12774.35546875,4346.640625,15619.705078125,2977.452392578125,4543.654296875,14470.267578125,40523.17578125,6105.1943359375,8575.62890625,11010.0234375,10817.38671875,6070.7548828125,3886.87158203125,11511.892578125,4816.9736328125,26725.115234375,7987.99609375,39981.1171875,5677.74658203125,3410.8671875,13188.4052734375,9548.6455078125,2996.633544921875,11722.1865234375,2330.392333984375,2029.5609130859375,11852.84375,9585.24609375,9732.8681640625,3733.148681640625,31082.919921875,2993.64111328125,10236.3291015625,38416.80859375,2094.611572265625,2759.698974609375,13624.646484375,11435.2705078125,22381.3828125,2766.292724609375,6484.4873046875,8627.4599609375,21867.18359375,2825.45068359375,9995.72265625,6043.9462890625,7564.7080078125,6036.76220703125,18586.23828125,5897.7607421875,23536.439453125,10822.427734375,7231.54150390625,14672.9580078125,36051.92578125,7465.099609375,2197.447509765625,17821.796875,11623.8154296875,11493.1533203125,16599.6875,9566.6689453125,12959.6875,11662.513671875,7874.013671875,11880.126953125,8760.111328125,3030.391845703125,11805.591796875,34757.015625,45485.328125,13601.76171875,10645.9248046875,13871.927734375,7664.982421875,6645.73291015625,13142.9501953125,12004.37109375,3667.8916015625,9372.0517578125,4256.8349609375,32630.341796875,13428.2158203125,2034.6142578125,31032.88671875,13755.169921875,11607.5390625,35266.34765625,5461.05078125,11952.7685546875,11862.337890625,2874.80322265625,35981.33984375,25817.177734375,14347.8671875,3427.682861328125,8878.1015625,13626.5634765625,8630.2099609375,13925.099609375,9540.23828125,6476.8681640625,11213.1953125,36864.140625,19938.111328125,7847.34130859375,1777.2042236328125,2315.845458984375,15995.150390625,6413.7412109375,39894.421875,8749.5673828125,2376.943115234375,6328.56298828125,11605.0693359375,10175.5078125,3911.50927734375,6524.95947265625,6692.068359375,5709.03857421875,22719.49609375,15240.99609375,2933.4052734375,4591.43310546875,10138.716796875,11433.056640625,34577.1796875,8588.3857421875,5767.0859375,12724.3828125,6926.56201171875,6349.58642578125,12018.662109375,3901.383056640625,7708.294921875,9786.1572265625,21822.89453125,8237.5048828125,6636.6259765625,15474.603515625,2764.382568359375,14860.408203125,37045.64453125,2318.985595703125,14118.470703125,11047.0146484375,8682.8310546875,8994.310546875,9925.4560546875,12694.279296875,10301.3798828125,26767.783203125,4813.2109375,30627.728515625,2482.556884765625,10329.4677734375],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predichos\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predichos\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de entrenamiento\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de prueba\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"shapes\":[{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":1764.9542236328125,\"x1\":56943.53125,\"y0\":1764.9542236328125,\"y1\":56943.53125},{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":1777.2042236328125,\"x1\":56784.82421875,\"xref\":\"x2\",\"y0\":1777.2042236328125,\"y1\":56784.82421875,\"yref\":\"y2\"}],\"title\":{\"text\":\"Segunda red neuronal\"},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e5e11df9-2cd0-4129-b1d4-1ef3ff8bf84d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuevamente, parece que hay 3 o 4 grupos de casos. Sin embargo, la diferencia es menos obvia que cuando se graficaron los resultados de la regresión lineal (y más aún en comparación a la primera red). "
      ],
      "metadata": {
        "id": "7Bxwkj7E0vCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión"
      ],
      "metadata": {
        "id": "aqJW27FvlX1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La regresión lineal fue un buen primer acercamiento a este problema considerando que arrojó una $R^2$ mayor a 0.7. Sin embargo, para cargos grandes no se comportó bien y se vieron 3 o 4 grupos distintos. Un análisis a detalle de las variables e ingeniería de características seguramente ayuda a mejorar el desempeño. \n",
        "\n",
        "El segundo intento con la red neuronal predice mejor que la anterior, es decir, logró *aprender* mejor las relaciones entre las variables. Con más capas, el desempeño mejora, pero se dejará como ejercicio."
      ],
      "metadata": {
        "id": "mirMh75XlZST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Opcionales"
      ],
      "metadata": {
        "id": "J1vgW_Dol06E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ¿Por qué se obtuvieron mejores resultados con la red neuronal que con la regresión lineal?\n",
        "\n",
        "    1.1 ¿Por qué la primera red neuronal predice peor que la regresión lineal?\n",
        "    \n",
        "    1.2 ¿Cuál es la intuición detrás de \"si hay más capas, usualmente la red neuronal predice mejor\"?\n",
        "\n",
        "2. Aplicar transformaciones a las variables y ajustar de nuevo la regresión lineal y una red neuronal para ver si mejoran los resultados.\n",
        "\n",
        "3. Diseñar una red más compleja que mejore los resultados obtenidos"
      ],
      "metadata": {
        "id": "X51_URG3l2EJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distintos modos de crear una red"
      ],
      "metadata": {
        "id": "HODEvKp3DVw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "xstbxHVeEVq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creando el modelo capa por capa"
      ],
      "metadata": {
        "id": "m6LlvY4GDbRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "     keras.layers.Input([X_train_tr.shape[1]]),\n",
        "     keras.layers.Dense(64, activation='relu'),\n",
        "     keras.layers.Dense(64, activation='relu'),\n",
        "     keras.layers.Dense(64, activation='relu'),     \n",
        "     keras.layers.Dense(1)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "HXx-2WjmDX7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dando uno a uno los elementos y diciendo el orden que habrá"
      ],
      "metadata": {
        "id": "56qduXKODelT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_input = keras.Input([X_train_tr.shape[1]])\n",
        "\n",
        "dense_1 = keras.layers.Dense(64, activation='relu')(data_input)\n",
        "dense_2 = keras.layers.Dense(64, activation='relu')(dense_1)\n",
        "dense_3 = keras.layers.Dense(64, activation='relu')(dense_2)\n",
        "output_layer = keras.layers.Dense(1)(dense_3)\n",
        "\n",
        "model = keras.Model(inputs=[data_input], outputs=[output_layer])"
      ],
      "metadata": {
        "id": "mkcNyT3IDhgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregando capas al modelo"
      ],
      "metadata": {
        "id": "6aRJQ_L4ESCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "model.add(keras.layers.Dense(1))"
      ],
      "metadata": {
        "id": "eS8ZdHHUEpxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ligas de interés\n",
        "\n",
        "- [Documentación de keras](https://keras.io/api/)\n",
        "- [Documentación de Capas](https://keras.io/api/layers/)\n",
        "- Mas documentación sobre optimizadores, métricas, funciones de perdida, etc. puede encontrarse en la documentación"
      ],
      "metadata": {
        "id": "DpfJLK_Ppg5N"
      }
    }
  ]
}